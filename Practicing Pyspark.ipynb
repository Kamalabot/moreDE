{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed0f3d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working on pyspark in Jupyterlab\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee49d1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93064bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c061e514",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf(). \\\n",
    "    setMaster(\"local\"). \\\n",
    "    setAppName(\"Orders_Revenue\"). \\\n",
    "    set(\"conf.ui.port\",\"10567\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf9e651d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: conf.ui.port\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/21 08:44:48 WARN Utils: Your hostname, codeStation resolves to a loopback address: 127.0.1.1; using 192.168.213.83 instead (on interface wlo1)\n",
      "22/11/21 08:44:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/21 08:44:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "922b877e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/21 06:01:17 WARN Utils: Your hostname, codeStation resolves to a loopback address: 127.0.1.1; using 192.168.213.83 instead (on interface wlo1)\n",
      "22/11/21 06:01:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/21 06:01:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Practice').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045ed2f5",
   "metadata": {},
   "source": [
    "#### Execution of the file in command prompt\n",
    "\n",
    "spark-submit --master local --deploy-mode client --conf spark.ui.port=12901 --conf spark.dynamicAllocation.anabled=false --num-executors 1 --executor-memory 512M --executor-cores 1 file_name_for_DE.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "909b50b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.213.83:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Practice>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75aa24c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39msparkContext\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0b61326",
   "metadata": {},
   "outputs": [],
   "source": [
    "productPath = \"/home/solverbot/spark-warehouse/retail_db/products/part-00000\"\n",
    "orderitemPath = \"/home/solverbot/spark-warehouse/retail_db/order_items/part-00000\"\n",
    "ordersPath = \"/home/solverbot/spark-warehouse/retail_db/orders/part-00000.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1273d61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = sc.textFile(productPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23eabafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method flatMap in module pyspark.rdd:\n",
      "\n",
      "flatMap(f: Callable[[~T], Iterable[~U]], preservesPartitioning: bool = False) -> 'RDD[U]' method of pyspark.rdd.RDD instance\n",
      "    Return a new RDD by first applying a function to all elements of this\n",
      "    RDD, and then flattening the results.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> rdd = sc.parallelize([2, 3, 4])\n",
      "    >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n",
      "    [1, 1, 1, 2, 2, 3]\n",
      "    >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n",
      "    [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(products.flatMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ac4d8a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3545912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,2,Quest Q64 10 FT. x 10 FT. Slant Leg Instant U,,59.98,http://images.acmesports.sports/Quest+Q64+10+FT.+x+10+FT.+Slant+Leg+Instant+Up+Canopy\n",
      "2,2,Under Armour Men's Highlight MC Football Clea,,129.99,http://images.acmesports.sports/Under+Armour+Men%27s+Highlight+MC+Football+Cleat\n",
      "3,2,Under Armour Men's Renegade D Mid Football Cl,,89.99,http://images.acmesports.sports/Under+Armour+Men%27s+Renegade+D+Mid+Football+Cleat\n",
      "4,2,Under Armour Men's Renegade D Mid Football Cl,,89.99,http://images.acmesports.sports/Under+Armour+Men%27s+Renegade+D+Mid+Football+Cleat\n",
      "5,2,Riddell Youth Revolution Speed Custom Footbal,,199.99,http://images.acmesports.sports/Riddell+Youth+Revolution+Speed+Custom+Football+Helmet\n"
     ]
    }
   ],
   "source": [
    "for i in products.take(5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "536812d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bce7a962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'solverbot'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299affb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Data Processing - Overview'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6299c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/21 06:08:57 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "newSessio = SparkSession.builder.config('spark.ui.port','0').\\\n",
    "            config(\"spark.sql.warehouse.dir\",f\"/home/{username}/spark-warehouse/retail_db/\"). \\\n",
    "            appName(f'{username} | Python - Data Processing - Overview'). \\\n",
    "            getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "764acd5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.213.83:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fca44759570>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newSessio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4614aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,2013-07-25 00:00:00.0,11599,CLOSED',\n",
       " '2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT',\n",
       " '3,2013-07-25 00:00:00.0,12111,COMPLETE',\n",
       " '4,2013-07-25 00:00:00.0,8827,CLOSED',\n",
       " '5,2013-07-25 00:00:00.0,11318,COMPLETE']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders = sc.textFile(ordersPath)\n",
    "orders.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ea3eb0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['37551,2014-03-13 00:00:00.0,3384,COMPLETE',\n",
       " '52013,2014-06-16 00:00:00.0,7642,COMPLETE']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orderFiltered = orders.filter(lambda o: o.split(',')[3] in ('COMPLETE','CLOSED'))\n",
    "orderFiltered.takeSample(True,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50cf4849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, '2013-07-25 00:00:00.0'), (2, '2013-07-25 00:00:00.0')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordersMap = orders.map(lambda o: (int(o.split(',')[0]), o.split(',')[1]))\n",
    "ordersMap.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4e29778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172198"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orderItems = sc.textFile(orderitemPath)\n",
    "orderItems.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e7999b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 299.98), (2, 199.99), (2, 250.0), (2, 129.99), (4, 49.98)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orderItemsMap = orderItems.map(lambda oi: (int(oi.split(',')[1]),float(oi.split(',')[4])))\n",
    "orderItemsMap.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56469b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(2, 579.98), (4, 699.85)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orderItemsReduce = orderItemsMap.reduceByKey(lambda o,p: o + p)\n",
    "orderItemsReduce.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57b15481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method groupByKey in module pyspark.rdd:\n",
      "\n",
      "groupByKey(numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7fca45489480>) -> 'RDD[Tuple[K, Iterable[V]]]' method of pyspark.rdd.PipelinedRDD instance\n",
      "    Group the values for each key in the RDD into a single sequence.\n",
      "    Hash-partitions the resulting RDD with numPartitions partitions.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    If you are grouping in order to perform an aggregation (such as a\n",
      "    sum or average) over each key, using reduceByKey or aggregateByKey will\n",
      "    provide much better performance.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      "    >>> sorted(rdd.groupByKey().mapValues(len).collect())\n",
      "    [('a', 2), ('b', 1)]\n",
      "    >>> sorted(rdd.groupByKey().mapValues(list).collect())\n",
      "    [('a', [1, 1]), ('b', [1])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(orderItemsMap.groupByKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "967336fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 21:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2=>[199.99, 250.0, 129.99]\n",
      "4=>[49.98, 299.95, 150.0, 199.92]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "orderItemsGBK = orderItemsMap.groupByKey()\n",
    "for i in orderItemsGBK.take(2): print(f'{i[0]}=>{list(i[1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "63de6b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(4, ('2013-07-25 00:00:00.0', 699.85)),\n",
       " (8, ('2013-07-25 00:00:00.0', 729.8399999999999)),\n",
       " (12, ('2013-07-25 00:00:00.0', 1299.8700000000001)),\n",
       " (16, ('2013-07-25 00:00:00.0', 419.93)),\n",
       " (20, ('2013-07-25 00:00:00.0', 879.8599999999999))]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orderJoin = ordersMap.join(orderItemsReduce)\n",
    "orderJoin.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04f12359",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(4,\n",
       "  ('2013-07-25 00:00:00.0',\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fca28369450>)),\n",
       " (8,\n",
       "  ('2013-07-25 00:00:00.0',\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fca2836ad10>)),\n",
       " (12,\n",
       "  ('2013-07-25 00:00:00.0',\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fca28368e50>)),\n",
       " (16,\n",
       "  ('2013-07-25 00:00:00.0',\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fca2836ab60>)),\n",
       " (20,\n",
       "  ('2013-07-25 00:00:00.0',\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fca2836bf40>))]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orderJoinList = ordersMap.join(orderItemsGBK)\n",
    "orderJoinList.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ed369045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[49.98, 299.95, 150.0, 199.92]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(orderJoinList.take(1)[0][1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "500465be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 61:>                                                         (0 + 4) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, ('2013-07-25 00:00:00.0', 299.98)),\n",
       " (2, ('2013-07-25 00:00:00.0', 579.98))]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orderJoinSorted = orderJoin.sortByKey()\n",
    "orderJoinSorted.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "55c12f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method sortBy in module pyspark.rdd:\n",
      "\n",
      "sortBy(keyfunc: Callable[[~T], ForwardRef('S')], ascending: bool = True, numPartitions: Optional[int] = None) -> 'RDD[T]' method of pyspark.rdd.PipelinedRDD instance\n",
      "    Sorts this RDD by the given keyfunc\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
      "    >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()\n",
      "    [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      "    >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()\n",
      "    [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(orderJoin.sortBy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7f67086d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(68703, ('2013-08-16 00:00:00.0', 3449.9100000000003))]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orderJoinSortedRevn = orderJoin.sortBy(lambda x: x[1][1], ascending=False)\n",
    "orderJoinSortedRevn.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3fea9412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, '2013-07-25 00:00:00.0', [49.98, 299.95, 150.0, 199.92])]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orderJoinListPop = orderJoinList.map(lambda x: (x[0], x[1][0], list(x[1][1])))\n",
    "orderJoinListPop.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b5d693a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#writing to file\n",
    "orderJoinListPop.saveAsTextFile(f\"/home/{username}/orders_and_sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c0dcd67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57431"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readingBack = sc.textFile(f\"/home/{username}/orders_and_sales\")\n",
    "readingBack.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "55c94392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"(7, '2013-07-25 00:00:00.0', [199.99, 299.98, 79.95])\",\n",
       " \"(11, '2013-07-25 00:00:00.0', [59.99, 159.96, 49.98, 399.96, 249.9])\"]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readingBack.take(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
