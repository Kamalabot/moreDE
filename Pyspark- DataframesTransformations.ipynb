{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02eb6a99",
   "metadata": {},
   "source": [
    "#### This notebook is about the Transformation to be done on DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f448b24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c1e3bac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [\n",
    "    (1, \"Scott\", \"Tiger\", 1000.0, \n",
    "      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "    ),\n",
    "     (2, \"Henry\", \"Ford\", 1250.0, \n",
    "      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "     ),\n",
    "     (3, \"Nick\", \"Junior\", 750.0, \n",
    "      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "     ),\n",
    "     (4, \"Bill\", \"Gomes\", 1500.0, \n",
    "      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "     )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0b61326",
   "metadata": {},
   "outputs": [],
   "source": [
    "productPath = \"/home/solverbot/spark-warehouse/retail_db/products/part-00000\"\n",
    "orderitemPath = \"/home/solverbot/spark-warehouse/retail_db/order_items/part-00000\"\n",
    "ordersPath = \"/home/solverbot/spark-warehouse/retail_db/orders/part-00000.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f609fb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/21 20:25:17 WARN Utils: Your hostname, codeStation resolves to a loopback address: 127.0.1.1; using 192.168.102.83 instead (on interface wlo1)\n",
      "22/11/21 20:25:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/21 20:25:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#What is the difference between Session and Context?\n",
    "spark = SparkSession.builder.appName('DF transformations').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4fb19e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = spark. \\\n",
    "    createDataFrame(employees,\n",
    "                    schema=\"\"\"employee_id INT, first_name STRING, \n",
    "                    last_name STRING, salary FLOAT, nationality STRING,\n",
    "                    phone_number STRING, ssn STRING\"\"\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e5e70e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be6049b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|upper_nationality|count|\n",
      "+-----------------+-----+\n",
      "|    UNITED STATES|    1|\n",
      "|            INDIA|    1|\n",
      "|   UNITED KINGDOM|    1|\n",
      "|        AUSTRALIA|    1|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    groupBy(upper(col(\"nationality\")).alias('upper_nationality')). \\\n",
    "    count(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92622c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 4:=============================>                             (1 + 1) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(order_item_id=1, order_item_order_id=1, product_id=957, qty=1, product_cost=299.98, order_subtotal=299.98),\n",
       " Row(order_item_id=2, order_item_order_id=2, product_id=1073, qty=1, product_cost=199.99, order_subtotal=199.99)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orderItemDF = spark.read.csv(\"/home/solverbot/spark-warehouse/retail_db/order_items/\",inferSchema=True).toDF(\"order_item_id\",\"order_item_order_id\",\"product_id\", \"qty\",\"product_cost\",\"order_subtotal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25050fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "orderDF = spark.read.csv(\"/home/solverbot/spark-warehouse/retail_db/orders\",inferSchema=True) \\\n",
    "            .toDF(\"order_id\",\"order_date\",\"order_customer_id\",\"order_status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "885564ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f3db55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+----------+---+------------+--------------+\n",
      "|order_item_id|order_item_order_id|product_id|qty|product_cost|order_subtotal|\n",
      "+-------------+-------------------+----------+---+------------+--------------+\n",
      "|            1|                  1|       957|  1|      299.98|        299.98|\n",
      "|            2|                  2|      1073|  1|      199.99|        199.99|\n",
      "+-------------+-------------------+----------+---+------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderItemDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4d25568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Note, even the numbers are read as strings when the DF is created\n",
    "orderDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3b5bd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_item_id: integer (nullable = true)\n",
      " |-- order_item_order_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- qty: integer (nullable = true)\n",
      " |-- product_cost: double (nullable = true)\n",
      " |-- order_subtotal: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderItemDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d5f5005",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method filter in module pyspark.sql.dataframe:\n",
      "\n",
      "filter(condition: 'ColumnOrName') -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Filters rows using the given condition.\n",
      "    \n",
      "    :func:`where` is an alias for :func:`filter`.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    condition : :class:`Column` or str\n",
      "        a :class:`Column` of :class:`types.BooleanType`\n",
      "        or a string of SQL expression.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df.filter(df.age > 3).collect()\n",
      "    [Row(age=5, name='Bob')]\n",
      "    >>> df.where(df.age == 2).collect()\n",
      "    [Row(age=2, name='Alice')]\n",
      "    \n",
      "    >>> df.filter(\"age > 3\").collect()\n",
      "    [Row(age=5, name='Bob')]\n",
      "    >>> df.where(\"age = 2\").collect()\n",
      "    [Row(age=2, name='Alice')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(orderDF.filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b53a348a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filterOrder = orderDF.filter(orderDF.order_status != \"COMPLETE\")\n",
    "filterOrder.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e2b3e616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+------------+\n",
      "|order_id|         order_date|order_customer_id|order_status|\n",
      "+--------+-------------------+-----------------+------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|      CLOSED|\n",
      "|       3|2013-07-25 00:00:00|            12111|    COMPLETE|\n",
      "+--------+-------------------+-----------------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderDF.where('order_status IN (\"COMPLETE\",\"CLOSED\")').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c9e479e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|(order_status = COMPLETE)|\n",
      "+-------------------------+\n",
      "|                    false|\n",
      "|                    false|\n",
      "+-------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderDF.selectExpr('order_status =\"COMPLETE\"').alias('filtered').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0e0229d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Column in module pyspark.sql.column object:\n",
      "\n",
      "class Column(builtins.object)\n",
      " |  Column(jc: py4j.java_gateway.JavaObject) -> None\n",
      " |  \n",
      " |  A column in a DataFrame.\n",
      " |  \n",
      " |  :class:`Column` instances can be created by::\n",
      " |  \n",
      " |      # 1. Select a column out of a DataFrame\n",
      " |  \n",
      " |      df.colName\n",
      " |      df[\"colName\"]\n",
      " |  \n",
      " |      # 2. Create from an expression\n",
      " |      df.colName + 1\n",
      " |      1 / df.colName\n",
      " |  \n",
      " |  .. versionadded:: 1.3.0\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __and__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __bool__ = __nonzero__(self) -> None\n",
      " |  \n",
      " |  __contains__(self, item: Any) -> None\n",
      " |      # container operators\n",
      " |  \n",
      " |  __div__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __eq__(self, other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary function\n",
      " |  \n",
      " |  __ge__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __getattr__(self, item: Any) -> 'Column'\n",
      " |  \n",
      " |  __getitem__(self, k: Any) -> 'Column'\n",
      " |  \n",
      " |  __gt__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __init__(self, jc: py4j.java_gateway.JavaObject) -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __invert__ = _(self: 'Column') -> 'Column'\n",
      " |  \n",
      " |  __iter__(self) -> None\n",
      " |  \n",
      " |  __le__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __lt__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __mod__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __mul__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __ne__(self, other: Any) -> 'Column'\n",
      " |      binary function\n",
      " |  \n",
      " |  __neg__ = _(self: 'Column') -> 'Column'\n",
      " |  \n",
      " |  __nonzero__(self) -> None\n",
      " |  \n",
      " |  __or__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __pow__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      " |      binary function\n",
      " |  \n",
      " |  __radd__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rand__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rdiv__ = _(self: 'Column', other: Union[ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rmod__ = _(self: 'Column', other: Union[ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rmul__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __ror__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rpow__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      " |      binary function\n",
      " |  \n",
      " |  __rsub__ = _(self: 'Column', other: Union[ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rtruediv__ = _(self: 'Column', other: Union[ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __sub__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __truediv__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  alias(self, *alias: str, **kwargs: Any) -> 'Column'\n",
      " |      Returns this column aliased with a new name or names (in the case of expressions that\n",
      " |      return more than one column, such as explode).\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      alias : str\n",
      " |          desired column names (collects all positional arguments passed)\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      metadata: dict\n",
      " |          a dict of information to be stored in ``metadata`` attribute of the\n",
      " |          corresponding :class:`StructField <pyspark.sql.types.StructField>` (optional, keyword\n",
      " |          only argument)\n",
      " |      \n",
      " |          .. versionchanged:: 2.2.0\n",
      " |             Added optional ``metadata`` argument.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.select(df.age.alias(\"age2\")).collect()\n",
      " |      [Row(age2=2), Row(age2=5)]\n",
      " |      >>> df.select(df.age.alias(\"age3\", metadata={'max': 99})).schema['age3'].metadata['max']\n",
      " |      99\n",
      " |  \n",
      " |  asc = _(self: 'Column') -> 'Column'\n",
      " |      Returns a sort expression based on ascending order of the column.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.asc()).collect()\n",
      " |      [Row(name='Alice'), Row(name='Tom')]\n",
      " |  \n",
      " |  asc_nulls_first = _(self: 'Column') -> 'Column'\n",
      " |      Returns a sort expression based on ascending order of the column, and null values\n",
      " |      return before non-null values.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.asc_nulls_first()).collect()\n",
      " |      [Row(name=None), Row(name='Alice'), Row(name='Tom')]\n",
      " |  \n",
      " |  asc_nulls_last = _(self: 'Column') -> 'Column'\n",
      " |      Returns a sort expression based on ascending order of the column, and null values\n",
      " |      appear after non-null values.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.asc_nulls_last()).collect()\n",
      " |      [Row(name='Alice'), Row(name='Tom'), Row(name=None)]\n",
      " |  \n",
      " |  astype = cast(self, dataType)\n",
      " |      :func:`astype` is an alias for :func:`cast`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  between(self, lowerBound: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DateTimeLiteral'), ForwardRef('DecimalLiteral')], upperBound: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DateTimeLiteral'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      " |      True if the current column is between the lower bound and upper bound, inclusive.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.select(df.name, df.age.between(2, 4)).show()\n",
      " |      +-----+---------------------------+\n",
      " |      | name|((age >= 2) AND (age <= 4))|\n",
      " |      +-----+---------------------------+\n",
      " |      |Alice|                       true|\n",
      " |      |  Bob|                      false|\n",
      " |      +-----+---------------------------+\n",
      " |  \n",
      " |  bitwiseAND = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      Compute bitwise AND of this expression with another expression.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other\n",
      " |          a value or :class:`Column` to calculate bitwise and(&) with\n",
      " |          this :class:`Column`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(a=170, b=75)])\n",
      " |      >>> df.select(df.a.bitwiseAND(df.b)).collect()\n",
      " |      [Row((a & b)=10)]\n",
      " |  \n",
      " |  bitwiseOR = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      Compute bitwise OR of this expression with another expression.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other\n",
      " |          a value or :class:`Column` to calculate bitwise or(|) with\n",
      " |          this :class:`Column`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(a=170, b=75)])\n",
      " |      >>> df.select(df.a.bitwiseOR(df.b)).collect()\n",
      " |      [Row((a | b)=235)]\n",
      " |  \n",
      " |  bitwiseXOR = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      Compute bitwise XOR of this expression with another expression.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other\n",
      " |          a value or :class:`Column` to calculate bitwise xor(^) with\n",
      " |          this :class:`Column`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(a=170, b=75)])\n",
      " |      >>> df.select(df.a.bitwiseXOR(df.b)).collect()\n",
      " |      [Row((a ^ b)=225)]\n",
      " |  \n",
      " |  cast(self, dataType: Union[pyspark.sql.types.DataType, str]) -> 'Column'\n",
      " |      Casts the column into type ``dataType``.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.select(df.age.cast(\"string\").alias('ages')).collect()\n",
      " |      [Row(ages='2'), Row(ages='5')]\n",
      " |      >>> df.select(df.age.cast(StringType()).alias('ages')).collect()\n",
      " |      [Row(ages='2'), Row(ages='5')]\n",
      " |  \n",
      " |  contains = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      Contains the other element. Returns a boolean :class:`Column` based on a string match.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other\n",
      " |          string in line. A value as a literal or a :class:`Column`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.filter(df.name.contains('o')).collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |  \n",
      " |  desc = _(self: 'Column') -> 'Column'\n",
      " |      Returns a sort expression based on the descending order of the column.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.desc()).collect()\n",
      " |      [Row(name='Tom'), Row(name='Alice')]\n",
      " |  \n",
      " |  desc_nulls_first = _(self: 'Column') -> 'Column'\n",
      " |      Returns a sort expression based on the descending order of the column, and null values\n",
      " |      appear before non-null values.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.desc_nulls_first()).collect()\n",
      " |      [Row(name=None), Row(name='Tom'), Row(name='Alice')]\n",
      " |  \n",
      " |  desc_nulls_last = _(self: 'Column') -> 'Column'\n",
      " |      Returns a sort expression based on the descending order of the column, and null values\n",
      " |      appear after non-null values.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.desc_nulls_last()).collect()\n",
      " |      [Row(name='Tom'), Row(name='Alice'), Row(name=None)]\n",
      " |  \n",
      " |  dropFields(self, *fieldNames: str) -> 'Column'\n",
      " |      An expression that drops fields in :class:`StructType` by name.\n",
      " |      This is a no-op if schema doesn't contain field name(s).\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> from pyspark.sql.functions import col, lit\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     Row(a=Row(b=1, c=2, d=3, e=Row(f=4, g=5, h=6)))])\n",
      " |      >>> df.withColumn('a', df['a'].dropFields('b')).show()\n",
      " |      +-----------------+\n",
      " |      |                a|\n",
      " |      +-----------------+\n",
      " |      |{2, 3, {4, 5, 6}}|\n",
      " |      +-----------------+\n",
      " |      \n",
      " |      >>> df.withColumn('a', df['a'].dropFields('b', 'c')).show()\n",
      " |      +--------------+\n",
      " |      |             a|\n",
      " |      +--------------+\n",
      " |      |{3, {4, 5, 6}}|\n",
      " |      +--------------+\n",
      " |      \n",
      " |      This method supports dropping multiple nested fields directly e.g.\n",
      " |      \n",
      " |      >>> df.withColumn(\"a\", col(\"a\").dropFields(\"e.g\", \"e.h\")).show()\n",
      " |      +--------------+\n",
      " |      |             a|\n",
      " |      +--------------+\n",
      " |      |{1, 2, 3, {4}}|\n",
      " |      +--------------+\n",
      " |      \n",
      " |      However, if you are going to add/replace multiple nested fields,\n",
      " |      it is preferred to extract out the nested struct before\n",
      " |      adding/replacing multiple fields e.g.\n",
      " |      \n",
      " |      >>> df.select(col(\"a\").withField(\n",
      " |      ...     \"e\", col(\"a.e\").dropFields(\"g\", \"h\")).alias(\"a\")\n",
      " |      ... ).show()\n",
      " |      +--------------+\n",
      " |      |             a|\n",
      " |      +--------------+\n",
      " |      |{1, 2, 3, {4}}|\n",
      " |      +--------------+\n",
      " |  \n",
      " |  endswith = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      String ends with. Returns a boolean :class:`Column` based on a string match.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`Column` or str\n",
      " |          string at end of line (do not use a regex `$`)\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.filter(df.name.endswith('ice')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      >>> df.filter(df.name.endswith('ice$')).collect()\n",
      " |      []\n",
      " |  \n",
      " |  eqNullSafe = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      Equality test that is safe for null values.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other\n",
      " |          a value or :class:`Column`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df1 = spark.createDataFrame([\n",
      " |      ...     Row(id=1, value='foo'),\n",
      " |      ...     Row(id=2, value=None)\n",
      " |      ... ])\n",
      " |      >>> df1.select(\n",
      " |      ...     df1['value'] == 'foo',\n",
      " |      ...     df1['value'].eqNullSafe('foo'),\n",
      " |      ...     df1['value'].eqNullSafe(None)\n",
      " |      ... ).show()\n",
      " |      +-------------+---------------+----------------+\n",
      " |      |(value = foo)|(value <=> foo)|(value <=> NULL)|\n",
      " |      +-------------+---------------+----------------+\n",
      " |      |         true|           true|           false|\n",
      " |      |         null|          false|            true|\n",
      " |      +-------------+---------------+----------------+\n",
      " |      >>> df2 = spark.createDataFrame([\n",
      " |      ...     Row(value = 'bar'),\n",
      " |      ...     Row(value = None)\n",
      " |      ... ])\n",
      " |      >>> df1.join(df2, df1[\"value\"] == df2[\"value\"]).count()\n",
      " |      0\n",
      " |      >>> df1.join(df2, df1[\"value\"].eqNullSafe(df2[\"value\"])).count()\n",
      " |      1\n",
      " |      >>> df2 = spark.createDataFrame([\n",
      " |      ...     Row(id=1, value=float('NaN')),\n",
      " |      ...     Row(id=2, value=42.0),\n",
      " |      ...     Row(id=3, value=None)\n",
      " |      ... ])\n",
      " |      >>> df2.select(\n",
      " |      ...     df2['value'].eqNullSafe(None),\n",
      " |      ...     df2['value'].eqNullSafe(float('NaN')),\n",
      " |      ...     df2['value'].eqNullSafe(42.0)\n",
      " |      ... ).show()\n",
      " |      +----------------+---------------+----------------+\n",
      " |      |(value <=> NULL)|(value <=> NaN)|(value <=> 42.0)|\n",
      " |      +----------------+---------------+----------------+\n",
      " |      |           false|           true|           false|\n",
      " |      |           false|          false|            true|\n",
      " |      |            true|          false|           false|\n",
      " |      +----------------+---------------+----------------+\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Unlike Pandas, PySpark doesn't consider NaN values to be NULL. See the\n",
      " |      `NaN Semantics <https://spark.apache.org/docs/latest/sql-ref-datatypes.html#nan-semantics>`_\n",
      " |      for details.\n",
      " |  \n",
      " |  getField(self, name: Any) -> 'Column'\n",
      " |      An expression that gets a field by name in a :class:`StructType`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(r=Row(a=1, b=\"b\"))])\n",
      " |      >>> df.select(df.r.getField(\"b\")).show()\n",
      " |      +---+\n",
      " |      |r.b|\n",
      " |      +---+\n",
      " |      |  b|\n",
      " |      +---+\n",
      " |      >>> df.select(df.r.a).show()\n",
      " |      +---+\n",
      " |      |r.a|\n",
      " |      +---+\n",
      " |      |  1|\n",
      " |      +---+\n",
      " |  \n",
      " |  getItem(self, key: Any) -> 'Column'\n",
      " |      An expression that gets an item at position ``ordinal`` out of a list,\n",
      " |      or gets an item by key out of a dict.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([([1, 2], {\"key\": \"value\"})], [\"l\", \"d\"])\n",
      " |      >>> df.select(df.l.getItem(0), df.d.getItem(\"key\")).show()\n",
      " |      +----+------+\n",
      " |      |l[0]|d[key]|\n",
      " |      +----+------+\n",
      " |      |   1| value|\n",
      " |      +----+------+\n",
      " |  \n",
      " |  ilike = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      SQL ILIKE expression (case insensitive LIKE). Returns a boolean :class:`Column`\n",
      " |      based on a case insensitive match.\n",
      " |      \n",
      " |      .. versionadded:: 3.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : str\n",
      " |          a SQL LIKE pattern\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pyspark.sql.Column.rlike\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.filter(df.name.ilike('%Ice')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  isNotNull = _(self: 'Column') -> 'Column'\n",
      " |      True if the current expression is NOT null.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(name='Tom', height=80), Row(name='Alice', height=None)])\n",
      " |      >>> df.filter(df.height.isNotNull()).collect()\n",
      " |      [Row(name='Tom', height=80)]\n",
      " |  \n",
      " |  isNull = _(self: 'Column') -> 'Column'\n",
      " |      True if the current expression is null.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(name='Tom', height=80), Row(name='Alice', height=None)])\n",
      " |      >>> df.filter(df.height.isNull()).collect()\n",
      " |      [Row(name='Alice', height=None)]\n",
      " |  \n",
      " |  isin(self, *cols: Any) -> 'Column'\n",
      " |      A boolean expression that is evaluated to true if the value of this\n",
      " |      expression is contained by the evaluated values of the arguments.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df[df.name.isin(\"Bob\", \"Mike\")].collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      >>> df[df.age.isin([1, 2, 3])].collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  like = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      SQL like expression. Returns a boolean :class:`Column` based on a SQL LIKE match.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : str\n",
      " |          a SQL LIKE pattern\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pyspark.sql.Column.rlike\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.filter(df.name.like('Al%')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  name = alias(self, *alias, **kwargs)\n",
      " |      :func:`name` is an alias for :func:`alias`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  otherwise(self, value: Any) -> 'Column'\n",
      " |      Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      " |      If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      value\n",
      " |          a literal value, or a :class:`Column` expression.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import functions as F\n",
      " |      >>> df.select(df.name, F.when(df.age > 3, 1).otherwise(0)).show()\n",
      " |      +-----+-------------------------------------+\n",
      " |      | name|CASE WHEN (age > 3) THEN 1 ELSE 0 END|\n",
      " |      +-----+-------------------------------------+\n",
      " |      |Alice|                                    0|\n",
      " |      |  Bob|                                    1|\n",
      " |      +-----+-------------------------------------+\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pyspark.sql.functions.when\n",
      " |  \n",
      " |  over(self, window: 'WindowSpec') -> 'Column'\n",
      " |      Define a windowing column.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      window : :class:`WindowSpec`\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`Column`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Window\n",
      " |      >>> window = Window.partitionBy(\"name\").orderBy(\"age\")                 .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
      " |      >>> from pyspark.sql.functions import rank, min\n",
      " |      >>> from pyspark.sql.functions import desc\n",
      " |      >>> df.withColumn(\"rank\", rank().over(window))                 .withColumn(\"min\", min('age').over(window)).sort(desc(\"age\")).show()\n",
      " |      +---+-----+----+---+\n",
      " |      |age| name|rank|min|\n",
      " |      +---+-----+----+---+\n",
      " |      |  5|  Bob|   1|  5|\n",
      " |      |  2|Alice|   1|  2|\n",
      " |      +---+-----+----+---+\n",
      " |  \n",
      " |  rlike = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      SQL RLIKE expression (LIKE with Regex). Returns a boolean :class:`Column` based on a regex\n",
      " |      match.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : str\n",
      " |          an extended regex expression\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.filter(df.name.rlike('ice$')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  startswith = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      String starts with. Returns a boolean :class:`Column` based on a string match.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`Column` or str\n",
      " |          string at start of line (do not use a regex `^`)\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.filter(df.name.startswith('Al')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      >>> df.filter(df.name.startswith('^Al')).collect()\n",
      " |      []\n",
      " |  \n",
      " |  substr(self, startPos: Union[int, ForwardRef('Column')], length: Union[int, ForwardRef('Column')]) -> 'Column'\n",
      " |      Return a :class:`Column` which is a substring of the column.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      startPos : :class:`Column` or int\n",
      " |          start position\n",
      " |      length : :class:`Column` or int\n",
      " |          length of the substring\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.select(df.name.substr(1, 3).alias(\"col\")).collect()\n",
      " |      [Row(col='Ali'), Row(col='Bob')]\n",
      " |  \n",
      " |  when(self, condition: 'Column', value: Any) -> 'Column'\n",
      " |      Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      " |      If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      condition : :class:`Column`\n",
      " |          a boolean :class:`Column` expression.\n",
      " |      value\n",
      " |          a literal value, or a :class:`Column` expression.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import functions as F\n",
      " |      >>> df.select(df.name, F.when(df.age > 4, 1).when(df.age < 3, -1).otherwise(0)).show()\n",
      " |      +-----+------------------------------------------------------------+\n",
      " |      | name|CASE WHEN (age > 4) THEN 1 WHEN (age < 3) THEN -1 ELSE 0 END|\n",
      " |      +-----+------------------------------------------------------------+\n",
      " |      |Alice|                                                          -1|\n",
      " |      |  Bob|                                                           1|\n",
      " |      +-----+------------------------------------------------------------+\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pyspark.sql.functions.when\n",
      " |  \n",
      " |  withField(self, fieldName: str, col: 'Column') -> 'Column'\n",
      " |      An expression that adds/replaces a field in :class:`StructType` by name.\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> from pyspark.sql.functions import lit\n",
      " |      >>> df = spark.createDataFrame([Row(a=Row(b=1, c=2))])\n",
      " |      >>> df.withColumn('a', df['a'].withField('b', lit(3))).select('a.b').show()\n",
      " |      +---+\n",
      " |      |  b|\n",
      " |      +---+\n",
      " |      |  3|\n",
      " |      +---+\n",
      " |      >>> df.withColumn('a', df['a'].withField('d', lit(4))).select('a.d').show()\n",
      " |      +---+\n",
      " |      |  d|\n",
      " |      +---+\n",
      " |      |  4|\n",
      " |      +---+\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(orderDF.order_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e4c929b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+------------+\n",
      "|order_id|         order_date|order_customer_id|order_status|\n",
      "+--------+-------------------+-----------------+------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|      CLOSED|\n",
      "|       3|2013-07-25 00:00:00|            12111|    COMPLETE|\n",
      "+--------+-------------------+-----------------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderDF.where((orderDF.order_status == 'COMPLETE').__or__(orderDF.order_status=='CLOSED')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17250338",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method join in module pyspark.sql.dataframe:\n",
      "\n",
      "join(other: 'DataFrame', on: Union[str, List[str], pyspark.sql.column.Column, List[pyspark.sql.column.Column], NoneType] = None, how: Optional[str] = None) -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Joins with another :class:`DataFrame`, using the given join expression.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    other : :class:`DataFrame`\n",
      "        Right side of the join\n",
      "    on : str, list or :class:`Column`, optional\n",
      "        a string for the join column name, a list of column names,\n",
      "        a join expression (Column), or a list of Columns.\n",
      "        If `on` is a string or a list of strings indicating the name of the join column(s),\n",
      "        the column(s) must exist on both sides, and this performs an equi-join.\n",
      "    how : str, optional\n",
      "        default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n",
      "        ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,\n",
      "        ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,\n",
      "        ``anti``, ``leftanti`` and ``left_anti``.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    The following performs a full outer join between ``df1`` and ``df2``.\n",
      "    \n",
      "    >>> from pyspark.sql.functions import desc\n",
      "    >>> df.join(df2, df.name == df2.name, 'outer').select(df.name, df2.height)                 .sort(desc(\"name\")).collect()\n",
      "    [Row(name='Bob', height=85), Row(name='Alice', height=None), Row(name=None, height=80)]\n",
      "    \n",
      "    >>> df.join(df2, 'name', 'outer').select('name', 'height').sort(desc(\"name\")).collect()\n",
      "    [Row(name='Tom', height=80), Row(name='Bob', height=85), Row(name='Alice', height=None)]\n",
      "    \n",
      "    >>> cond = [df.name == df3.name, df.age == df3.age]\n",
      "    >>> df.join(df3, cond, 'outer').select(df.name, df3.age).collect()\n",
      "    [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      "    \n",
      "    >>> df.join(df2, 'name').select(df.name, df2.height).collect()\n",
      "    [Row(name='Bob', height=85)]\n",
      "    \n",
      "    >>> df.join(df4, ['name', 'age']).select(df.name, df.age).collect()\n",
      "    [Row(name='Bob', age=5)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(orderDF.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2f5186e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orderJoined = orderDF.join(orderItemDF, orderDF.order_id == orderItemDF.order_item_id, 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47044ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(order_id=1, order_date=datetime.datetime(2013, 7, 25, 0, 0), order_customer_id=11599, order_status='CLOSED', order_item_id=1, order_item_order_id=1, product_id=957, qty=1, product_cost=299.98, order_subtotal=299.98),\n",
       " Row(order_id=1, order_date=datetime.datetime(2013, 7, 25, 0, 0), order_customer_id=11599, order_status='CLOSED', order_item_id=1, order_item_order_id=1, product_id=957, qty=1, product_cost=299.98, order_subtotal=299.98)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orderJoined.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e436f108",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method groupBy in module pyspark.sql.dataframe:\n",
      "\n",
      "groupBy(*cols: 'ColumnOrName') -> 'GroupedData' method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Groups the :class:`DataFrame` using the specified columns,\n",
      "    so we can run aggregation on them. See :class:`GroupedData`\n",
      "    for all the available aggregate functions.\n",
      "    \n",
      "    :func:`groupby` is an alias for :func:`groupBy`.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    cols : list, str or :class:`Column`\n",
      "        columns to group by.\n",
      "        Each element should be a column name (string) or an expression (:class:`Column`).\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df.groupBy().avg().collect()\n",
      "    [Row(avg(age)=3.5)]\n",
      "    >>> sorted(df.groupBy('name').agg({'age': 'mean'}).collect())\n",
      "    [Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
      "    >>> sorted(df.groupBy(df.name).avg().collect())\n",
      "    [Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
      "    >>> sorted(df.groupBy(['name', df.age]).count().collect())\n",
      "    [Row(name='Alice', age=2, count=1), Row(name='Bob', age=5, count=1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(orderDF.groupBy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f71a2ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "orderJoinedGroup= orderJoined.groupBy('order_id').agg(sum(col(\"order_subtotal\")).alias('order_total'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e037f206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|order_id|order_total|\n",
      "+--------+-----------+\n",
      "|     148|      99.99|\n",
      "|     463|     399.98|\n",
      "+--------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderJoinedGroup.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a72bda1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method sort in module pyspark.sql.dataframe:\n",
      "\n",
      "sort(*cols: Union[str, pyspark.sql.column.Column, List[Union[str, pyspark.sql.column.Column]]], **kwargs: Any) -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Returns a new :class:`DataFrame` sorted by the specified column(s).\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    cols : str, list, or :class:`Column`, optional\n",
      "         list of :class:`Column` or column names to sort by.\n",
      "    \n",
      "    Other Parameters\n",
      "    ----------------\n",
      "    ascending : bool or list, optional\n",
      "        boolean or list of boolean (default ``True``).\n",
      "        Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      "        If a list is specified, length of the list must equal length of the `cols`.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df.sort(df.age.desc()).collect()\n",
      "    [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "    >>> df.sort(\"age\", ascending=False).collect()\n",
      "    [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "    >>> df.orderBy(df.age.desc()).collect()\n",
      "    [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "    >>> from pyspark.sql.functions import *\n",
      "    >>> df.sort(asc(\"age\")).collect()\n",
      "    [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      "    >>> df.orderBy(desc(\"age\"), \"name\").collect()\n",
      "    [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "    >>> df.orderBy([\"age\", \"name\"], ascending=[0, 1]).collect()\n",
      "    [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(orderJoinedGroup.sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "65a46de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 179:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|order_id|order_total|\n",
      "+--------+-----------+\n",
      "|       1|     599.96|\n",
      "|       9|     599.96|\n",
      "|   18498|     399.98|\n",
      "|   26623|     399.98|\n",
      "|   26708|     399.98|\n",
      "|   27760|     399.98|\n",
      "|   38723|     399.98|\n",
      "|   30361|     399.98|\n",
      "|    1645|     399.98|\n",
      "|   30970|     399.98|\n",
      "|    3749|     399.98|\n",
      "|   29719|     399.98|\n",
      "|    6336|     399.98|\n",
      "|   10206|     399.98|\n",
      "|   30654|     399.98|\n",
      "|   13840|     399.98|\n",
      "|     463|     399.98|\n",
      "|   31912|     399.98|\n",
      "|    1829|     399.98|\n",
      "|   32396|     399.98|\n",
      "+--------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "orderJoinedGroup.sort(col(\"order_total\").desc()).dropna().show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "408821a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method select in module pyspark.sql.dataframe:\n",
      "\n",
      "select(*cols: 'ColumnOrName') -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Projects a set of expressions and returns a new :class:`DataFrame`.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    cols : str, :class:`Column`, or list\n",
      "        column names (string) or expressions (:class:`Column`).\n",
      "        If one of the column names is '*', that column is expanded to include all columns\n",
      "        in the current :class:`DataFrame`.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df.select('*').collect()\n",
      "    [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      "    >>> df.select('name', 'age').collect()\n",
      "    [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      "    >>> df.select(df.name, (df.age + 10).alias('age')).collect()\n",
      "    [Row(name='Alice', age=12), Row(name='Bob', age=15)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(orderDF.select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "092a3b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|order_id|   order_status|\n",
      "+--------+---------------+\n",
      "|       1|         CLOSED|\n",
      "|       2|PENDING_PAYMENT|\n",
      "+--------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderDF.select(col('order_id'),col('order_status')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2650e7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|order_id|   order_status|\n",
      "+--------+---------------+\n",
      "|       1|         CLOSED|\n",
      "|       2|PENDING_PAYMENT|\n",
      "+--------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderDF.select('order_id','order_status').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fac0b678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|order_id|         order_date|\n",
      "+--------+-------------------+\n",
      "|       1|2013-07-25 00:00:00|\n",
      "|       2|2013-07-25 00:00:00|\n",
      "+--------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderDF.select(orderDF.order_id, orderDF.order_date).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6ff599ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Column in module pyspark.sql.column object:\n",
      "\n",
      "class Column(builtins.object)\n",
      " |  Column(jc: py4j.java_gateway.JavaObject) -> None\n",
      " |  \n",
      " |  A column in a DataFrame.\n",
      " |  \n",
      " |  :class:`Column` instances can be created by::\n",
      " |  \n",
      " |      # 1. Select a column out of a DataFrame\n",
      " |  \n",
      " |      df.colName\n",
      " |      df[\"colName\"]\n",
      " |  \n",
      " |      # 2. Create from an expression\n",
      " |      df.colName + 1\n",
      " |      1 / df.colName\n",
      " |  \n",
      " |  .. versionadded:: 1.3.0\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __and__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __bool__ = __nonzero__(self) -> None\n",
      " |  \n",
      " |  __contains__(self, item: Any) -> None\n",
      " |      # container operators\n",
      " |  \n",
      " |  __div__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __eq__(self, other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary function\n",
      " |  \n",
      " |  __ge__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __getattr__(self, item: Any) -> 'Column'\n",
      " |  \n",
      " |  __getitem__(self, k: Any) -> 'Column'\n",
      " |  \n",
      " |  __gt__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __init__(self, jc: py4j.java_gateway.JavaObject) -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __invert__ = _(self: 'Column') -> 'Column'\n",
      " |  \n",
      " |  __iter__(self) -> None\n",
      " |  \n",
      " |  __le__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __lt__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __mod__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __mul__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __ne__(self, other: Any) -> 'Column'\n",
      " |      binary function\n",
      " |  \n",
      " |  __neg__ = _(self: 'Column') -> 'Column'\n",
      " |  \n",
      " |  __nonzero__(self) -> None\n",
      " |  \n",
      " |  __or__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __pow__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      " |      binary function\n",
      " |  \n",
      " |  __radd__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rand__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rdiv__ = _(self: 'Column', other: Union[ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rmod__ = _(self: 'Column', other: Union[ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rmul__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __ror__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rpow__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      " |      binary function\n",
      " |  \n",
      " |  __rsub__ = _(self: 'Column', other: Union[ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rtruediv__ = _(self: 'Column', other: Union[ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __sub__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  __truediv__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      binary operator\n",
      " |  \n",
      " |  alias(self, *alias: str, **kwargs: Any) -> 'Column'\n",
      " |      Returns this column aliased with a new name or names (in the case of expressions that\n",
      " |      return more than one column, such as explode).\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      alias : str\n",
      " |          desired column names (collects all positional arguments passed)\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      metadata: dict\n",
      " |          a dict of information to be stored in ``metadata`` attribute of the\n",
      " |          corresponding :class:`StructField <pyspark.sql.types.StructField>` (optional, keyword\n",
      " |          only argument)\n",
      " |      \n",
      " |          .. versionchanged:: 2.2.0\n",
      " |             Added optional ``metadata`` argument.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.select(df.age.alias(\"age2\")).collect()\n",
      " |      [Row(age2=2), Row(age2=5)]\n",
      " |      >>> df.select(df.age.alias(\"age3\", metadata={'max': 99})).schema['age3'].metadata['max']\n",
      " |      99\n",
      " |  \n",
      " |  asc = _(self: 'Column') -> 'Column'\n",
      " |      Returns a sort expression based on ascending order of the column.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.asc()).collect()\n",
      " |      [Row(name='Alice'), Row(name='Tom')]\n",
      " |  \n",
      " |  asc_nulls_first = _(self: 'Column') -> 'Column'\n",
      " |      Returns a sort expression based on ascending order of the column, and null values\n",
      " |      return before non-null values.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.asc_nulls_first()).collect()\n",
      " |      [Row(name=None), Row(name='Alice'), Row(name='Tom')]\n",
      " |  \n",
      " |  asc_nulls_last = _(self: 'Column') -> 'Column'\n",
      " |      Returns a sort expression based on ascending order of the column, and null values\n",
      " |      appear after non-null values.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.asc_nulls_last()).collect()\n",
      " |      [Row(name='Alice'), Row(name='Tom'), Row(name=None)]\n",
      " |  \n",
      " |  astype = cast(self, dataType)\n",
      " |      :func:`astype` is an alias for :func:`cast`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  between(self, lowerBound: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DateTimeLiteral'), ForwardRef('DecimalLiteral')], upperBound: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DateTimeLiteral'), ForwardRef('DecimalLiteral')]) -> 'Column'\n",
      " |      True if the current column is between the lower bound and upper bound, inclusive.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.select(df.name, df.age.between(2, 4)).show()\n",
      " |      +-----+---------------------------+\n",
      " |      | name|((age >= 2) AND (age <= 4))|\n",
      " |      +-----+---------------------------+\n",
      " |      |Alice|                       true|\n",
      " |      |  Bob|                      false|\n",
      " |      +-----+---------------------------+\n",
      " |  \n",
      " |  bitwiseAND = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      Compute bitwise AND of this expression with another expression.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other\n",
      " |          a value or :class:`Column` to calculate bitwise and(&) with\n",
      " |          this :class:`Column`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(a=170, b=75)])\n",
      " |      >>> df.select(df.a.bitwiseAND(df.b)).collect()\n",
      " |      [Row((a & b)=10)]\n",
      " |  \n",
      " |  bitwiseOR = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      Compute bitwise OR of this expression with another expression.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other\n",
      " |          a value or :class:`Column` to calculate bitwise or(|) with\n",
      " |          this :class:`Column`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(a=170, b=75)])\n",
      " |      >>> df.select(df.a.bitwiseOR(df.b)).collect()\n",
      " |      [Row((a | b)=235)]\n",
      " |  \n",
      " |  bitwiseXOR = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      Compute bitwise XOR of this expression with another expression.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other\n",
      " |          a value or :class:`Column` to calculate bitwise xor(^) with\n",
      " |          this :class:`Column`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(a=170, b=75)])\n",
      " |      >>> df.select(df.a.bitwiseXOR(df.b)).collect()\n",
      " |      [Row((a ^ b)=225)]\n",
      " |  \n",
      " |  cast(self, dataType: Union[pyspark.sql.types.DataType, str]) -> 'Column'\n",
      " |      Casts the column into type ``dataType``.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.select(df.age.cast(\"string\").alias('ages')).collect()\n",
      " |      [Row(ages='2'), Row(ages='5')]\n",
      " |      >>> df.select(df.age.cast(StringType()).alias('ages')).collect()\n",
      " |      [Row(ages='2'), Row(ages='5')]\n",
      " |  \n",
      " |  contains = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      Contains the other element. Returns a boolean :class:`Column` based on a string match.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other\n",
      " |          string in line. A value as a literal or a :class:`Column`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.filter(df.name.contains('o')).collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |  \n",
      " |  desc = _(self: 'Column') -> 'Column'\n",
      " |      Returns a sort expression based on the descending order of the column.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.desc()).collect()\n",
      " |      [Row(name='Tom'), Row(name='Alice')]\n",
      " |  \n",
      " |  desc_nulls_first = _(self: 'Column') -> 'Column'\n",
      " |      Returns a sort expression based on the descending order of the column, and null values\n",
      " |      appear before non-null values.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.desc_nulls_first()).collect()\n",
      " |      [Row(name=None), Row(name='Tom'), Row(name='Alice')]\n",
      " |  \n",
      " |  desc_nulls_last = _(self: 'Column') -> 'Column'\n",
      " |      Returns a sort expression based on the descending order of the column, and null values\n",
      " |      appear after non-null values.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.desc_nulls_last()).collect()\n",
      " |      [Row(name='Tom'), Row(name='Alice'), Row(name=None)]\n",
      " |  \n",
      " |  dropFields(self, *fieldNames: str) -> 'Column'\n",
      " |      An expression that drops fields in :class:`StructType` by name.\n",
      " |      This is a no-op if schema doesn't contain field name(s).\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> from pyspark.sql.functions import col, lit\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     Row(a=Row(b=1, c=2, d=3, e=Row(f=4, g=5, h=6)))])\n",
      " |      >>> df.withColumn('a', df['a'].dropFields('b')).show()\n",
      " |      +-----------------+\n",
      " |      |                a|\n",
      " |      +-----------------+\n",
      " |      |{2, 3, {4, 5, 6}}|\n",
      " |      +-----------------+\n",
      " |      \n",
      " |      >>> df.withColumn('a', df['a'].dropFields('b', 'c')).show()\n",
      " |      +--------------+\n",
      " |      |             a|\n",
      " |      +--------------+\n",
      " |      |{3, {4, 5, 6}}|\n",
      " |      +--------------+\n",
      " |      \n",
      " |      This method supports dropping multiple nested fields directly e.g.\n",
      " |      \n",
      " |      >>> df.withColumn(\"a\", col(\"a\").dropFields(\"e.g\", \"e.h\")).show()\n",
      " |      +--------------+\n",
      " |      |             a|\n",
      " |      +--------------+\n",
      " |      |{1, 2, 3, {4}}|\n",
      " |      +--------------+\n",
      " |      \n",
      " |      However, if you are going to add/replace multiple nested fields,\n",
      " |      it is preferred to extract out the nested struct before\n",
      " |      adding/replacing multiple fields e.g.\n",
      " |      \n",
      " |      >>> df.select(col(\"a\").withField(\n",
      " |      ...     \"e\", col(\"a.e\").dropFields(\"g\", \"h\")).alias(\"a\")\n",
      " |      ... ).show()\n",
      " |      +--------------+\n",
      " |      |             a|\n",
      " |      +--------------+\n",
      " |      |{1, 2, 3, {4}}|\n",
      " |      +--------------+\n",
      " |  \n",
      " |  endswith = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      String ends with. Returns a boolean :class:`Column` based on a string match.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`Column` or str\n",
      " |          string at end of line (do not use a regex `$`)\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.filter(df.name.endswith('ice')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      >>> df.filter(df.name.endswith('ice$')).collect()\n",
      " |      []\n",
      " |  \n",
      " |  eqNullSafe = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      Equality test that is safe for null values.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other\n",
      " |          a value or :class:`Column`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df1 = spark.createDataFrame([\n",
      " |      ...     Row(id=1, value='foo'),\n",
      " |      ...     Row(id=2, value=None)\n",
      " |      ... ])\n",
      " |      >>> df1.select(\n",
      " |      ...     df1['value'] == 'foo',\n",
      " |      ...     df1['value'].eqNullSafe('foo'),\n",
      " |      ...     df1['value'].eqNullSafe(None)\n",
      " |      ... ).show()\n",
      " |      +-------------+---------------+----------------+\n",
      " |      |(value = foo)|(value <=> foo)|(value <=> NULL)|\n",
      " |      +-------------+---------------+----------------+\n",
      " |      |         true|           true|           false|\n",
      " |      |         null|          false|            true|\n",
      " |      +-------------+---------------+----------------+\n",
      " |      >>> df2 = spark.createDataFrame([\n",
      " |      ...     Row(value = 'bar'),\n",
      " |      ...     Row(value = None)\n",
      " |      ... ])\n",
      " |      >>> df1.join(df2, df1[\"value\"] == df2[\"value\"]).count()\n",
      " |      0\n",
      " |      >>> df1.join(df2, df1[\"value\"].eqNullSafe(df2[\"value\"])).count()\n",
      " |      1\n",
      " |      >>> df2 = spark.createDataFrame([\n",
      " |      ...     Row(id=1, value=float('NaN')),\n",
      " |      ...     Row(id=2, value=42.0),\n",
      " |      ...     Row(id=3, value=None)\n",
      " |      ... ])\n",
      " |      >>> df2.select(\n",
      " |      ...     df2['value'].eqNullSafe(None),\n",
      " |      ...     df2['value'].eqNullSafe(float('NaN')),\n",
      " |      ...     df2['value'].eqNullSafe(42.0)\n",
      " |      ... ).show()\n",
      " |      +----------------+---------------+----------------+\n",
      " |      |(value <=> NULL)|(value <=> NaN)|(value <=> 42.0)|\n",
      " |      +----------------+---------------+----------------+\n",
      " |      |           false|           true|           false|\n",
      " |      |           false|          false|            true|\n",
      " |      |            true|          false|           false|\n",
      " |      +----------------+---------------+----------------+\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Unlike Pandas, PySpark doesn't consider NaN values to be NULL. See the\n",
      " |      `NaN Semantics <https://spark.apache.org/docs/latest/sql-ref-datatypes.html#nan-semantics>`_\n",
      " |      for details.\n",
      " |  \n",
      " |  getField(self, name: Any) -> 'Column'\n",
      " |      An expression that gets a field by name in a :class:`StructType`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(r=Row(a=1, b=\"b\"))])\n",
      " |      >>> df.select(df.r.getField(\"b\")).show()\n",
      " |      +---+\n",
      " |      |r.b|\n",
      " |      +---+\n",
      " |      |  b|\n",
      " |      +---+\n",
      " |      >>> df.select(df.r.a).show()\n",
      " |      +---+\n",
      " |      |r.a|\n",
      " |      +---+\n",
      " |      |  1|\n",
      " |      +---+\n",
      " |  \n",
      " |  getItem(self, key: Any) -> 'Column'\n",
      " |      An expression that gets an item at position ``ordinal`` out of a list,\n",
      " |      or gets an item by key out of a dict.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([([1, 2], {\"key\": \"value\"})], [\"l\", \"d\"])\n",
      " |      >>> df.select(df.l.getItem(0), df.d.getItem(\"key\")).show()\n",
      " |      +----+------+\n",
      " |      |l[0]|d[key]|\n",
      " |      +----+------+\n",
      " |      |   1| value|\n",
      " |      +----+------+\n",
      " |  \n",
      " |  ilike = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      SQL ILIKE expression (case insensitive LIKE). Returns a boolean :class:`Column`\n",
      " |      based on a case insensitive match.\n",
      " |      \n",
      " |      .. versionadded:: 3.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : str\n",
      " |          a SQL LIKE pattern\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pyspark.sql.Column.rlike\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.filter(df.name.ilike('%Ice')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  isNotNull = _(self: 'Column') -> 'Column'\n",
      " |      True if the current expression is NOT null.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(name='Tom', height=80), Row(name='Alice', height=None)])\n",
      " |      >>> df.filter(df.height.isNotNull()).collect()\n",
      " |      [Row(name='Tom', height=80)]\n",
      " |  \n",
      " |  isNull = _(self: 'Column') -> 'Column'\n",
      " |      True if the current expression is null.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(name='Tom', height=80), Row(name='Alice', height=None)])\n",
      " |      >>> df.filter(df.height.isNull()).collect()\n",
      " |      [Row(name='Alice', height=None)]\n",
      " |  \n",
      " |  isin(self, *cols: Any) -> 'Column'\n",
      " |      A boolean expression that is evaluated to true if the value of this\n",
      " |      expression is contained by the evaluated values of the arguments.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df[df.name.isin(\"Bob\", \"Mike\")].collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      >>> df[df.age.isin([1, 2, 3])].collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  like = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      SQL like expression. Returns a boolean :class:`Column` based on a SQL LIKE match.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : str\n",
      " |          a SQL LIKE pattern\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pyspark.sql.Column.rlike\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.filter(df.name.like('Al%')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  name = alias(self, *alias, **kwargs)\n",
      " |      :func:`name` is an alias for :func:`alias`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  otherwise(self, value: Any) -> 'Column'\n",
      " |      Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      " |      If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      value\n",
      " |          a literal value, or a :class:`Column` expression.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import functions as F\n",
      " |      >>> df.select(df.name, F.when(df.age > 3, 1).otherwise(0)).show()\n",
      " |      +-----+-------------------------------------+\n",
      " |      | name|CASE WHEN (age > 3) THEN 1 ELSE 0 END|\n",
      " |      +-----+-------------------------------------+\n",
      " |      |Alice|                                    0|\n",
      " |      |  Bob|                                    1|\n",
      " |      +-----+-------------------------------------+\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pyspark.sql.functions.when\n",
      " |  \n",
      " |  over(self, window: 'WindowSpec') -> 'Column'\n",
      " |      Define a windowing column.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      window : :class:`WindowSpec`\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`Column`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Window\n",
      " |      >>> window = Window.partitionBy(\"name\").orderBy(\"age\")                 .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
      " |      >>> from pyspark.sql.functions import rank, min\n",
      " |      >>> from pyspark.sql.functions import desc\n",
      " |      >>> df.withColumn(\"rank\", rank().over(window))                 .withColumn(\"min\", min('age').over(window)).sort(desc(\"age\")).show()\n",
      " |      +---+-----+----+---+\n",
      " |      |age| name|rank|min|\n",
      " |      +---+-----+----+---+\n",
      " |      |  5|  Bob|   1|  5|\n",
      " |      |  2|Alice|   1|  2|\n",
      " |      +---+-----+----+---+\n",
      " |  \n",
      " |  rlike = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      SQL RLIKE expression (LIKE with Regex). Returns a boolean :class:`Column` based on a regex\n",
      " |      match.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : str\n",
      " |          an extended regex expression\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.filter(df.name.rlike('ice$')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  startswith = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n",
      " |      String starts with. Returns a boolean :class:`Column` based on a string match.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`Column` or str\n",
      " |          string at start of line (do not use a regex `^`)\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.filter(df.name.startswith('Al')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      >>> df.filter(df.name.startswith('^Al')).collect()\n",
      " |      []\n",
      " |  \n",
      " |  substr(self, startPos: Union[int, ForwardRef('Column')], length: Union[int, ForwardRef('Column')]) -> 'Column'\n",
      " |      Return a :class:`Column` which is a substring of the column.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      startPos : :class:`Column` or int\n",
      " |          start position\n",
      " |      length : :class:`Column` or int\n",
      " |          length of the substring\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.select(df.name.substr(1, 3).alias(\"col\")).collect()\n",
      " |      [Row(col='Ali'), Row(col='Bob')]\n",
      " |  \n",
      " |  when(self, condition: 'Column', value: Any) -> 'Column'\n",
      " |      Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      " |      If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      condition : :class:`Column`\n",
      " |          a boolean :class:`Column` expression.\n",
      " |      value\n",
      " |          a literal value, or a :class:`Column` expression.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import functions as F\n",
      " |      >>> df.select(df.name, F.when(df.age > 4, 1).when(df.age < 3, -1).otherwise(0)).show()\n",
      " |      +-----+------------------------------------------------------------+\n",
      " |      | name|CASE WHEN (age > 4) THEN 1 WHEN (age < 3) THEN -1 ELSE 0 END|\n",
      " |      +-----+------------------------------------------------------------+\n",
      " |      |Alice|                                                          -1|\n",
      " |      |  Bob|                                                           1|\n",
      " |      +-----+------------------------------------------------------------+\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pyspark.sql.functions.when\n",
      " |  \n",
      " |  withField(self, fieldName: str, col: 'Column') -> 'Column'\n",
      " |      An expression that adds/replaces a field in :class:`StructType` by name.\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> from pyspark.sql.functions import lit\n",
      " |      >>> df = spark.createDataFrame([Row(a=Row(b=1, c=2))])\n",
      " |      >>> df.withColumn('a', df['a'].withField('b', lit(3))).select('a.b').show()\n",
      " |      +---+\n",
      " |      |  b|\n",
      " |      +---+\n",
      " |      |  3|\n",
      " |      +---+\n",
      " |      >>> df.withColumn('a', df['a'].withField('d', lit(4))).select('a.d').show()\n",
      " |      +---+\n",
      " |      |  d|\n",
      " |      +---+\n",
      " |      |  4|\n",
      " |      +---+\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(date_format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3df52f91",
   "metadata": {},
   "outputs": [
    {
     "ename": "SparkUpgradeException",
     "evalue": "You may get a different result due to the upgrading to Spark >= 3.0: Fail to recognize 'YYYYMM' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSparkUpgradeException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43morderDF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselectExpr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate_format(order_date, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mYYYYMM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m) AS order_month\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 606\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mSparkUpgradeException\u001b[0m: You may get a different result due to the upgrading to Spark >= 3.0: Fail to recognize 'YYYYMM' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html"
     ]
    }
   ],
   "source": [
    "orderDF.selectExpr(\"date_format(order_date, 'YYYYMM') AS order_month\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dacc6683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(orders_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f072673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(order_id=1, order_date=datetime.datetime(2013, 7, 25, 0, 0), order_customer_id=11599, order_status='CLOSED'),\n",
       " Row(order_id=2, order_date=datetime.datetime(2013, 7, 25, 0, 0), order_customer_id=256, order_status='PENDING_PAYMENT')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "201e231a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "007eeec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "63ab5703",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"jdbc:postgresql://localhost/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "823daa4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method jdbc in module pyspark.sql.readwriter:\n",
      "\n",
      "jdbc(url: str, table: str, column: Optional[str] = None, lowerBound: Union[str, int, NoneType] = None, upperBound: Union[str, int, NoneType] = None, numPartitions: Optional[int] = None, predicates: Optional[List[str]] = None, properties: Optional[Dict[str, str]] = None) -> 'DataFrame' method of pyspark.sql.readwriter.DataFrameReader instance\n",
      "    Construct a :class:`DataFrame` representing the database table named ``table``\n",
      "    accessible via JDBC URL ``url`` and connection ``properties``.\n",
      "    \n",
      "    Partitions of the table will be retrieved in parallel if either ``column`` or\n",
      "    ``predicates`` is specified. ``lowerBound``, ``upperBound`` and ``numPartitions``\n",
      "    is needed when ``column`` is specified.\n",
      "    \n",
      "    If both ``column`` and ``predicates`` are specified, ``column`` will be used.\n",
      "    \n",
      "    .. versionadded:: 1.4.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    table : str\n",
      "        the name of the table\n",
      "    column : str, optional\n",
      "        alias of ``partitionColumn`` option. Refer to ``partitionColumn`` in\n",
      "        `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n",
      "        in the version you use.\n",
      "    predicates : list, optional\n",
      "        a list of expressions suitable for inclusion in WHERE clauses;\n",
      "        each one defines one partition of the :class:`DataFrame`\n",
      "    properties : dict, optional\n",
      "        a dictionary of JDBC database connection arguments. Normally at\n",
      "        least properties \"user\" and \"password\" with their corresponding values.\n",
      "        For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n",
      "    \n",
      "    Other Parameters\n",
      "    ----------------\n",
      "    Extra options\n",
      "        For the extra options, refer to\n",
      "        `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n",
      "        in the version you use.\n",
      "    \n",
      "        .. # noqa\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Don't create too many partitions in parallel on a large cluster;\n",
      "    otherwise Spark might crash your external database systems.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`DataFrame`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sparkSQL.read.jdbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4fcf2e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "orders_df.write.csv('ordersdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "776d8687",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remember to give the alias to the table that is created with select\n",
    "ordersQuery = sparkSQL.read.format('jdbc') \\\n",
    "        .option('url',\"jdbc:postgresql://localhost/postgres\") \\\n",
    "        .option('dbtable','(SELECT * FROM orders LIMIT 5) q') \\\n",
    "        .option('user','postgres') \\\n",
    "        .option('password',1234) \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "92a99a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordersQuery.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "43e83bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8b861ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b244373",
   "metadata": {},
   "outputs": [],
   "source": [
    "orderitems= sparkSQL.read.format('jdbc') \\\n",
    "        .option('url',\"jdbc:postgresql://localhost/postgres\") \\\n",
    "        .option('dbtable','order_items') \\\n",
    "        .option('user','postgres') \\\n",
    "        .option('password',1234) \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4c8d6122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_item_id: integer (nullable = true)\n",
      " |-- order_item_order_id: integer (nullable = true)\n",
      " |-- order_item_product_id: integer (nullable = true)\n",
      " |-- order_item_quantity: integer (nullable = true)\n",
      " |-- order_item_subtotal: double (nullable = true)\n",
      " |-- order_item_product_price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderitems.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6ce4afec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|order_item_quantity|order_item_subtotal|\n",
      "+-------------------+-------------------+\n",
      "|                  1|             299.98|\n",
      "|                  1|             199.99|\n",
      "|                  5|              250.0|\n",
      "|                  1|             129.99|\n",
      "|                  2|              49.98|\n",
      "|                  5|             299.95|\n",
      "|                  3|              150.0|\n",
      "|                  4|             199.92|\n",
      "|                  1|             299.98|\n",
      "|                  5|             299.95|\n",
      "|                  2|              99.96|\n",
      "|                  1|             299.98|\n",
      "|                  1|             129.99|\n",
      "|                  1|             199.99|\n",
      "|                  1|             299.98|\n",
      "|                  5|              79.95|\n",
      "|                  3|             179.97|\n",
      "|                  5|             299.95|\n",
      "|                  4|             199.92|\n",
      "|                  1|               50.0|\n",
      "+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderitems.select(\"order_item_quantity\", \"order_item_subtotal\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7c697bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "productTable = sparkSQL.read.format('jdbc') \\\n",
    "        .option('url',\"jdbc:postgresql://localhost/postgres\") \\\n",
    "        .option('dbtable','products') \\\n",
    "        .option('user','postgres') \\\n",
    "        .option('password',1234) \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f12907ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+--------------------+-------------------+-------------+--------------------+\n",
      "|product_id|product_category_id|        product_name|product_description|product_price|       product_image|\n",
      "+----------+-------------------+--------------------+-------------------+-------------+--------------------+\n",
      "|         1|                  2|Quest Q64 10 FT. ...|                   |        59.98|http://images.acm...|\n",
      "|         2|                  2|Under Armour Men'...|                   |       129.99|http://images.acm...|\n",
      "+----------+-------------------+--------------------+-------------------+-------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "productTable.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dd77963f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------+\n",
      "|CASE WHEN (order_status IN (COMPLETE, CLOSED)) THEN COMPLETED ELSE PENDING END|\n",
      "+------------------------------------------------------------------------------+\n",
      "|                                                                     COMPLETED|\n",
      "|                                                                       PENDING|\n",
      "+------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.selectExpr(\"CASE WHEN order_status IN ('COMPLETE','CLOSED') THEN 'COMPLETED' ELSE 'PENDING' END\").show(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
