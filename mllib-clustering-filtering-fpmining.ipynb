{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kamaljp/mllib-clustering-filtering-fpmining?scriptVersionId=112202353\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"### Clustering Filtering and Frequency Pattern Mining\n\nThis notebook brings out the most used algorithms in the wild, after the classification and regression. The full explanations can be found in the MLlib [official documentation here](https://spark.apache.org/docs/latest/ml-guide.html)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"### Collaborative filtering\n\nCollaborative filtering is commonly used for recommender systems. These techniques aim to fill in the missing entries of a user-item association matrix. spark.ml currently supports model-based collaborative filtering, in which users and products are described by a small set of latent factors that can be used to predict missing entries. spark.ml uses the alternating least squares (ALS) algorithm to learn these latent factors. The implementation in spark.ml has the following parameters:\n\n- numBlocks is the number of blocks the users and items will be partitioned into in order to parallelize computation (defaults to 10).\n\n- rank is the number of latent factors in the model (defaults to 10).\n\n- maxIter is the maximum number of iterations to run (defaults to 10).\n\n- regParam specifies the regularization parameter in ALS (defaults to 1.0).\n\n- implicitPrefs specifies whether to use the explicit feedback ALS variant or one adapted for implicit feedback data (defaults to false which means using explicit feedback).\n\n- alpha is a parameter applicable to the implicit feedback variant of ALS that governs the baseline confidence in preference observations (defaults to 1.0).\n\n- nonnegative specifies whether or not to use nonnegative constraints for least squares (defaults to false).\n\nNote: The DataFrame-based API for ALS currently only supports integers for user and item ids. Other numeric types are supported for the user and item id columns, but the ids must be within the integer value range.","metadata":{}},{"cell_type":"markdown","source":"### Cold-start strategy\n\nWhen making predictions using an ALSModel, it is common to encounter users and/or items in the test dataset that were not present during training the model. This typically occurs in two scenarios:\n\nIn production, for new users or items that have no rating history and on which the model has not been trained (this is the “cold start problem”).\nDuring cross-validation, the data is split between training and evaluation sets. When using simple random splits as in Spark’s CrossValidator or TrainValidationSplit, it is actually very common to encounter users and/or items in the evaluation set that are not in the training set\n\nBy default, Spark assigns NaN predictions during ALSModel.transform when a user and/or item factor is not present in the model. This can be useful in a production system, since it indicates a new user or item, and so the system can make a decision on some fallback to use as the prediction.\n\nHowever, this is undesirable during cross-validation, since any NaN predicted values will result in NaN results for the evaluation metric (for example when using RegressionEvaluator). This makes model selection impossible.\n\nSpark allows users to set the coldStartStrategy parameter to “drop” in order to drop any rows in the DataFrame of predictions that contain NaN values. The evaluation metric will then be computed over the non-NaN data and will be valid. Usage of this parameter is illustrated in the example below.","metadata":{}},{"cell_type":"code","source":"!pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2022-11-27T07:53:14.146294Z","iopub.execute_input":"2022-11-27T07:53:14.146773Z","iopub.status.idle":"2022-11-27T07:54:20.437605Z","shell.execute_reply.started":"2022-11-27T07:53:14.146685Z","shell.execute_reply":"2022-11-27T07:54:20.43611Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting py4j==0.10.9.5\n  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845513 sha256=6a4819faaff7ff486c58d32369d9b03f08a96f5c2d011078a6dae0c2e67705e3\n  Stored in directory: /root/.cache/pip/wheels/42/59/f5/79a5bf931714dcd201b26025347785f087370a10a3329a899c\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark\n  Attempting uninstall: py4j\n    Found existing installation: py4j 0.10.9.7\n    Uninstalling py4j-0.10.9.7:\n      Successfully uninstalled py4j-0.10.9.7\nSuccessfully installed py4j-0.10.9.5 pyspark-3.3.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"fpc\").getOrCreate()","metadata":{"execution":{"iopub.status.busy":"2022-11-27T07:54:27.848317Z","iopub.execute_input":"2022-11-27T07:54:27.848724Z","iopub.status.idle":"2022-11-27T07:54:33.454291Z","shell.execute_reply.started":"2022-11-27T07:54:27.848693Z","shell.execute_reply":"2022-11-27T07:54:33.452822Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","output_type":"stream"},{"name":"stdout","text":"22/11/27 07:54:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.sql import Row","metadata":{"execution":{"iopub.status.busy":"2022-11-27T07:54:33.457283Z","iopub.execute_input":"2022-11-27T07:54:33.458189Z","iopub.status.idle":"2022-11-27T07:54:33.566034Z","shell.execute_reply.started":"2022-11-27T07:54:33.458129Z","shell.execute_reply":"2022-11-27T07:54:33.564816Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"lines = spark.read.text(\"/kaggle/input/mllib-datasets/sample_movielens_ratings.txt\").rdd\nparts = lines.map(lambda row: row.value.split(\"::\"))\nratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]),\n                                     rating=float(p[2]), timestamp=int(p[3])))\nratings = spark.createDataFrame(ratingsRDD)\n(training, test) = ratings.randomSplit([0.8, 0.2])\n\n# Build the recommendation model using ALS on the training data\n# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\nals = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n          coldStartStrategy=\"drop\")\nmodel = als.fit(training)\n\n# Evaluate the model by computing the RMSE on the test data\npredictions = model.transform(test)\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n                                predictionCol=\"prediction\")\nrmse = evaluator.evaluate(predictions)\nprint(\"Root-mean-square error = \" + str(rmse))\n\n# Generate top 10 movie recommendations for each user\nuserRecs = model.recommendForAllUsers(10)\n# Generate top 10 user recommendations for each movie\nmovieRecs = model.recommendForAllItems(10)\n\n# Generate top 10 movie recommendations for a specified set of users\nusers = ratings.select(als.getUserCol()).distinct().limit(3)\nuserSubsetRecs = model.recommendForUserSubset(users, 10)\n# Generate top 10 user recommendations for a specified set of movies\nmovies = ratings.select(als.getItemCol()).distinct().limit(3)\nmovieSubSetRecs = model.recommendForItemSubset(movies, 10)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T07:55:06.040274Z","iopub.execute_input":"2022-11-27T07:55:06.040714Z","iopub.status.idle":"2022-11-27T07:55:22.147182Z","shell.execute_reply.started":"2022-11-27T07:55:06.040678Z","shell.execute_reply":"2022-11-27T07:55:22.145994Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Root-mean-square error = 2.014127008497032\n","output_type":"stream"}]},{"cell_type":"code","source":"movieRecs.show(2)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T08:30:14.941182Z","iopub.execute_input":"2022-11-27T08:30:14.941638Z","iopub.status.idle":"2022-11-27T08:30:17.042299Z","shell.execute_reply.started":"2022-11-27T08:30:14.941606Z","shell.execute_reply":"2022-11-27T08:30:17.040522Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"[Stage 126:==============================================>       (87 + 4) / 100]\r","output_type":"stream"},{"name":"stdout","text":"+-------+--------------------+\n|movieId|     recommendations|\n+-------+--------------------+\n|     20|[{17, 4.8588567},...|\n|     40|[{2, 3.855298}, {...|\n+-------+--------------------+\nonly showing top 2 rows\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"markdown","source":"## K-means\n\nk-means is one of the most commonly used clustering algorithms that clusters the data points into a predefined number of clusters. The MLlib implementation includes a parallelized variant of the k-means++ method called kmeans||.","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.clustering import KMeans,LDA,BisectingKMeans,GaussianMixture, PowerIterationClustering\nfrom pyspark.ml.evaluation import ClusteringEvaluator","metadata":{"execution":{"iopub.status.busy":"2022-11-27T08:49:35.592364Z","iopub.execute_input":"2022-11-27T08:49:35.592834Z","iopub.status.idle":"2022-11-27T08:49:35.597839Z","shell.execute_reply.started":"2022-11-27T08:49:35.592795Z","shell.execute_reply":"2022-11-27T08:49:35.596976Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Loads data.\ndataset = spark.read.format(\"libsvm\").load(\"/kaggle/input/mllib-datasets/sample_kmeans_data.txt\")\n\n# Trains a k-means model.\nkmeans = KMeans().setK(2).setSeed(1)\nmodel = kmeans.fit(dataset)\n\n# Make predictions\npredictions = model.transform(dataset)\n\n# Evaluate clustering by computing Silhouette score\nevaluator = ClusteringEvaluator()\n\nsilhouette = evaluator.evaluate(predictions)\nprint(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n\n# Shows the result.\ncenters = model.clusterCenters()\nprint(\"Cluster Centers: \")\nfor center in centers:\n    print(center)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T08:42:20.400277Z","iopub.execute_input":"2022-11-27T08:42:20.400701Z","iopub.status.idle":"2022-11-27T08:42:23.08667Z","shell.execute_reply.started":"2022-11-27T08:42:20.40067Z","shell.execute_reply":"2022-11-27T08:42:23.08551Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"22/11/27 08:42:20 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\nSilhouette with squared euclidean distance = 0.9997530305375207\nCluster Centers: \n[9.1 9.1 9.1]\n[0.1 0.1 0.1]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Latent Dirichlet allocation (LDA)\n\nLDA is implemented as an Estimator that supports both EMLDAOptimizer and OnlineLDAOptimizer, and generates a LDAModel as the base model. Expert users may cast a LDAModel generated by EMLDAOptimizer to a DistributedLDAModel if needed.","metadata":{}},{"cell_type":"code","source":"# Loads data.\ndataset = spark.read.format(\"libsvm\").load(\"/kaggle/input/mllib-datasets/sample_lda_libsvm_data.txt\")\n\n# Trains a LDA model.\nlda = LDA(k=10, maxIter=10)\nmodel = lda.fit(dataset)\n\nll = model.logLikelihood(dataset)\nlp = model.logPerplexity(dataset)\nprint(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\nprint(\"The upper bound on perplexity: \" + str(lp))\n\n# Describe topics.\ntopics = model.describeTopics(3)\nprint(\"The topics described by their top-weighted terms:\")\ntopics.show(truncate=False)\n\n# Shows the result\ntransformed = model.transform(dataset)\ntransformed.show(truncate=False)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T08:45:56.783029Z","iopub.execute_input":"2022-11-27T08:45:56.783494Z","iopub.status.idle":"2022-11-27T08:45:59.759494Z","shell.execute_reply.started":"2022-11-27T08:45:56.783459Z","shell.execute_reply":"2022-11-27T08:45:59.758339Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"22/11/27 08:45:56 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n22/11/27 08:45:58 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n22/11/27 08:45:58 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\nThe lower bound on the log likelihood of the entire corpus: -812.0929503197723\nThe upper bound on perplexity: 3.1234344243068164\nThe topics described by their top-weighted terms:\n+-----+-----------+---------------------------------------------------------------+\n|topic|termIndices|termWeights                                                    |\n+-----+-----------+---------------------------------------------------------------+\n|0    |[4, 1, 2]  |[0.10187655816539348, 0.10075695037550687, 0.09684803108559074]|\n|1    |[8, 6, 1]  |[0.10548089860006415, 0.10496801280997388, 0.09542000997382412]|\n|2    |[7, 3, 6]  |[0.1042311794621062, 0.09692303180373728, 0.09466832871846075] |\n|3    |[0, 10, 1] |[0.10054608747479737, 0.10004895260287626, 0.09762321012945942]|\n|4    |[0, 4, 6]  |[0.11056777155468063, 0.10300552907894556, 0.096271584535885]  |\n|5    |[6, 8, 5]  |[0.1163700203192149, 0.11012152602914499, 0.10372928197829465] |\n|6    |[8, 9, 3]  |[0.1145834554270615, 0.1097451375843496, 0.09561032582625444]  |\n|7    |[6, 5, 8]  |[0.10834871269357194, 0.10027643673946986, 0.09946416711499119]|\n|8    |[6, 10, 1] |[0.2052599154722762, 0.1664967568176489, 0.1192783573413433]   |\n|9    |[5, 4, 9]  |[0.1628628358260565, 0.14277945052691035, 0.13622313365837543] |\n+-----+-----------+---------------------------------------------------------------+\n\n+-----+---------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|label|features                                                       |topicDistribution                                                                                                                                                                                                     |\n+-----+---------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|0.0  |(11,[0,1,2,4,5,6,7,10],[1.0,2.0,6.0,2.0,3.0,1.0,1.0,3.0])      |[0.004779599535770422,0.004779498281775203,0.004779426865279169,0.0047794006174554605,0.004779445963946917,0.0047793784334843,0.004779293617057973,0.00477945938796326,0.4001074877850297,0.5616570095122375]         |\n|1.0  |(11,[0,1,3,4,7,10],[1.0,3.0,1.0,3.0,2.0,1.0])                  |[0.007971987701622432,0.00797175788103354,0.007971979508893787,0.007971925824280648,0.007971893517948147,0.007971808384334282,0.007971754396827645,0.007971868901845485,0.008749329679914925,0.9274756942032991]      |\n|2.0  |(11,[0,1,2,5,6,8,9],[1.0,4.0,1.0,4.0,9.0,1.0,2.0])             |[0.004155176019138158,0.004155203376829213,0.004155162742598084,0.0041551480240973404,0.004155141144612404,0.004155235896983388,0.004155117997936774,0.004155228712993579,0.6728203553701677,0.2939382307146435]      |\n|3.0  |(11,[0,1,3,6,8,9,10],[2.0,1.0,3.0,5.0,2.0,3.0,9.0])            |[0.003675234137221753,0.0036752320379403597,0.003675272459585467,0.0036752538564533165,0.0036752550606097027,0.0036752332711702837,0.0036752447061174694,0.00367525499145557,0.9664127113717281,0.0041853081077178915]|\n|4.0  |(11,[0,1,2,3,4,6,9,10],[3.0,1.0,1.0,9.0,3.0,2.0,1.0,3.0])      |[0.003982006422833086,0.003981972687943059,0.003982055162934932,0.003981982004980483,0.003982047613876551,0.003981975850172121,0.0039819928089552675,0.003982042427476757,0.6444877984125128,0.323656126608315]       |\n|5.0  |(11,[0,1,3,4,5,6,7,8,9],[4.0,2.0,3.0,4.0,5.0,1.0,1.0,1.0,4.0]) |[0.00367530440482237,0.0036753100297932644,0.0036753332481989217,0.003675289234551249,0.0036753180143780017,0.003675337513665001,0.003675318675091584,0.003675332545305001,0.004031822043107149,0.9665656342910874]   |\n|6.0  |(11,[0,1,3,6,8,9,10],[2.0,1.0,3.0,5.0,2.0,2.0,9.0])            |[0.003822378569985716,0.003822377287709493,0.003822417403310558,0.003822400642247162,0.003822399501164915,0.0038223773014121913,0.0038223836475085485,0.0038224021755816477,0.965068091583353,0.004352771887726759]   |\n|7.0  |(11,[0,1,2,3,4,5,6,9,10],[1.0,1.0,1.0,9.0,2.0,1.0,2.0,1.0,3.0])|[0.00434438898355629,0.004344355316629327,0.004344454257291959,0.004344363513529331,0.004344418220282564,0.004344388902219172,0.004344392348482398,0.004344456659212246,0.7573286608042888,0.20791612099450787]       |\n|8.0  |(11,[0,1,3,4,5,6,7],[4.0,4.0,3.0,4.0,2.0,1.0,3.0])             |[0.0043443195583715965,0.004344295173558983,0.004344338870003851,0.004344275430874754,0.004344373016185171,0.004344310930949862,0.004344258569279843,0.004344308715863422,0.004765735062485239,0.9604797846724272]    |\n|9.0  |(11,[0,1,2,4,6,8,9,10],[2.0,8.0,2.0,3.0,2.0,2.0,7.0,2.0])      |[0.0032948640217928694,0.003294840669933305,0.0032948145564150903,0.003294806332137364,0.00329481900185513,0.003294822524753011,0.003294811195333221,0.003294836984824627,0.4363948163274926,0.5372465683854628]      |\n|10.0 |(11,[0,1,2,3,5,6,9,10],[1.0,1.0,1.0,9.0,2.0,2.0,3.0,3.0])      |[0.004155282736064479,0.00415527913735444,0.004155388343049935,0.004155309309876033,0.004155305387830202,0.004155315940568894,0.004155331013255636,0.004155377721750988,0.9620222589524715,0.004735151457778036]      |\n|11.0 |(11,[0,1,4,5,6,7,9],[4.0,1.0,4.0,5.0,1.0,3.0,1.0])             |[0.004779167501543745,0.00477914215251958,0.0047791510075575715,0.004779103498040384,0.00477918760290429,0.004779182988685892,0.004779076618893035,0.004779122231623882,0.0052421543470598225,0.9565247120511718]     |\n+-----+---------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Bisecting k-means\n\nBisecting k-means is a kind of hierarchical clustering using a divisive (or “top-down”) approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.\n\nBisecting K-means can often be much faster than regular K-means, but it will generally produce a different clustering.\n\nBisectingKMeans is implemented as an Estimator and generates a BisectingKMeansModel as the base model.","metadata":{}},{"cell_type":"code","source":"dataset = spark.read.format(\"libsvm\").load(\"/kaggle/input/mllib-datasets/sample_kmeans_data.txt\")\n\n# Trains a bisecting k-means model.\nbkm = BisectingKMeans().setK(2).setSeed(1)\nmodel = bkm.fit(dataset)\n\n# Make predictions\npredictions = model.transform(dataset)\n\n# Evaluate clustering by computing Silhouette score\nevaluator = ClusteringEvaluator()\n\nsilhouette = evaluator.evaluate(predictions)\nprint(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n\n# Shows the result.\nprint(\"Cluster Centers: \")\ncenters = model.clusterCenters()\nfor center in centers:\n    print(center)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T08:46:41.639077Z","iopub.execute_input":"2022-11-27T08:46:41.639497Z","iopub.status.idle":"2022-11-27T08:46:44.547749Z","shell.execute_reply.started":"2022-11-27T08:46:41.639466Z","shell.execute_reply":"2022-11-27T08:46:44.546489Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"22/11/27 08:46:41 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\nSilhouette with squared euclidean distance = 0.9997530305375207\nCluster Centers: \n[0.1 0.1 0.1]\n[9.1 9.1 9.1]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### A Gaussian Mixture Model \n\nRepresents a composite distribution whereby points are drawn from one of k Gaussian sub-distributions, each with its own probability. The spark.ml implementation uses the expectation-maximization algorithm to induce the maximum-likelihood model given a set of samples.\n\nGaussianMixture is implemented as an Estimator and generates a GaussianMixtureModel as the base model.","metadata":{}},{"cell_type":"code","source":"# loads data\ndataset = spark.read.format(\"libsvm\").load(\"/kaggle/input/mllib-datasets/sample_kmeans_data.txt\")\n\ngmm = GaussianMixture().setK(2).setSeed(538009335)\nmodel = gmm.fit(dataset)\n\nprint(\"Gaussians shown as a DataFrame: \")\nmodel.gaussiansDF.show(truncate=False)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T08:48:47.858071Z","iopub.execute_input":"2022-11-27T08:48:47.858547Z","iopub.status.idle":"2022-11-27T08:48:49.827073Z","shell.execute_reply.started":"2022-11-27T08:48:47.858513Z","shell.execute_reply":"2022-11-27T08:48:49.82617Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"22/11/27 08:48:47 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n22/11/27 08:48:48 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n22/11/27 08:48:48 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\nGaussians shown as a DataFrame: \n+-------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|mean                                                         |cov                                                                                                                                                                                                       |\n+-------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|[0.10000000000001552,0.10000000000001552,0.10000000000001552]|0.006666666666806454  0.006666666666806454  0.006666666666806454  \\n0.006666666666806454  0.006666666666806454  0.006666666666806454  \\n0.006666666666806454  0.006666666666806454  0.006666666666806454  |\n|[9.099999999999984,9.099999999999984,9.099999999999984]      |0.006666666666812185  0.006666666666812185  0.006666666666812185  \\n0.006666666666812185  0.006666666666812185  0.006666666666812185  \\n0.006666666666812185  0.006666666666812185  0.006666666666812185  |\n+-------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Power Iteration Clustering (PIC)\n\nPower Iteration Clustering (PIC) is a scalable graph clustering algorithm developed by Lin and Cohen. From the abstract: PIC finds a very low-dimensional embedding of a dataset using truncated power iteration on a normalized pair-wise similarity matrix of the data.\n\nspark.ml’s PowerIterationClustering implementation takes the following parameters:\n\nk: the number of clusters to create\n\ninitMode: param for the initialization algorithm\n\nmaxIter: param for maximum number of iterations\n\nsrcCol: param for the name of the input column for source vertex IDs\n\ndstCol: name of the input column for destination vertex IDs\n\nweightCol: Param for weight column name","metadata":{}},{"cell_type":"code","source":"df = spark.createDataFrame([\n    (0, 1, 1.0),\n    (0, 2, 1.0),\n    (1, 2, 1.0),\n    (3, 4, 1.0),\n    (4, 0, 0.1)\n], [\"src\", \"dst\", \"weight\"])\n\npic = PowerIterationClustering(k=2, maxIter=20, initMode=\"degree\", weightCol=\"weight\")\n\n# Shows the cluster assignment\npic.assignClusters(df).show()","metadata":{"execution":{"iopub.status.busy":"2022-11-27T08:49:38.772537Z","iopub.execute_input":"2022-11-27T08:49:38.774041Z","iopub.status.idle":"2022-11-27T08:49:48.92679Z","shell.execute_reply.started":"2022-11-27T08:49:38.773979Z","shell.execute_reply":"2022-11-27T08:49:48.924217Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"+---+-------+\n| id|cluster|\n+---+-------+\n|  4|      0|\n|  0|      1|\n|  1|      1|\n|  2|      1|\n|  3|      0|\n+---+-------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}