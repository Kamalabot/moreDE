{"metadata":{"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":5,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kamaljp/mllib-clusteringregression-part2?scriptVersionId=112194441\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"### Clustering, Classification and Regression\n\nThe notebook brings together all the algorithms used for classification and regression challenges. The features has been selected in the [MLLib part1](MLlib_training_part1) notebook. The logical next step is covering the fitting and prediction using the models","metadata":{}},{"cell_type":"code","source":"!pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2022-11-27T05:40:41.440233Z","iopub.execute_input":"2022-11-27T05:40:41.440647Z","iopub.status.idle":"2022-11-27T05:41:32.117112Z","shell.execute_reply.started":"2022-11-27T05:40:41.440615Z","shell.execute_reply":"2022-11-27T05:41:32.115985Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting py4j==0.10.9.5\n  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845513 sha256=1b72b7d88f59ed103311a68d005373702fac3c3a569bec5525d34caba20383a8\n  Stored in directory: /root/.cache/pip/wheels/42/59/f5/79a5bf931714dcd201b26025347785f087370a10a3329a899c\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark\n  Attempting uninstall: py4j\n    Found existing installation: py4j 0.10.9.7\n    Uninstalling py4j-0.10.9.7:\n      Successfully uninstalled py4j-0.10.9.7\nSuccessfully installed py4j-0.10.9.5 pyspark-3.3.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import pyspark\nfrom pyspark.sql import SparkSession ","metadata":{"execution":{"iopub.status.busy":"2022-11-27T05:41:32.118903Z","iopub.execute_input":"2022-11-27T05:41:32.119213Z","iopub.status.idle":"2022-11-27T05:41:32.1918Z","shell.execute_reply.started":"2022-11-27T05:41:32.119182Z","shell.execute_reply":"2022-11-27T05:41:32.190859Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#Initiating Pyspark\nspark = SparkSession.builder.appName(\"CCRModels\").getOrCreate()","metadata":{"execution":{"iopub.status.busy":"2022-11-27T05:41:32.192936Z","iopub.execute_input":"2022-11-27T05:41:32.19322Z","iopub.status.idle":"2022-11-27T05:41:37.362987Z","shell.execute_reply.started":"2022-11-27T05:41:32.193194Z","shell.execute_reply":"2022-11-27T05:41:37.361674Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","output_type":"stream"},{"name":"stdout","text":"22/11/27 05:41:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}]},{"cell_type":"code","source":"#The classification algorithms are under the ml.classification\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import DecisionTreeClassifier, LogisticRegression\nfrom pyspark.ml.classification import RandomForestClassifier,GBTClassifier\nfrom pyspark.ml.classification import MultilayerPerceptronClassifier,LinearSVC,OneVsRest\nfrom pyspark.ml.classification import NaiveBayes, FMClassifier\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer,IndexToString,MinMaxScaler\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:58:11.893048Z","iopub.execute_input":"2022-11-27T06:58:11.893496Z","iopub.status.idle":"2022-11-27T06:58:11.900455Z","shell.execute_reply.started":"2022-11-27T06:58:11.893444Z","shell.execute_reply":"2022-11-27T06:58:11.899284Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"#The regression algorithms are under the ml.regression \nfrom pyspark.ml.regression import DecisionTreeRegressor, LinearRegression\nfrom pyspark.ml.regression import RandomForestRegressor,GBTRegressor\nfrom pyspark.ml.regression import FMRegressor","metadata":{"execution":{"iopub.status.busy":"2022-11-27T07:00:36.859214Z","iopub.execute_input":"2022-11-27T07:00:36.86031Z","iopub.status.idle":"2022-11-27T07:00:36.865824Z","shell.execute_reply.started":"2022-11-27T07:00:36.860269Z","shell.execute_reply":"2022-11-27T07:00:36.864851Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Load and parse the data file, converting it to a DataFrame.\ndata = spark.read.format(\"libsvm\").load(\"/kaggle/input/mllib-datasets/sample_libsvm_data.txt\")\ndata.show(5)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T05:52:46.460303Z","iopub.execute_input":"2022-11-27T05:52:46.460744Z","iopub.status.idle":"2022-11-27T05:52:46.750375Z","shell.execute_reply.started":"2022-11-27T05:52:46.460706Z","shell.execute_reply":"2022-11-27T05:52:46.749203Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"22/11/27 05:52:46 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n+-----+--------------------+\n|label|            features|\n+-----+--------------------+\n|  0.0|(692,[127,128,129...|\n|  1.0|(692,[158,159,160...|\n|  1.0|(692,[124,125,126...|\n|  1.0|(692,[152,153,154...|\n|  1.0|(692,[151,152,153...|\n+-----+--------------------+\nonly showing top 5 rows\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### FM Classifier","metadata":{}},{"cell_type":"code","source":"# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n# Scale features.\nfeatureScaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\").fit(data)\n\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = data.randomSplit([0.7, 0.3])\n\n# Train a FM model.\nfm = FMClassifier(labelCol=\"indexedLabel\", featuresCol=\"scaledFeatures\", stepSize=0.001)\n\n# Create a Pipeline.\npipeline = Pipeline(stages=[labelIndexer, featureScaler, fm])\n\n# Train model.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\npredictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test accuracy\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test set accuracy = %g\" % accuracy)\n\nfmModel = model.stages[2]\nprint(\"Factors: \" + str(fmModel.factors))  # type: ignore\nprint(\"Linear: \" + str(fmModel.linear))  # type: ignore\nprint(\"Intercept: \" + str(fmModel.intercept))  # type: ignore","metadata":{"execution":{"iopub.status.busy":"2022-11-27T05:49:33.575132Z","iopub.execute_input":"2022-11-27T05:49:33.575627Z","iopub.status.idle":"2022-11-27T05:49:53.598369Z","shell.execute_reply.started":"2022-11-27T05:49:33.57559Z","shell.execute_reply":"2022-11-27T05:49:53.597027Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"22/11/27 05:49:33 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"+----------+------------+--------------------+\n|prediction|indexedLabel|            features|\n+----------+------------+--------------------+\n|       1.0|         1.0|(692,[95,96,97,12...|\n|       1.0|         1.0|(692,[98,99,100,1...|\n|       1.0|         1.0|(692,[123,124,125...|\n|       1.0|         1.0|(692,[124,125,126...|\n|       1.0|         1.0|(692,[124,125,126...|\n+----------+------------+--------------------+\nonly showing top 5 rows\n\nTest set accuracy = 1\nFactors: DenseMatrix([[ 1.22740955e-02,  9.71533891e-03,  9.72144058e-03, ...,\n              -1.01719619e-03,  1.97359464e-03, -1.26642452e-02],\n             [ 4.01891399e-03,  4.99263763e-03, -9.29502918e-03, ...,\n               5.64145256e-03,  4.16917000e-03,  1.64749289e-02],\n             [-1.85051239e-02,  5.71564539e-03,  1.38837819e-02, ...,\n               5.47109351e-05, -6.96925333e-03,  1.08227695e-02],\n             ...,\n             [ 2.25340529e-02,  2.66272112e-02, -1.60442680e-02, ...,\n              -1.86733853e-02, -3.05739989e-02, -2.52269964e-02],\n             [ 2.59529695e-02,  1.91597537e-02, -1.99705722e-02, ...,\n              -2.82591357e-02, -3.21121801e-02, -4.76950549e-03],\n             [ 2.43377140e-02,  2.15768682e-02, -2.06364704e-02, ...,\n              -1.83299291e-02, -2.46854392e-02, -1.02710508e-02]])\nLinear: [-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,0.0,0.0,-0.033512107376264,-0.033512169208832994,-0.03183170071018431,-0.02866655706860756,-0.003123677608244055,-0.022522652655035325,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,0.0,0.0,0.0122343348156065,0.013184666175333674,-0.003909493314024539,-0.02517914846562917,-0.0274710781922893,-0.027859072714265284,-0.02628681885142046,-0.023278858832970085,-0.02362007191757448,-0.030628871581305395,-0.029678793364892642,-0.027519176765949022,0.012473292834650707,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,0.0,0.0,0.015451155650912119,0.014232569310295706,0.011822214434811565,0.005235511267386609,-0.020757178440953105,-0.02496117343798862,-0.019971553945246193,-0.014397936345022021,-0.02005555891644125,-0.02916061192949887,-0.027783669331535504,-0.031695896029901215,-0.030995662242836595,0.0009030918487785456,0.012098473189501684,0.010911880439892965,0.010911880439892965,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,0.0,0.016328293267022915,0.01641434001277641,0.014606126823543808,0.011717780692353155,0.005553519787200952,-0.00274965946782979,-0.017575525241603333,-0.02487162180440946,-0.020530198086768228,-0.024837969303770167,-0.026805003821147637,-0.02447917897455124,-0.02344897872883409,-0.0043713223493318,0.0010821393273928819,0.010755100195363992,0.010911880439892965,0.010911880439892965,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,0.01668847523240336,0.01621755247153583,0.01457492564080387,0.012005465036481307,0.008543476179808268,0.012314291744856225,0.0015314561534768626,-0.016314473501638345,-0.027933463045471632,-0.026072672378812216,-0.02698594468075501,-0.023357846889339028,-0.020018689106147833,-0.011303135480883091,0.0031966153420423067,0.003981795814150354,0.0044353879118593665,0.011126466052289296,0.010911880439892965,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,0.015451257670839958,0.01582751341428624,0.015212585647473645,0.013355185171994995,0.008043925804344777,0.011545614769832338,0.01372485601562897,0.0009854165803185502,-0.024802049932735666,-0.03353805501047444,-0.03113991601120004,-0.02899889107835916,-0.02969700203290335,-0.016790640474517664,0.0011171438962201655,0.008224576198118658,0.010002822098332333,0.009325191897158166,0.013244603406848753,0.011265330652939373,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,0.01413896801314895,0.01542470887377619,0.014246899596301313,0.012775915676250239,0.012368261198960681,0.013216781447906794,0.0150926192800755,-0.0031900255750430904,-0.02893016651498723,-0.029805723785799567,-0.02755795840416886,-0.026584223336718067,-0.028703126568204063,-0.024179464174307833,0.011464554827921922,0.015293244554063327,0.015919060403356958,0.015139764507731508,0.013287157943287865,0.013364884636053204,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,0.015847579121533447,0.015168304188555404,0.013926161050093176,0.012807425952927423,0.013202567447295014,0.014935165123162166,0.011782620431124484,-0.007603430133120081,-0.03216996110715905,-0.028027441197967812,-0.026441459108620634,-0.025963428885256268,-0.0285025108154374,-0.02773403427143419,0.011548359197710818,0.015648315986670113,0.016663127244189172,0.01790709006834841,0.02040048017384374,0.0156658293420304,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,0.015152304959557703,0.016187710164907904,0.013874039289077173,0.010922997560379505,0.010511058237156014,0.012584455032264288,0.013998138342990647,-0.005381507972020736,-0.029535940075825613,-0.026467594762554905,-0.02581187311803723,-0.025775674097239867,-0.02790583826457008,-0.0295721568469588,0.01779733285219458,0.01525573291622118,0.013903171011736267,0.016019447153673604,0.019336267124114657,0.022905353574557738,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,0.01656977403919637,0.015558916475312124,0.007840115702169923,0.007663138614623254,0.011101601926589095,0.015142851070385373,0.01422310430557975,-0.02146402129564479,-0.027645137930712427,-0.02622946864248424,-0.025822979020281606,-0.025970740519415725,-0.028222862153484064,-0.012364644102177845,0.02176004207842651,0.015706140566645724,0.01345628218632107,0.013536167671977397,0.017510777344044205,0.023886862341902147,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,0.01854749437483802,0.013050018632024919,0.010617927920475695,0.013683917952013753,0.014850250422767046,0.01709350815551265,0.01586455772335486,-0.02984052406372258,-0.02602797606415119,-0.02582435944584048,-0.025829439543298992,-0.02602624693148518,-0.029448870907953277,0.01620671798352886,0.02114241743702583,0.01604877736179754,0.014007557701202076,0.01320399028301669,0.016019733043434764,0.023638795903846703,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,0.0,0.01916045271734417,0.014461144468436018,0.012927680547107046,0.014662420823472747,0.01591200592335752,0.01864663314489473,0.016670621972232673,-0.02649003549034953,-0.025689881786811676,-0.0258298829332329,-0.026054768638545427,-0.026858462263755295,-0.031057386017535032,0.021113568751178466,0.018662815933217048,0.01612085801783235,0.014797280175162323,0.012938469887623845,0.0158201051767209,0.021712642013399352,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,0.011112209220940411,0.020315793625732124,0.014603634583957177,0.013121049502425916,0.014705954038646073,0.016062896201659554,0.019612154213096932,-0.0009584426753157387,-0.024962746674104655,-0.025674862171456023,-0.026130311629202627,-0.02622999699228661,-0.02806957527960051,-0.031904271002202104,0.021203317169405857,0.01780991225629671,0.01602994710895229,0.014374703966665386,0.015087203014601553,0.01797644377313197,0.019749956717980084,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,0.01111235312521004,0.01875493895856552,0.01511893816765953,0.013518616197708716,0.014839192234183302,0.01632925082261199,0.019231902960435275,-0.02435787506281222,-0.02475922810328392,-0.0256053344994119,-0.026288624347398475,-0.026693032337563188,-0.02733373771723954,-0.011092667517251676,0.020583924903029543,0.01682019348039336,0.01550754674153399,0.016520669402180305,0.017435107416606477,0.015317853563880508,0.01408121977704731,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,0.01207375225694239,0.01837734852335619,0.015392298128529979,0.01364585342303272,0.014897061637608441,0.015862420293348306,0.00033933356678512083,-0.023886236554940182,-0.02460060541673596,-0.025566405948525765,-0.026532935013095598,-0.027410384750207153,-0.02995226498100905,0.012109862166451716,0.016245531171590985,0.014022221358231023,0.016598382893006,0.017484395019126665,0.014821916641155666,0.013168763871062212,0.013186758260923147,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,0.012185421508578431,0.016301953955218412,0.01693233278619853,0.014724480696228656,0.01466263533597288,0.012441662776173638,-0.025416602715056077,-0.02396474136417929,-0.024963237908868955,-0.026137759305500535,-0.027239525113946547,-0.02888202449232092,-0.03196583648126626,0.017700873282507256,0.017892586214854136,0.014827812012555149,0.013190080417459036,0.012212678997277964,0.013053406302119925,0.01369140081023654,0.013578890191088867,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,0.012185421508578431,0.013806056544180857,0.016170946052536665,0.01644197403699115,0.015660255594862005,0.007085965499884415,-0.028206181903647022,-0.025680960874913294,-0.025754807180117908,-0.02681683554882596,-0.027883411159072936,-0.02971731502741022,-0.018085031988966133,0.012562026067126131,0.010100236239628484,0.007659076390382502,0.00839882295772129,0.010541266830121793,0.013525599345485417,0.014006348443668147,0.01374176469697253,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,0.012123826901199527,0.013084908970454398,0.013151679272074807,0.012588836040565782,0.013693140938631113,0.006047754811886537,-0.022461821012436538,-0.02696466467943903,-0.026921618632623635,-0.02925561319436852,-0.03137586770822421,-0.027628427599012455,-0.004463162420772574,0.007657784509665293,0.0037174571459626157,0.011730715713027287,0.01132824741638474,0.012435901443645982,0.01389017825470778,0.013073525159003589,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,0.0,0.012210583811348132,0.0007791375683791604,0.0058584274172469334,0.008194960827618917,-0.00600248981395406,-0.022271885723994713,-0.030117633782999243,-0.030261290032088797,-0.028778770860291934,-0.026058134050848603,-0.015860682367177435,-0.0017098592372638492,0.0050267948992681845,0.00988650272879694,0.013778163264360583,0.013329861457600168,0.01411462902722752,0.01431330491108526,0.011918583734661937,-0.030355727259455904,-0.030355727259455904,-0.02329267340992247,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,0.0,0.0,-0.03129602661501977,-0.012573882541289792,-0.006168877020272765,-0.019343400124048414,-0.024440842464368104,-0.02872254570791743,-0.026362693994490326,-0.02498805189892624,-0.023147350631134253,-0.0131193448367434,-0.000842078945652463,0.009369297424358112,0.010569463041472405,0.011376191124067193,0.01417264780908353,0.01463715527733591,0.01099617551010057,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,0.0,-0.03761071845085931,-0.03191451342605139,-0.029000466844346658,-0.03241102638546745,-0.02985074379925254,-0.02618174432264888,-0.01126225626419671,-0.016123589800283843,-0.019976148396363566,-0.012045385394732282,0.006271955587461521,0.006880039479443065,0.00657634084780653,0.011608452582618556,0.014832607340905992,0.014009348833422272,0.010990147981313633,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,-0.030355727259455904,0.0,0.0,-0.025596006778993027,-0.02547481137028931,-0.026017698888707034,-0.0327454603822912,0.014048516727378323,0.020249670295914617,-0.0018052313538627356,-0.000946792051576916,0.016322785770906047,0.01584098747048019,0.015515257109316108,0.014836312769382205]\nIntercept: -0.030355730775338142\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Naive Bayes Classifier\n\nNaive Bayes can be trained very efficiently. With a single pass over the training data, it computes the conditional probability distribution of each feature given each label. For prediction, it applies Bayes’ theorem to compute the conditional probability distribution of each label given an observation.\n\nMLlib supports Multinomial naive Bayes, Complement naive Bayes, Bernoulli naive Bayes and Gaussian naive Bayes.","metadata":{}},{"cell_type":"code","source":"data = spark.read.format(\"libsvm\") \\\n    .load(\"/kaggle/input/mllib-datasets/sample_libsvm_data.txt\")\n\n# Split the data into train and test\nsplits = data.randomSplit([0.6, 0.4], 1234)\ntrain = splits[0]\ntest = splits[1]\n\n# create the trainer and set its parameters\nnb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n\n# train the model\nmodel = nb.fit(train)\n\n# select example rows to display.\npredictions = model.transform(test)\npredictions.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-27T05:59:19.561485Z","iopub.execute_input":"2022-11-27T05:59:19.561938Z","iopub.status.idle":"2022-11-27T05:59:20.816187Z","shell.execute_reply.started":"2022-11-27T05:59:19.561868Z","shell.execute_reply":"2022-11-27T05:59:20.814985Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"22/11/27 05:59:19 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n+-----+--------------------+--------------------+-----------+----------+\n|label|            features|       rawPrediction|probability|prediction|\n+-----+--------------------+--------------------+-----------+----------+\n|  0.0|(692,[95,96,97,12...|[-172664.79564650...|  [1.0,0.0]|       0.0|\n|  0.0|(692,[98,99,100,1...|[-176279.15054306...|  [1.0,0.0]|       0.0|\n|  0.0|(692,[122,123,124...|[-189600.55409526...|  [1.0,0.0]|       0.0|\n|  0.0|(692,[124,125,126...|[-274673.88337431...|  [1.0,0.0]|       0.0|\n|  0.0|(692,[124,125,126...|[-183393.03869049...|  [1.0,0.0]|       0.0|\n|  0.0|(692,[125,126,127...|[-256992.48807619...|  [1.0,0.0]|       0.0|\n|  0.0|(692,[126,127,128...|[-210411.53649773...|  [1.0,0.0]|       0.0|\n|  0.0|(692,[127,128,129...|[-170627.63616681...|  [1.0,0.0]|       0.0|\n|  0.0|(692,[127,128,129...|[-212157.96750469...|  [1.0,0.0]|       0.0|\n|  0.0|(692,[127,128,129...|[-183253.80108550...|  [1.0,0.0]|       0.0|\n|  0.0|(692,[128,129,130...|[-246528.93739632...|  [1.0,0.0]|       0.0|\n|  0.0|(692,[150,151,152...|[-158348.34683571...|  [1.0,0.0]|       0.0|\n|  0.0|(692,[152,153,154...|[-210229.50765957...|  [1.0,0.0]|       0.0|\n|  0.0|(692,[152,153,154...|[-242985.16248889...|  [1.0,0.0]|       0.0|\n|  0.0|(692,[152,153,154...|[-94622.933454005...|  [1.0,0.0]|       0.0|\n|  0.0|(692,[153,154,155...|[-266465.39689814...|  [1.0,0.0]|       0.0|\n|  0.0|(692,[153,154,155...|[-144989.71469229...|  [1.0,0.0]|       0.0|\n|  0.0|(692,[154,155,156...|[-283834.57437738...|  [1.0,0.0]|       0.0|\n|  0.0|(692,[181,182,183...|[-155256.59399829...|  [1.0,0.0]|       0.0|\n|  1.0|(692,[100,101,102...|[-147726.11958982...|  [0.0,1.0]|       1.0|\n+-----+--------------------+--------------------+-----------+----------+\nonly showing top 20 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.sql.functions import *\npredictions. \\\n    select(col(\"prediction\"),col(\"features\")). \\\n    filter(\"prediction = 1.0\"). \\\n    show()","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:16:02.608787Z","iopub.execute_input":"2022-11-27T06:16:02.609215Z","iopub.status.idle":"2022-11-27T06:16:02.811761Z","shell.execute_reply.started":"2022-11-27T06:16:02.609178Z","shell.execute_reply":"2022-11-27T06:16:02.81061Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"+----------+--------------------+\n|prediction|            features|\n+----------+--------------------+\n|       0.0|(692,[95,96,97,12...|\n|       0.0|(692,[98,99,100,1...|\n|       0.0|(692,[122,123,124...|\n|       0.0|(692,[124,125,126...|\n|       0.0|(692,[124,125,126...|\n|       0.0|(692,[125,126,127...|\n|       0.0|(692,[126,127,128...|\n|       0.0|(692,[127,128,129...|\n|       0.0|(692,[127,128,129...|\n|       0.0|(692,[127,128,129...|\n|       0.0|(692,[128,129,130...|\n|       0.0|(692,[150,151,152...|\n|       0.0|(692,[152,153,154...|\n|       0.0|(692,[152,153,154...|\n|       0.0|(692,[152,153,154...|\n|       0.0|(692,[153,154,155...|\n|       0.0|(692,[153,154,155...|\n|       0.0|(692,[154,155,156...|\n|       0.0|(692,[181,182,183...|\n|       1.0|(692,[100,101,102...|\n+----------+--------------------+\nonly showing top 20 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# compute accuracy on the test set\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n                                              metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test set accuracy = \" + str(accuracy))","metadata":{"execution":{"iopub.status.busy":"2022-11-27T05:59:32.714459Z","iopub.execute_input":"2022-11-27T05:59:32.714938Z","iopub.status.idle":"2022-11-27T05:59:32.893431Z","shell.execute_reply.started":"2022-11-27T05:59:32.714876Z","shell.execute_reply":"2022-11-27T05:59:32.892295Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Test set accuracy = 1.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### One-vs-Rest classifier (a.k.a. One-vs-All)\n\nOneVsRest is an example of a machine learning reduction for performing multiclass classification given a base classifier that can perform binary classification efficiently. It is also known as “One-vs-All.”\n\nOneVsRest is implemented as an Estimator. For the base classifier, it takes instances of Classifier and creates a binary classification problem for each of the k classes. The classifier for class i is trained to predict whether the label is i or not, distinguishing class i from all other classes.\n\nPredictions are done by evaluating each binary classifier and the index of the most confident classifier is output as label.","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.classification import OneVsRest\n\n# load data file.\ninputData = spark.read.format(\"libsvm\") \\\n    .load(\"/kaggle/input/mllib-datasets/sample_multiclass_classification_data.txt\")\n\n# generate the train/test split.\n(train, test) = inputData.randomSplit([0.8, 0.2])\n\n# instantiate the base classifier.\nlr = LogisticRegression(maxIter=10, tol=1E-6, fitIntercept=True)\n\n# instantiate the One Vs Rest Classifier.\novr = OneVsRest(classifier=lr)\n\n# train the multiclass model.\novrModel = ovr.fit(train)\n\n# score the model on test data.\npredictions = ovrModel.transform(test)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:35:01.708232Z","iopub.execute_input":"2022-11-27T06:35:01.708624Z","iopub.status.idle":"2022-11-27T06:35:06.998148Z","shell.execute_reply.started":"2022-11-27T06:35:01.708592Z","shell.execute_reply":"2022-11-27T06:35:06.996946Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"22/11/27 06:35:01 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n","output_type":"stream"}]},{"cell_type":"code","source":"# obtain evaluator.\nevaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n\n# compute the classification error on test data.\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Linear Support Vector Machine\n\nA support vector machine constructs a hyperplane or set of hyperplanes in a high- or infinite-dimensional space, which can be used for classification, regression, or other tasks. \nIntuitively, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier. \n\nLinearSVC in Spark ML supports binary classification with linear SVM. Internally, it optimizes the Hinge Loss using OWLQN optimizer.","metadata":{}},{"cell_type":"code","source":"# Load training data\ntraining = spark.read.format(\"libsvm\").load(\"/kaggle/input/mllib-datasets/sample_libsvm_data.txt\")\n#You need to send the training set as labels and features\nlsvc = LinearSVC(maxIter=10, regParam=0.1)\n\n# Fit the model\nlsvcModel = lsvc.fit(training)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:38:13.060179Z","iopub.execute_input":"2022-11-27T06:38:13.060581Z","iopub.status.idle":"2022-11-27T06:38:16.996391Z","shell.execute_reply.started":"2022-11-27T06:38:13.06055Z","shell.execute_reply":"2022-11-27T06:38:16.99517Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"22/11/27 06:38:13 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n22/11/27 06:38:13 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n22/11/27 06:38:13 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n","output_type":"stream"}]},{"cell_type":"code","source":"lsvcResult = lsvcModel.transform(training)\nlsvcResult.show(2)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:38:49.599129Z","iopub.execute_input":"2022-11-27T06:38:49.599849Z","iopub.status.idle":"2022-11-27T06:38:49.800776Z","shell.execute_reply.started":"2022-11-27T06:38:49.599795Z","shell.execute_reply":"2022-11-27T06:38:49.799671Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"+-----+--------------------+--------------------+----------+\n|label|            features|       rawPrediction|prediction|\n+-----+--------------------+--------------------+----------+\n|  0.0|(692,[127,128,129...|[1.46985828303639...|       0.0|\n|  1.0|(692,[158,159,160...|[-1.3052815074026...|       1.0|\n+-----+--------------------+--------------------+----------+\nonly showing top 2 rows\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Multilayer perceptron classifier\n\nMultilayer perceptron classifier (MLPC) is a classifier based on the feedforward artificial neural network. MLPC consists of multiple layers of nodes. Each layer is fully connected to the next layer in the network. Nodes in the input layer represent the input data. ","metadata":{}},{"cell_type":"code","source":"# Load training data\ndata = spark.read.format(\"libsvm\")\\\n    .load(\"/kaggle/input/mllib-datasets/sample_multiclass_classification_data.txt\")\n\n# Split the data into train and test\nsplits = data.randomSplit([0.6, 0.4], 1234)\ntrain = splits[0]\ntest = splits[1]\n\n# specify layers for the neural network:\n# input layer of size 4 (features), two intermediate of size 5 and 4\n# and output of size 3 (classes)\nlayers = [4, 5, 4, 3]\n\n# create the trainer and set its parameters\ntrainer = MultilayerPerceptronClassifier(maxIter=100, layers=layers, \n                                         blockSize=128, seed=1234)\n\n# train the model\nmodel = trainer.fit(train)\n\n# compute accuracy on the test set\nresult = model.transform(test)\npredictionAndLabels = result.select(\"prediction\",\"features\",\"label\")\npredictionAndLabels.show(2)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:54:20.634838Z","iopub.execute_input":"2022-11-27T06:54:20.635221Z","iopub.status.idle":"2022-11-27T06:54:22.995626Z","shell.execute_reply.started":"2022-11-27T06:54:20.635184Z","shell.execute_reply":"2022-11-27T06:54:22.994815Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"22/11/27 06:54:20 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n+----------+--------------------+-----+\n|prediction|            features|label|\n+----------+--------------------+-----+\n|       0.0|(4,[0,1,2,3],[-0....|  0.0|\n|       0.0|(4,[0,1,2,3],[-0....|  0.0|\n+----------+--------------------+-----+\nonly showing top 2 rows\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Gradient-boosted tree & Random Forest classifier\n\nGradient-boosted trees (GBTs) are a popular classification and regression method using ensembles of decision trees. More information about the spark.ml implementation can be found further in the section on GBTs.\n\nExamples\n\nThe following examples load a dataset in LibSVM format, split it into training and test sets, train on the first dataset, and then evaluate on the held-out test set. We use two feature transformers to prepare the data; these help index categories for the label and categorical features, adding metadata to the DataFrame which the tree-based algorithms can recognize.","metadata":{}},{"cell_type":"code","source":"# Load and parse the data file, converting it to a DataFrame.\ndata = spark.read.format(\"libsvm\").load(\"/kaggle/input/mllib-datasets/sample_libsvm_data.txt\")\n\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n# Automatically identify categorical features, and index them.\n# Set maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer =\\\n    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = data.randomSplit([0.7, 0.3])\n\n# Train a GBT model.\ngbt = GBTClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", maxIter=10)\n\n# Chain indexers and GBT in a Pipeline\npipeline = Pipeline(stages=[labelIndexer, featureIndexer, gbt])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\npredictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:55:58.982554Z","iopub.execute_input":"2022-11-27T06:55:58.983054Z","iopub.status.idle":"2022-11-27T06:56:03.615643Z","shell.execute_reply.started":"2022-11-27T06:55:58.983006Z","shell.execute_reply":"2022-11-27T06:56:03.614503Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"22/11/27 06:55:59 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n+----------+------------+--------------------+\n|prediction|indexedLabel|            features|\n+----------+------------+--------------------+\n|       1.0|         1.0|(692,[95,96,97,12...|\n|       1.0|         1.0|(692,[122,123,124...|\n|       1.0|         1.0|(692,[122,123,148...|\n|       1.0|         1.0|(692,[124,125,126...|\n|       1.0|         1.0|(692,[124,125,126...|\n+----------+------------+--------------------+\nonly showing top 5 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"data = spark.read.format(\"libsvm\").load(\"/kaggle/input/mllib-datasets/sample_libsvm_data.txt\")\n\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n\n# Automatically identify categorical features, and index them.\n# Set maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer =\\\n    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = data.randomSplit([0.7, 0.3])\n\n# Train a RandomForest model.\nrf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=10)\n\n# Convert indexed labels back to original labels.\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n                               labels=labelIndexer.labels)\n\n# Chain indexers and forest in a Pipeline\npipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\npredictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))","metadata":{"execution":{"iopub.status.busy":"2022-11-27T06:57:36.764747Z","iopub.execute_input":"2022-11-27T06:57:36.765587Z","iopub.status.idle":"2022-11-27T06:57:37.359158Z","shell.execute_reply.started":"2022-11-27T06:57:36.765547Z","shell.execute_reply":"2022-11-27T06:57:37.357786Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"22/11/27 06:57:36 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/279310381.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Train a RandomForest model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"indexedLabel\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeaturesCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"indexedFeatures\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumTrees\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Convert indexed labels back to original labels.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'RandomForestClassifier' is not defined"],"ename":"NameError","evalue":"name 'RandomForestClassifier' is not defined","output_type":"error"}]}]}