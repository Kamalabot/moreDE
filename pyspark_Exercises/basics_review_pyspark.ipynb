{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38842d0c",
   "metadata": {},
   "source": [
    "The notebook follows subham kharwal's examples from the repo https://github.com/subhamkharwal/ease-with-apache-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a7a4f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad5e9644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/24 18:36:14 WARN Utils: Your hostname, codeStation resolves to a loopback address: 127.0.1.1; using 192.168.88.83 instead (on interface wlo1)\n",
      "23/01/24 18:36:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/24 18:36:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    appName(\"creating spark\"). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d749651a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|id |\n",
      "+---+\n",
      "|0  |\n",
      "|1  |\n",
      "|2  |\n",
      "|3  |\n",
      "|4  |\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_range = spark.range(5)\n",
    "df_range.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36e57d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  2|\n",
      "|  4|\n",
      "|  6|\n",
      "|  8|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_range_se = spark.range(start=0,end=10,step=2,numPartitions=2)\n",
    "df_range_se.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3530a0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating native dataframe with schema\n",
    "_data = [\n",
    "    [1,\"Spark\"],\n",
    "    [2,\"Hive\"],\n",
    "    [3,\"pig\"],\n",
    "    [4,\"Sqoop\"]\n",
    "]\n",
    "\n",
    "_cols = [\"id\",\"software\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3b2951b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|software|\n",
      "+---+--------+\n",
      "|  1|   Spark|\n",
      "|  2|    Hive|\n",
      "|  3|     pig|\n",
      "|  4|   Sqoop|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_wares = spark.createDataFrame(data=_data,schema=_cols)\n",
    "df_wares.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8751511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'Spark'], [2, 'Hive'], [3, 'pig'], [4, 'Sqoop']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_data_rdd = spark.sparkContext.parallelize(_data)\n",
    "_data_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d8855f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(_data_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1be20bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb14632e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_wares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a24ca495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|software|\n",
      "+---+--------+\n",
      "|  1|   Spark|\n",
      "|  2|    Hive|\n",
      "|  3|     pig|\n",
      "|  4|   Sqoop|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_data_rdd.toDF(schema=_cols).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f310d96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parsing string data_types and creating schemas out of it...\n",
    "\n",
    "from pyspark.sql.types import _parse_datatype_json_string\n",
    "from pyspark.sql.types import _parse_datatype_json_value\n",
    "from pyspark.sql.types import _parse_datatype_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "730a2e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('id', IntegerType(), True), StructField('name', StringType(), True)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_schema = 'id int, name string'\n",
    "new_schema  = _parse_datatype_string(_schema)\n",
    "new_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a76d93f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('id', IntegerType(), True), StructField('name', MapType(StringType(), StringType(), True), True), StructField('subject', ArrayType(StringType(), True), True)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_komplex_schema = \"id int, name map<string, string>, subject array<string>\"\n",
    "_komplex = _parse_datatype_string(_komplex_schema)\n",
    "_komplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e436b717",
   "metadata": {},
   "outputs": [],
   "source": [
    "### How the complex schema will be used in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af048fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working with apis\n",
    "\n",
    "import requests, json\n",
    "\n",
    "def get_apiData(url: str):\n",
    "    normalised_data = dict()\n",
    "    data = requests.get(url).json()\n",
    "    normalised_data['_data'] = data\n",
    "    return json.dumps(normalised_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1257f11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url = r\"https://api.coindesk.com/v1/bpi/currentprice.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "638804c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url_wzr = \"https://api.wazirx.com/sapi/v1/tickers/24hr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "969eca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "coindex = get_apiData(api_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c9c9a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "wzr_dex = get_apiData(api_url_wzr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e470ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_data': {'time': {'updated': 'Jan 24, 2023 13:50:00 UTC',\n",
       "   'updatedISO': '2023-01-24T13:50:00+00:00',\n",
       "   'updateduk': 'Jan 24, 2023 at 13:50 GMT'},\n",
       "  'disclaimer': 'This data was produced from the CoinDesk Bitcoin Price Index (USD). Non-USD currency data converted using hourly conversion rate from openexchangerates.org',\n",
       "  'chartName': 'Bitcoin',\n",
       "  'bpi': {'USD': {'code': 'USD',\n",
       "    'symbol': '&#36;',\n",
       "    'rate': '22,857.1066',\n",
       "    'description': 'United States Dollar',\n",
       "    'rate_float': 22857.1066},\n",
       "   'GBP': {'code': 'GBP',\n",
       "    'symbol': '&pound;',\n",
       "    'rate': '19,099.2154',\n",
       "    'description': 'British Pound Sterling',\n",
       "    'rate_float': 19099.2154},\n",
       "   'EUR': {'code': 'EUR',\n",
       "    'symbol': '&euro;',\n",
       "    'rate': '22,266.1590',\n",
       "    'description': 'Euro',\n",
       "    'rate_float': 22266.159}}}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dexData = json.loads(coindex)\n",
    "dexData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "811f592d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'symbol': 'btcinr',\n",
       " 'baseAsset': 'btc',\n",
       " 'quoteAsset': 'inr',\n",
       " 'openPrice': '1928806',\n",
       " 'lowPrice': '1821009.0',\n",
       " 'highPrice': '1944606.0',\n",
       " 'lastPrice': '1924855.0',\n",
       " 'volume': '21.30043',\n",
       " 'bidPrice': '1924000.0',\n",
       " 'askPrice': '1925723.0',\n",
       " 'at': 1674569056000}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ext_wzr = json.loads(wzr_dex)\n",
    "ext_wzr[\"_data\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c00c3938",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_rdd = spark.sparkContext.parallelize([ext_wzr[\"_data\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "869f1d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_data': {'time': {'updated': 'Jan 24, 2023 13:50:00 UTC',\n",
       "    'updatedISO': '2023-01-24T13:50:00+00:00',\n",
       "    'updateduk': 'Jan 24, 2023 at 13:50 GMT'},\n",
       "   'disclaimer': 'This data was produced from the CoinDesk Bitcoin Price Index (USD). Non-USD currency data converted using hourly conversion rate from openexchangerates.org',\n",
       "   'chartName': 'Bitcoin',\n",
       "   'bpi': {'USD': {'code': 'USD',\n",
       "     'symbol': '&#36;',\n",
       "     'rate': '22,857.1066',\n",
       "     'description': 'United States Dollar',\n",
       "     'rate_float': 22857.1066},\n",
       "    'GBP': {'code': 'GBP',\n",
       "     'symbol': '&pound;',\n",
       "     'rate': '19,099.2154',\n",
       "     'description': 'British Pound Sterling',\n",
       "     'rate_float': 19099.2154},\n",
       "    'EUR': {'code': 'EUR',\n",
       "     'symbol': '&euro;',\n",
       "     'rate': '22,266.1590',\n",
       "     'description': 'Euro',\n",
       "     'rate_float': 22266.159}}}}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dexRdd = spark.sparkContext.parallelize([dexData])\n",
    "dexRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b558a47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- askPrice: string (nullable = true)\n",
      " |-- at: long (nullable = true)\n",
      " |-- baseAsset: string (nullable = true)\n",
      " |-- bidPrice: string (nullable = true)\n",
      " |-- highPrice: string (nullable = true)\n",
      " |-- lastPrice: string (nullable = true)\n",
      " |-- lowPrice: string (nullable = true)\n",
      " |-- openPrice: string (nullable = true)\n",
      " |-- quoteAsset: string (nullable = true)\n",
      " |-- symbol: string (nullable = true)\n",
      " |-- volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ext_rdd_df = spark.read.json(ext_rdd)\n",
    "ext_rdd_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14b8b56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _data: struct (nullable = true)\n",
      " |    |-- bpi: struct (nullable = true)\n",
      " |    |    |-- EUR: struct (nullable = true)\n",
      " |    |    |    |-- code: string (nullable = true)\n",
      " |    |    |    |-- description: string (nullable = true)\n",
      " |    |    |    |-- rate: string (nullable = true)\n",
      " |    |    |    |-- rate_float: double (nullable = true)\n",
      " |    |    |    |-- symbol: string (nullable = true)\n",
      " |    |    |-- GBP: struct (nullable = true)\n",
      " |    |    |    |-- code: string (nullable = true)\n",
      " |    |    |    |-- description: string (nullable = true)\n",
      " |    |    |    |-- rate: string (nullable = true)\n",
      " |    |    |    |-- rate_float: double (nullable = true)\n",
      " |    |    |    |-- symbol: string (nullable = true)\n",
      " |    |    |-- USD: struct (nullable = true)\n",
      " |    |    |    |-- code: string (nullable = true)\n",
      " |    |    |    |-- description: string (nullable = true)\n",
      " |    |    |    |-- rate: string (nullable = true)\n",
      " |    |    |    |-- rate_float: double (nullable = true)\n",
      " |    |    |    |-- symbol: string (nullable = true)\n",
      " |    |-- chartName: string (nullable = true)\n",
      " |    |-- disclaimer: string (nullable = true)\n",
      " |    |-- time: struct (nullable = true)\n",
      " |    |    |-- updated: string (nullable = true)\n",
      " |    |    |-- updatedISO: string (nullable = true)\n",
      " |    |    |-- updateduk: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df = spark.read.json(dexRdd)\n",
    "spark_df.select(\"_data\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "04d2ad72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+--------------------+\n",
      "|                 bpi|chartName|          disclaimer|                time|\n",
      "+--------------------+---------+--------------------+--------------------+\n",
      "|{{EUR, Euro, 22,2...|  Bitcoin|This data was pro...|{Jan 24, 2023 13:...|\n",
      "+--------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.select(\"_data.*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f97e21e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----------+----------+-------+\n",
      "|code|         description|       rate|rate_float| symbol|\n",
      "+----+--------------------+-----------+----------+-------+\n",
      "| GBP|British Pound Ste...|19,099.2154|19099.2154|&pound;|\n",
      "+----+--------------------+-----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.select(\"_data.*\").select(\"bpi.*\").select(\"GBP.*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5d01deaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+\n",
      "|  symbol| bidPrice| lowPrice|\n",
      "+--------+---------+---------+\n",
      "|  btcinr|1924000.0|1821009.0|\n",
      "|  xrpinr|    35.51|  35.1409|\n",
      "|  ethinr| 135969.7| 135000.0|\n",
      "|  trxinr|     5.25|      5.2|\n",
      "|  eosinr|     89.1|    87.96|\n",
      "|  zilinr|     2.37|      2.3|\n",
      "|  batinr|   21.805|     21.0|\n",
      "|  zrxinr|     19.9|     19.0|\n",
      "|  reqinr|   9.2009|     9.11|\n",
      "|  icxinr|   21.538|     16.5|\n",
      "|  omginr|  120.105|  115.001|\n",
      "| iostinr|     0.85|      0.8|\n",
      "| dentinr|    0.076|    0.075|\n",
      "|  hotinr|    0.178|    0.174|\n",
      "| usdtinr|    84.75|    83.61|\n",
      "|  wrxinr|     16.2|    15.56|\n",
      "|maticinr|   84.401|   82.521|\n",
      "|  bchinr|  11012.0|  10908.0|\n",
      "|  bnbinr|  26516.0|  25409.0|\n",
      "|  oneinr|   1.5751|     1.52|\n",
      "+--------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ext_rdd_df.select(\"symbol\",\"bidPrice\",\"lowPrice\").show()a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b7b9e1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example where the columns have json data\n",
    "\n",
    "_jdata = [\n",
    "    ['EMP001', '{\"dept\" : \"account\", \"fname\": \"Ramesh\", \"lname\": \"Singh\", \"skills\": [\"excel\", \"tally\", \"word\"]}'],\n",
    "    ['EMP002', '{\"dept\" : \"sales\", \"fname\": \"Siv\", \"lname\": \"Kumar\", \"skills\": [\"biking\", \"sales\"]}'],\n",
    "    ['EMP003', '{\"dept\" : \"hr\", \"fname\": \"MS Raghvan\", \"skills\": [\"communication\", \"soft-skills\"]}']\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "813fe865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EMP001',\n",
       " '{\"dept\" : \"account\", \"fname\": \"Ramesh\", \"lname\": \"Singh\", \"skills\": [\"excel\", \"tally\", \"word\"]}']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_jdata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7fd78857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: string (nullable = true)\n",
      " |-- raw_data: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_colsJ = ['emp_no','raw_data']\n",
    "\n",
    "df_jraw = spark.createDataFrame(_jdata,schema=_colsJ)\n",
    "\n",
    "df_jraw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eed3d3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------------------------------------------------------------------------------+\n",
      "|emp_no|raw_data                                                                                       |\n",
      "+------+-----------------------------------------------------------------------------------------------+\n",
      "|EMP001|{\"dept\" : \"account\", \"fname\": \"Ramesh\", \"lname\": \"Singh\", \"skills\": [\"excel\", \"tally\", \"word\"]}|\n",
      "|EMP002|{\"dept\" : \"sales\", \"fname\": \"Siv\", \"lname\": \"Kumar\", \"skills\": [\"biking\", \"sales\"]}            |\n",
      "|EMP003|{\"dept\" : \"hr\", \"fname\": \"MS Raghvan\", \"skills\": [\"communication\", \"soft-skills\"]}             |\n",
      "+------+-----------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_jraw.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "655afbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Using rdd to generate the schema \n",
    "\n",
    "json_schema_df = spark.read.json(df_jraw.rdd.map(lambda row: row.raw_data))\n",
    "json_schema = json_schema_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2b4b38f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+----------------------------+\n",
      "|dept   |fname     |lname|skills                      |\n",
      "+-------+----------+-----+----------------------------+\n",
      "|account|Ramesh    |Singh|[excel, tally, word]        |\n",
      "|sales  |Siv       |Kumar|[biking, sales]             |\n",
      "|hr     |MS Raghvan|null |[communication, soft-skills]|\n",
      "+-------+----------+-----+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_schema_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "01ff8cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_newj = df_jraw.withColumn(\"parsed_data\",\n",
    "                             from_json(df_jraw['raw_data'], \n",
    "                                       schema=json_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b66bbe7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: string (nullable = true)\n",
      " |-- raw_data: string (nullable = true)\n",
      " |-- parsed_data: struct (nullable = true)\n",
      " |    |-- dept: string (nullable = true)\n",
      " |    |-- fname: string (nullable = true)\n",
      " |    |-- lname: string (nullable = true)\n",
      " |    |-- skills: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_newj.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0b948e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+-----+-------------+\n",
      "|emp_no|dept   |fname     |lname|Skills       |\n",
      "+------+-------+----------+-----+-------------+\n",
      "|EMP001|account|Ramesh    |Singh|excel        |\n",
      "|EMP001|account|Ramesh    |Singh|tally        |\n",
      "|EMP001|account|Ramesh    |Singh|word         |\n",
      "|EMP002|sales  |Siv       |Kumar|biking       |\n",
      "|EMP002|sales  |Siv       |Kumar|sales        |\n",
      "|EMP003|hr     |MS Raghvan|null |communication|\n",
      "|EMP003|hr     |MS Raghvan|null |soft-skills  |\n",
      "+------+-------+----------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_newj.select(\"emp_no\",\"parsed_data.*\"). \\\n",
    "        withColumn('Skills',explode(\"skills\")). \\\n",
    "        drop(\"parsed_data\"). \\\n",
    "        show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9aabac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python function to flatten the data dynamically\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Create outer method to return the flattened Data Frame\n",
    "def flatten_json_df(_df: DataFrame) -> DataFrame:\n",
    "    # List to hold the dynamically generated column names\n",
    "    flattened_col_list = []\n",
    "    \n",
    "    # Inner method to iterate over Data Frame to generate the column list\n",
    "    def get_flattened_cols(df: DataFrame, struct_col: str = None) -> None:\n",
    "        for col in df.columns:\n",
    "            if df.schema[col].dataType.typeName() != 'struct':\n",
    "                if struct_col is None:\n",
    "                    flattened_col_list.append(f\"{col} as {col.replace('.','_')}\")\n",
    "                else:\n",
    "                    t = struct_col + \".\" + col\n",
    "                    flattened_col_list.append(f\"{t} as {t.replace('.','_')}\")\n",
    "            else:\n",
    "                chained_col = struct_col +\".\"+ col if struct_col is not None else col\n",
    "                get_flattened_cols(df.select(col+\".*\"), chained_col)\n",
    "    \n",
    "    # Call the inner Method\n",
    "    get_flattened_cols(_df)\n",
    "    \n",
    "    # Return the flattened Data Frame\n",
    "    return _df.selectExpr(flattened_col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5aa6af4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_df = flatten_json_df(df_newj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "670dd122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: string (nullable = true)\n",
      " |-- raw_data: string (nullable = true)\n",
      " |-- parsed_data_dept: string (nullable = true)\n",
      " |-- parsed_data_fname: string (nullable = true)\n",
      " |-- parsed_data_lname: string (nullable = true)\n",
      " |-- parsed_data_skills: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flattened_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "aada3c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+-----------------+-------------+\n",
      "|emp_no|parsed_data_fname|parsed_data_lname|       skills|\n",
      "+------+-----------------+-----------------+-------------+\n",
      "|EMP001|           Ramesh|            Singh|        excel|\n",
      "|EMP001|           Ramesh|            Singh|        tally|\n",
      "|EMP001|           Ramesh|            Singh|         word|\n",
      "|EMP002|              Siv|            Kumar|       biking|\n",
      "|EMP002|              Siv|            Kumar|        sales|\n",
      "|EMP003|       MS Raghvan|             null|communication|\n",
      "|EMP003|       MS Raghvan|             null|  soft-skills|\n",
      "+------+-----------------+-----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flattened_df.withColumn(\"skills\",explode(\"parsed_data_skills\")). \\\n",
    "        drop(\"parsed_data_skills\",\"parsed_data_skills\",\n",
    "             \"raw_data\",\"parsed_data_dept\"). \\\n",
    "        show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c6f0a7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---+----------+\n",
      "|  ID|    NAME|Age|       DOB|\n",
      "+----+--------+---+----------+\n",
      "|K101|newFrame| 28| 5-05-2023|\n",
      "|K201|fineGram| 79|21-07-2021|\n",
      "|K103|    urad| 57|02-07-2022|\n",
      "+----+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# merging dataframes\n",
    "_data1 = [\n",
    "    [\"K101\",\"newFrame\",28,\"5-05-2023\"],\n",
    "    [\"K201\",\"fineGram\",79,\"21-07-2021\"],\n",
    "    [\"K103\",\"urad\",57,\"02-07-2022\"]\n",
    "]\n",
    "\n",
    "_col1 = [\"ID\",\"NAME\",\"Age\",\"DOB\"]\n",
    "\n",
    "df_1 = spark.createDataFrame(data=_data1,schema=_col1)\n",
    "df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "35777efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "_data2 = [\n",
    "    [\"c101\",\"Saku\",\"Indore\",[\"new Science\", \"physics\"]],\n",
    "    [\"D105\",\"Godu\",\"Nokrum\",[\"philo\",\"Physics\",\"Alchemy\"]],\n",
    "    [\"Y056\",\"Nokka\",\"wakrund\",[\"reunmod\",\"rune sophy\",\"magic\"]]\n",
    "] \n",
    "\n",
    "_col2 = [\"ID\",\"Name\",\"Place\",\"Subjects\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "92a64891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------+--------------------+\n",
      "|  ID| Name|  Place|            Subjects|\n",
      "+----+-----+-------+--------------------+\n",
      "|c101| Saku| Indore|[new Science, phy...|\n",
      "|D105| Godu| Nokrum|[philo, Physics, ...|\n",
      "|Y056|Nokka|wakrund|[reunmod, rune so...|\n",
      "+----+-----+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2 = spark.createDataFrame(data=_data2,schema=_col2)\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "24e95d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---+----------+-----+--------+\n",
      "|  ID|Name|Age|       DOB|Place|Subjects|\n",
      "+----+----+---+----------+-----+--------+\n",
      "|K101|null| 28| 5-05-2023| null|    null|\n",
      "|K201|null| 79|21-07-2021| null|    null|\n",
      "|K103|null| 57|02-07-2022| null|    null|\n",
      "+----+----+---+----------+-----+--------+\n",
      "\n",
      "+----+-----+-------+--------------------+----+----+\n",
      "|  ID| Name|  Place|            Subjects| Age| DOB|\n",
      "+----+-----+-------+--------------------+----+----+\n",
      "|c101| Saku| Indore|[new Science, phy...|null|null|\n",
      "|D105| Godu| Nokrum|[philo, Physics, ...|null|null|\n",
      "|Y056|Nokka|wakrund|[reunmod, rune so...|null|null|\n",
      "+----+-----+-------+--------------------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Before merging the missing cols to be added\n",
    "\n",
    "for col in df_2.columns:\n",
    "    if col not in df_1.columns:\n",
    "        df_1 = df_1.withColumn(col,lit(None))\n",
    "        \n",
    "for col in df_1.columns:\n",
    "    if col not in df_2.columns:\n",
    "        df_2 = df_2.withColumn(col,lit(None))\n",
    "        \n",
    "df_1.show()\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f27e96d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+----------+-------+--------------------+\n",
      "|  ID| Name| Age|       DOB|  Place|            Subjects|\n",
      "+----+-----+----+----------+-------+--------------------+\n",
      "|K101| null|  28| 5-05-2023|   null|                null|\n",
      "|K201| null|  79|21-07-2021|   null|                null|\n",
      "|K103| null|  57|02-07-2022|   null|                null|\n",
      "|c101| Saku|null|      null| Indore|[new Science, phy...|\n",
      "|D105| Godu|null|      null| Nokrum|[philo, Physics, ...|\n",
      "|Y056|Nokka|null|      null|wakrund|[reunmod, rune so...|\n",
      "+----+-----+----+----------+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dunion = df_1.unionByName(df_2)\n",
    "dunion.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "046bb45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----+\n",
      "|NAME    |SUBJECT|MARKS|\n",
      "+--------+-------+-----+\n",
      "|Ramesh  |PHY    |90   |\n",
      "|Ramesh  |MATH   |95   |\n",
      "|Ramesh  |CHEM   |100  |\n",
      "|Sangeeta|PHY    |90   |\n",
      "|Sangeeta|MATH   |100  |\n",
      "|Sangeeta|CHEM   |83   |\n",
      "|Mohan   |BIO    |90   |\n",
      "|Mohan   |MATH   |70   |\n",
      "|Mohan   |CHEM   |76   |\n",
      "|Imran   |PHY    |96   |\n",
      "|Imran   |MATH   |87   |\n",
      "|Imran   |CHEM   |79   |\n",
      "|Imran   |BIO    |82   |\n",
      "+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_datafp = [\n",
    "\t[\"Ramesh\", \"PHY\", 90],\n",
    "\t[\"Ramesh\", \"MATH\", 95],\n",
    "\t[\"Ramesh\", \"CHEM\", 100],\n",
    "\t[\"Sangeeta\", \"PHY\", 90],\n",
    "\t[\"Sangeeta\", \"MATH\", 100],\n",
    "\t[\"Sangeeta\", \"CHEM\", 83],\n",
    "\t[\"Mohan\", \"BIO\", 90],\n",
    "\t[\"Mohan\", \"MATH\", 70],\n",
    "\t[\"Mohan\", \"CHEM\", 76],\n",
    "\t[\"Imran\", \"PHY\", 96],\n",
    "\t[\"Imran\", \"MATH\", 87],\n",
    "\t[\"Imran\", \"CHEM\", 79],\n",
    "\t[\"Imran\", \"BIO\", 82]\n",
    "]\n",
    "\n",
    "_colsfp = [\"NAME\", \"SUBJECT\", \"MARKS\"]\n",
    "\n",
    "# Generate Data Frame\n",
    "df = spark.createDataFrame(data=_datafp, schema = _colsfp)\n",
    "df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "836b28d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a simple Python decorator - {get_time} to get the execution timings\n",
    "# If you dont know about Python decorators - check out : https://www.geeksforgeeks.org/decorators-in-python/\n",
    "import time\n",
    "\n",
    "def get_time(func):\n",
    "    def inner_get_time() -> str:\n",
    "        start_time = time.time()\n",
    "        func()\n",
    "        end_time = time.time()\n",
    "        return (f\"Execution time: {(end_time - start_time)*1000} ms\")\n",
    "    print(inner_get_time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8dbbf22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 822.1645355224609 ms\n"
     ]
    }
   ],
   "source": [
    "@get_time\n",
    "\n",
    "def x(): df.groupBy(\"NAME\").pivot(\"SUBJECT\").agg(sum(\"MARKS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b25bb633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- BIO: long (nullable = true)\n",
      " |-- CHEM: long (nullable = true)\n",
      " |-- MATH: long (nullable = true)\n",
      " |-- PHY: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 95:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----+----+----+\n",
      "|NAME    |BIO |CHEM|MATH|PHY |\n",
      "+--------+----+----+----+----+\n",
      "|Mohan   |90  |76  |70  |null|\n",
      "|Ramesh  |null|100 |95  |90  |\n",
      "|Imran   |82  |79  |87  |96  |\n",
      "|Sangeeta|null|83  |100 |90  |\n",
      "+--------+----+----+----+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pivot_df_1 = df.groupBy(\"NAME\").pivot(\"SUBJECT\").agg(sum(\"MARKS\"))\n",
    "pivot_df_1.printSchema()\n",
    "pivot_df_1.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1d865207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|SUBJECT|\n",
      "+-------+\n",
      "|   MATH|\n",
      "|    PHY|\n",
      "|   CHEM|\n",
      "|    BIO|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"SUBJECT\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bc1cb482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 610.4531288146973 ms\n"
     ]
    }
   ],
   "source": [
    "@get_time\n",
    "def x(): df.select(\"SUBJECT\").distinct().rdd.map(lambda x: x[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ed95e7e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MATH', 'PHY', 'CHEM', 'BIO']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the distinct list of Subjects\n",
    "_subjects = df.select(\"SUBJECT\").distinct().rdd.map(lambda x: x[0]).collect()\n",
    "_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "594b9ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- MATH: long (nullable = true)\n",
      " |-- PHY: long (nullable = true)\n",
      " |-- CHEM: long (nullable = true)\n",
      " |-- BIO: long (nullable = true)\n",
      "\n",
      "+--------+----+----+----+----+\n",
      "|NAME    |MATH|PHY |CHEM|BIO |\n",
      "+--------+----+----+----+----+\n",
      "|Mohan   |70  |null|76  |90  |\n",
      "|Ramesh  |95  |90  |100 |null|\n",
      "|Imran   |87  |96  |79  |82  |\n",
      "|Sangeeta|100 |90  |83  |null|\n",
      "+--------+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets check the data and schema\n",
    "pivot_df_2 = df.groupBy(\"NAME\").pivot(\"SUBJECT\", _subjects).agg(sum(\"MARKS\"))\n",
    "pivot_df_2.printSchema()\n",
    "pivot_df_2.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "088bf419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- cities: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 117:===================>                                     (1 + 2) / 3]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|              cities|\n",
      "+---+--------------------+\n",
      "|  1|[Bangalore, Mumba...|\n",
      "|  2|         [Bangalore]|\n",
      "|  3|                  []|\n",
      "|  4|[Kolkata, Bhubane...|\n",
      "|  5|[Bangalore, Mumba...|\n",
      "|  6|[Delhi, Mumbai, K...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Creating and registering udfs\n",
    "\n",
    "_data = [\n",
    "    [1, [\"Bangalore\", \"Mumbai\", \"Pune\", \"Indore\"]],\n",
    "    [2, [\"Bangalore\"]],\n",
    "    [3, []],\n",
    "    [4, [\"Kolkata\", \"Bhubaneshwar\"]],\n",
    "    [5, [\"Bangalore\", \"Mumbai\", \"Pune\", \"Indore\", \"Ahmedabad\", \"Suratkal\"]],\n",
    "    [6, [\"Delhi\", \"Mumbai\", \"Kolkāta\", \"Bangalore\", \"Chennai\", \"Hyderābād\", \"Pune\", \"Ahmedabad\", \"Sūrat\", \"Lucknow\", \"Jaipur\", \"Cawnpore\", \"Mirzāpur\", \"Nāgpur\", \"Ghāziābād\", \"Indore\", \"Vadodara\", \"Vishākhapatnam\", \"Bhopāl\", \"Chinchvad\", \"Patna\", \"Ludhiāna\", \"Āgra\", \"Kalyān\", \"Madurai\", \"Jamshedpur\", \"Nāsik\", \"Farīdābād\", \"Aurangābād\", \"Rājkot\", \"Meerut\", \"Jabalpur\", \"Thāne\", \"Dhanbād\", \"Allahābād\", \"Vārānasi\", \"Srīnagar\", \"Amritsar\", \"Alīgarh\", \"Bhiwandi\", \"Gwalior\", \"Bhilai\", \"Hāora\", \"Rānchi\", \"Bezwāda\", \"Chandīgarh\", \"Mysore\", \"Raipur\", \"Kota\", \"Bareilly\", \"Jodhpur\", \"Coimbatore\", \"Dispur\", \"Guwāhāti\", \"Solāpur\", \"Trichinopoly\", \"Hubli\", \"Jalandhar\", \"Bhubaneshwar\", \"Bhayandar\", \"Morādābād\", \"Kolhāpur\", \"Thiruvananthapuram\", \"Sahāranpur\", \"Warangal\", \"Salem\", \"Mālegaon\", \"Kochi\", \"Gorakhpur\", \"Shimoga\", \"Tiruppūr\", \"Guntūr\", \"Raurkela\", \"Mangalore\", \"Nānded\", \"Cuttack\", \"Chānda\", \"Dehra Dūn\", \"Durgāpur\", \"Āsansol\", \"Bhāvnagar\", \"Amrāvati\", \"Nellore\", \"Ajmer\", \"Tinnevelly\", \"Bīkaner\", \"Agartala\", \"Ujjain\", \"Jhānsi\", \"Ulhāsnagar\", \"Davangere\", \"Jammu\", \"Belgaum\", \"Gulbarga\", \"Jāmnagar\", \"Dhūlia\", \"Gaya\", \"Jalgaon\", \"Kurnool\", \"Udaipur\", \"Bellary\", \"Sāngli\", \"Tuticorin\", \"Calicut\", \"Akola\", \"Bhāgalpur\", \"Sīkar\", \"Tumkūr\", \"Quilon\", \"Muzaffarnagar\", \"Bhīlwāra\", \"Nizāmābād\", \"Bhātpāra\", \"Kākināda\", \"Parbhani\", \"Pānihāti\", \"Lātūr\", \"Rohtak\", \"Rājapālaiyam\", \"Ahmadnagar\", \"Cuddapah\", \"Rājahmundry\", \"Alwar\", \"Muzaffarpur\", \"Bilāspur\", \"Mathura\", \"Kāmārhāti\", \"Patiāla\", \"Saugor\", \"Bijāpur\", \"Brahmapur\", \"Shāhjānpur\", \"Trichūr\", \"Barddhamān\", \"Kulti\", \"Sambalpur\", \"Purnea\", \"Hisar\", \"Fīrozābād\", \"Bīdar\", \"Rāmpur\", \"Shiliguri\", \"Bāli\", \"Pānīpat\", \"Karīmnagar\", \"Bhuj\", \"Ichalkaranji\", \"Tirupati\", \"Hospet\", \"Āīzawl\", \"Sannai\", \"Bārāsat\", \"Ratlām\", \"Handwāra\", \"Drug\", \"Imphāl\", \"Anantapur\", \"Etāwah\", \"Rāichūr\", \"Ongole\", \"Bharatpur\", \"Begusarai\", \"Sonīpat\", \"Rāmgundam\", \"Hāpur\", \"Uluberiya\", \"Porbandar\", \"Pāli\", \"Vizianagaram\", \"Puducherry\", \"Karnāl\", \"Nāgercoil\", \"Tanjore\", \"Sambhal\", \"Naihāti\", \"Secunderābād\", \"Kharagpur\", \"Dindigul\", \"Shimla\", \"Ingrāj Bāzār\", \"Ellore\", \"Puri\", \"Haldia\", \"Nandyāl\", \"Bulandshahr\", \"Chakradharpur\", \"Bhiwāni\", \"Gurgaon\", \"Burhānpur\", \"Khammam\", \"Madhyamgram\", \"Ghāndīnagar\", \"Baharampur\", \"Mahbūbnagar\", \"Mahesāna\", \"Ādoni\", \"Rāiganj\", \"Bhusāval\", \"Bahraigh\", \"Shrīrāmpur\", \"Tonk\", \"Sirsa\", \"Jaunpur\", \"Madanapalle\", \"Hugli\", \"Vellore\", \"Alleppey\", \"Cuddalore\", \"Deo\", \"Chīrāla\", \"Machilīpatnam\", \"Medinīpur\", \"Bāramūla\", \"Chandannagar\", \"Fatehpur\", \"Udipi\", \"Tenāli\", \"Sitalpur\", \"Conjeeveram\", \"Proddatūr\", \"Navsāri\", \"Godhra\", \"Budaun\", \"Chittoor\", \"Harīpur\", \"Saharsa\", \"Vidisha\", \"Pathānkot\", \"Nalgonda\", \"Dibrugarh\", \"Bālurghāt\", \"Krishnanagar\", \"Fyzābād\", \"Silchar\", \"Shāntipur\", \"Hindupur\"]]\n",
    "]\n",
    "\n",
    "_cols = [\"id\", \"cities\"]\n",
    "\n",
    "# Create Data Frame\n",
    "df = spark.createDataFrame(data = _data, schema = _cols)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e9110174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_of_cities(col):\n",
    "    _len = 0\n",
    "    for i in col:\n",
    "        _len += len(i)\n",
    "    return _len\n",
    "\n",
    "#Above function is made into an udf using the lambda function\n",
    "\n",
    "len_of_cities_udf = udf(lambda x: len_of_cities(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7dae1f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------+\n",
      "| id|              cities|udf_len|\n",
      "+---+--------------------+-------+\n",
      "|  1|[Bangalore, Mumba...|     25|\n",
      "|  2|         [Bangalore]|      9|\n",
      "|  3|                  []|      0|\n",
      "|  4|[Kolkata, Bhubane...|     19|\n",
      "|  5|[Bangalore, Mumba...|     42|\n",
      "|  6|[Delhi, Mumbai, K...|   1806|\n",
      "+---+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"udf_len\", len_of_cities_udf(\"cities\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "85f49f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------------+\n",
      "| id|              cities|len_of_cities|\n",
      "+---+--------------------+-------------+\n",
      "|  1|[Bangalore, Mumba...|           25|\n",
      "|  2|         [Bangalore]|            9|\n",
      "|  3|                  []|            0|\n",
      "|  4|[Kolkata, Bhubane...|           19|\n",
      "|  5|[Bangalore, Mumba...|           42|\n",
      "|  6|[Delhi, Mumbai, K...|         1806|\n",
      "+---+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"len_of_cities\", aggregate(\"cities\", lit(0), lambda x, y: x + length(y))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "06e8a088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create the example dataset of fact and dimesion we would use for demonstration\n",
    "# Python program to generate random Fact table data\n",
    "# [1, ,\"ORD1001\", \"D102\", 56]\n",
    "import random\n",
    "\n",
    "\n",
    "def generate_fact_data(counter=100):\n",
    "    fact_records = []\n",
    "    dim_keys = [\"D100\", \"D101\", \"D102\", \"D103\", \"D104\"]\n",
    "    order_ids = [\"ORD\" + str(i) for i in range(1001, 1010)]\n",
    "    qty_range = [i for i in range(10, 120)]\n",
    "    for i in range(counter):\n",
    "        _record = [i, random.choice(order_ids), random.choice(dim_keys), random.choice(qty_range)]\n",
    "        fact_records.append(_record)\n",
    "    return fact_records\n",
    "\n",
    "# We will generate 200 records with random data in Fact to create skewness\n",
    "fact_records = generate_fact_data(200)\n",
    "\n",
    "dim_records = [\n",
    "    [\"D100\", \"Product A\"],\n",
    "    [\"D101\", \"Product B\"],\n",
    "    [\"D102\", \"Product C\"],\n",
    "    [\"D103\", \"Product D\"],\n",
    "    [\"D104\", \"Product E\"]\n",
    "]\n",
    "\n",
    "_fact_cols = [\"id\", \"order_id\", \"prod_id\", \"qty\"]\n",
    "_dim_cols = [\"prod_id\", \"prod_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6dd4bbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+---+\n",
      "| id|order_id|prod_id|qty|\n",
      "+---+--------+-------+---+\n",
      "|  0| ORD1003|   D103| 19|\n",
      "|  1| ORD1002|   D104| 29|\n",
      "+---+--------+-------+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_df = spark.createDataFrame(data=fact_records,schema=_fact_cols)\n",
    "fact_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "15ee17d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|prod_id|prod_name|\n",
      "+-------+---------+\n",
      "|   D100|Product A|\n",
      "|   D101|Product B|\n",
      "|   D102|Product C|\n",
      "|   D103|Product D|\n",
      "|   D104|Product E|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_df = spark.createDataFrame(data=dim_records,schema=_dim_cols)\n",
    "dim_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1b31efc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Set Spark parameters - We have to turn off AQL to demonstrate Salting\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    "\n",
    "# Check the parameters\n",
    "print(spark.conf.get(\"spark.sql.adaptive.enabled\"))\n",
    "print(spark.conf.get(\"spark.sql.shuffle.partitions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "081ceaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 132:==============>                                          (1 + 3) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------+---+---------+\n",
      "|prod_id| id|order_id|qty|prod_name|\n",
      "+-------+---+--------+---+---------+\n",
      "|   D103|  0| ORD1003| 19|Product D|\n",
      "|   D103|  2| ORD1007| 52|Product D|\n",
      "|   D103|100| ORD1003| 44|Product D|\n",
      "|   D103|151| ORD1004| 92|Product D|\n",
      "|   D103|152| ORD1001|119|Product D|\n",
      "|   D104|  1| ORD1002| 29|Product E|\n",
      "|   D104| 59| ORD1009| 44|Product E|\n",
      "|   D104|104| ORD1003| 38|Product E|\n",
      "|   D104|158| ORD1002| 73|Product E|\n",
      "|   D104|159| ORD1005| 80|Product E|\n",
      "+-------+---+--------+---+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinDF = fact_df.join(dim_df,on=\"prod_id\",how=\"leftouter\")\n",
    "joinDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b866084e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|partition_num|count(id)|\n",
      "+-------------+---------+\n",
      "|            4|      117|\n",
      "|            2|       83|\n",
      "+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "partition_df = joinDF.withColumn(\"partition_num\", \n",
    "                                    spark_partition_id())\\\n",
    "        .groupBy(\"partition_num\").agg(count(\"id\"))\n",
    "\n",
    "partition_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "53acd5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# UDF to return a random number every time\n",
    "def rand(): return random.randint(0, 4) #Since we are distributing the data in 5 partitions\n",
    "rand_udf = udf(rand)\n",
    "\n",
    "# Salt Data Frame to add to dimension\n",
    "salt_df = spark.range(0, 5)\n",
    "salt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c9ae4fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+---+--------------+\n",
      "|id |order_id|prod_id|qty|salted_prod_id|\n",
      "+---+--------+-------+---+--------------+\n",
      "|0  |ORD1003 |D103   |19 |D103_4        |\n",
      "|1  |ORD1002 |D104   |29 |D104_4        |\n",
      "|2  |ORD1007 |D103   |52 |D103_0        |\n",
      "|3  |ORD1001 |D101   |117|D101_0        |\n",
      "|4  |ORD1009 |D101   |105|D101_4        |\n",
      "|5  |ORD1004 |D101   |78 |D101_4        |\n",
      "|6  |ORD1003 |D101   |106|D101_0        |\n",
      "|7  |ORD1004 |D101   |56 |D101_3        |\n",
      "|8  |ORD1002 |D102   |119|D102_1        |\n",
      "|9  |ORD1008 |D102   |116|D102_0        |\n",
      "+---+--------+-------+---+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salted_fact_df = fact_df.withColumn(\"salted_prod_id\",\n",
    "                                    concat(\"prod_id\",lit(\"_\"), \n",
    "                                           lit(rand_udf())))\n",
    "salted_fact_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "53fdd836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---+\n",
      "|prod_id|prod_name| id|\n",
      "+-------+---------+---+\n",
      "|   D100|Product A|  0|\n",
      "|   D100|Product A|  1|\n",
      "+-------+---------+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salted_dim_df = dim_df.join(salt_df, how=\"cross\")\n",
    "salted_dim_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "325c36f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------+\n",
      "|prod_id|prod_name|salted_prod_id|\n",
      "+-------+---------+--------------+\n",
      "|   D100|Product A|        D100_0|\n",
      "|   D100|Product A|        D100_1|\n",
      "+-------+---------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salted_dim_df = dim_df.join(salt_df, how=\"cross\").withColumn(\"salted_prod_id\", concat(\"prod_id\", lit(\"_\"), \"id\")).drop(\"id\")\n",
    "\n",
    "salted_dim_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "98030261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+--------+-------+---+-------+---------+\n",
      "|salted_prod_id|id |order_id|prod_id|qty|prod_id|prod_name|\n",
      "+--------------+---+--------+-------+---+-------+---------+\n",
      "|D100_0        |107|ORD1004 |D100   |86 |D100   |Product A|\n",
      "|D101_1        |5  |ORD1004 |D101   |78 |D101   |Product B|\n",
      "|D101_1        |52 |ORD1005 |D101   |105|D101   |Product B|\n",
      "|D101_1        |102|ORD1004 |D101   |82 |D101   |Product B|\n",
      "|D103_1        |152|ORD1001 |D103   |119|D103   |Product D|\n",
      "|D103_3        |2  |ORD1007 |D103   |52 |D103   |Product D|\n",
      "|D103_3        |151|ORD1004 |D103   |92 |D103   |Product D|\n",
      "|D104_0        |59 |ORD1009 |D104   |44 |D104   |Product E|\n",
      "|D104_0        |159|ORD1005 |D104   |80 |D104   |Product E|\n",
      "|D104_2        |104|ORD1003 |D104   |38 |D104   |Product E|\n",
      "+--------------+---+--------+-------+---+-------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salted_joined_df = salted_fact_df.join(salted_dim_df, on=\"salted_prod_id\", how=\"leftouter\")\n",
    "salted_joined_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "50357645",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 174:============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|partition_num|count|\n",
      "+-------------+-----+\n",
      "|            0|   22|\n",
      "|            1|   56|\n",
      "|            2|   28|\n",
      "|            3|   59|\n",
      "|            4|   35|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "partition_df = salted_joined_df.withColumn(\"partition_num\", spark_partition_id()).groupBy(\"partition_num\") \\\n",
    "    .agg(count(lit(1)).alias(\"count\")).orderBy(\"partition_num\")\n",
    "\n",
    "partition_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce9b3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset read without specifying the schema\n",
    "\n",
    "df_sales = spark \\\n",
    "    .read \\\n",
    "    .format(\"parquet\") \\\n",
    "    .load(\"dataset/sales.parquet\")\n",
    "\n",
    "# Dataset read with schema\n",
    "_schema = \"transacted_at STRING, trx_id STRING, retailer_id STRING, description STRING, amount STRING, city_id STRING\"\n",
    "\n",
    "df_sales = spark \\\n",
    "    .read \\\n",
    "    .schema(_schema) \\\n",
    "    .format(\"parquet\") \\\n",
    "    .load(\"dataset/sales.parquet\")\n",
    "\n",
    "df_sales = spark \\\n",
    "    .read \\\n",
    "    .schema(_required_schema) \\\n",
    "    .format(\"parquet\") \\\n",
    "    .load(\"dataset/sales.parquet\")\n",
    "\n",
    "df_sales = spark \\\n",
    "    .read \\\n",
    "    .schema(_schema) \\\n",
    "    .parquet(\"dataset/sales.parquet\") \\\n",
    "    .select(\"transacted_at\", \"trx_id\", \"amount\")\n",
    "\n",
    "df_sales = spark \\\n",
    "    .read \\\n",
    "    .schema(_schema) \\\n",
    "    .parquet(\"dataset/sales.parquet\") \\\n",
    "    .drop(\"retailer_id\", \"description\", \"city_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed20616c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
