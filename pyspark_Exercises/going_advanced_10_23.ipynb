{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec74369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddf4fbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/24 21:40:45 WARN Utils: Your hostname, codeStation resolves to a loopback address: 127.0.1.1; using 192.168.88.83 instead (on interface wlo1)\n",
      "23/01/24 21:40:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/24 21:40:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/01/24 21:40:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"advance\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5caf749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basics_review_pyspark.ipynb\n",
      "going_advanced_10_15.ipynb\n",
      "nyc-collisions-2019-reduced.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc51f7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n",
      "true\n",
      "200\n",
      "67108864b\n"
     ]
    }
   ],
   "source": [
    "# Lets check the current spark conf for AQE and shuffle partitions\n",
    "print(spark.conf.get(\"spark.sql.adaptive.enabled\"))\n",
    "print(spark.conf.get(\"spark.sql.adaptive.coalescePartitions.enabled\"))\n",
    "print(spark.conf.get(\"spark.sql.shuffle.partitions\"))\n",
    "print(spark.conf.get(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\")) #approx 64MB Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a389df87",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e792f028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- borough: string (nullable = true)\n",
      " |-- zip: string (nullable = true)\n",
      " |-- injured: string (nullable = true)\n",
      " |-- killed: string (nullable = true)\n",
      " |-- cause: string (nullable = true)\n",
      "\n",
      "Initial Partition after read: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read example data set\n",
    "df = spark.read.format(\"csv\").option(\"header\", True).load(\"nyc-collisions-2019-reduced.csv\")\n",
    "df.printSchema()\n",
    "print(\"Initial Partition after read: \" + str(df.rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "949b0491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:>                                                          (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shuffle partitions: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:===================>                                       (1 + 2) / 3]\r"
     ]
    }
   ],
   "source": [
    "# GroupBy opeartion to trigger Shuffle\n",
    "# Since our output with city_id as group by is smaller than < 64MB thus the data is written in single partiton\n",
    "from pyspark.sql.functions import sum\n",
    "df_count = df.select(\"borough\",\"injured\").groupBy(\"borough\").agg(sum(\"injured\"))\n",
    "print(\"Output shuffle partitions: \" + str(df_count.rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7892210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shuffle partitions: 1\n"
     ]
    }
   ],
   "source": [
    "# GroupBy opeartion to trigger Shuffle but this time with zipcode(which is more unique - thus more data)\n",
    "# Since our output with zipcode as group by is > 64MB thus the data is written in multiple partitions\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "df_count = df.selectExpr(\"date\",\"killed\").groupBy(\"date\").agg(sum(\"killed\"))\n",
    "\n",
    "print(\"Output shuffle partitions: \" + str(df_count.rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91b354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zipped = spark \\\n",
    "    .read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"compression\", \"gzip\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .load(\"dataset/tmp/sales.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de7576c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- modificationTime: timestamp (nullable = true)\n",
      " |-- length: long (nullable = true)\n",
      " |-- content: binary (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+--------------------+\n",
      "|                path|    modificationTime|length|             content|\n",
      "+--------------------+--------------------+------+--------------------+\n",
      "|file:/run/media/s...|2023-01-24 21:54:...| 99936|[63 75 73 74 6F 6...|\n",
      "+--------------------+--------------------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_png = spark \\\n",
    "    .read \\\n",
    "    .format(\"binaryFile\") \\\n",
    "    .load(\"vizheads.csv\")\n",
    "\n",
    "df_spark_png.printSchema()\n",
    "df_spark_png.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80a6cc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets generate the text file back from the binary content\n",
    "byte_content = df_spark_png.select(\"content\").collect()[0][0]\n",
    "\n",
    "# Lets write the byte content as file back\n",
    "with open(\"new_example.txt\", \"w\") as f:\n",
    "    f.write(str(byte_content))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbc2f436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaining performance benefits\n",
    "# Set up to read from JDBC SQLite database\n",
    "driver: str = \"org.sqlite.JDBC\"\n",
    "db_path: str = \"dataset/jdbc/demo-sqlite.db\"\n",
    "jdbc_url: str = \"jdbc:sqlite:\" + db_path\n",
    "table_name: str = \"sales_csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c37593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets read the SQLite table using JDBC driver and validate the data\n",
    "df = spark \\\n",
    "    .read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\", driver) \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", table_name) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ecf9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the explain plan\n",
    "df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6099e632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def x():\n",
    "    df_full = spark \\\n",
    "    .read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\", driver) \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", table_name) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0887e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = spark \\\n",
    "    .read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\", driver) \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", table_name) \\\n",
    "    .load() \\\n",
    "    .filter(\"city_id = 216510442\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20d8ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    " df_filtered = spark \\\n",
    "    .read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\", driver) \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", table_name) \\\n",
    "    .load() \\\n",
    "    .cache() \\\n",
    "    .filter(\"city_id = 216510442\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ac0c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "pushDownQuery = \"\"\"(select city_id, count(1) as cnt from sales_csv group by city_id) as sales_csv\"\"\"\n",
    "@get_time\n",
    "def x():\n",
    "    df_filtered = spark \\\n",
    "    .read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\", driver) \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", pushDownQuery) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b3c319",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = spark \\\n",
    "    .read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\", driver) \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", table_name) \\\n",
    "    .load()\n",
    "\n",
    "df_full.selectExpr(\"min(trx_id) as min_trx_id\", \"max(trx_id) as max_trx_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e9a5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = spark \\\n",
    "    .read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\", driver) \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", table_name) \\\n",
    "    .option(\"partitionColumn\", \"trx_id\") \\\n",
    "    .option(\"lowerBound\", 20) \\\n",
    "    .option(\"upperBound\", 2147474653) \\\n",
    "    .option(\"numPartitions\", 8) \\\n",
    "    .load()\n",
    "    \n",
    "    df_full.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "    df_full.printSchema()\n",
    "    print(\"Number of Partitons: \"+ str(df_full.rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d39ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = spark \\\n",
    "    .read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\", driver) \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", table_name) \\\n",
    "    .option(\"partitionColumn\", \"trx_id\") \\\n",
    "    .option(\"lowerBound\", 20) \\\n",
    "    .option(\"upperBound\", 2147474653) \\\n",
    "    .option(\"numPartitions\", 8) \\\n",
    "    .option(\"fetchsize\", 8000) \\\n",
    "    .load()\n",
    "    \n",
    "    df_full.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "    df_full.printSchema()\n",
    "    print(\"Number of Partitons: \"+ str(df_full.rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21e7854",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Count(1) vs Count(*) vs Count(col_name)\n",
    "\n",
    "df.groupBy(\"trx_id\").agg(count(\"*\")).explain(True)\n",
    "\n",
    "df.groupBy(\"trx_id\").agg(count(lit(1))).explain(True)\n",
    "\n",
    "df.groupBy(\"trx_id\").agg(count(\"city_id\")).explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9cb8b6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type is: <class 'pyspark.broadcast.Broadcast'>\n",
      "{'D001': 'Department 1', 'D002': 'Department 2', 'D003': 'Department 3'}\n",
      "{'D001': 1990, 'D003': 2001}\n"
     ]
    }
   ],
   "source": [
    "# Broadcast Varible\n",
    "\n",
    "dept_names = {\"D001\": \"Department 1\", \"D002\": \"Department 2\", \"D003\": \"Department 3\"}\n",
    "dept_est = {\"D001\": 1990, \"D003\": 2001}\n",
    "\n",
    "broadcast_dept_names = spark.sparkContext.broadcast(dept_names)\n",
    "broadcast_dept_est = spark.sparkContext.broadcast(dept_est)\n",
    "\n",
    "# Check the type of the variable\n",
    "print(\"The type is: \" + str(type(broadcast_dept_est)))\n",
    "\n",
    "# In case we want to check the value\n",
    "print(broadcast_dept_names.value)\n",
    "print(broadcast_dept_est.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d41b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets use broadcast variable to use Dept info\n",
    "_new_schema = [\"NAME\", \"DEPT_CODE\", \"FAV_SUBJECT\", \"DEPT_NAME\", \"ESTD\"]\n",
    "\n",
    "# Use lambda function to iterate over row to get the broadcast value\n",
    "df = df_students.rdd.map(lambda row: [\n",
    "    row.NAME, \n",
    "    row.DEPT_CODE, \n",
    "    row.FAV_SUBJECT,\n",
    "    broadcast_dept_names.value.get(row.DEPT_CODE),\n",
    "    broadcast_dept_est.value.get(row.DEPT_CODE)]).toDF(_new_schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7704930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading partitioned files\n",
    "\n",
    "# Read with Schema\n",
    "_schema = \"transacted_at timestamp, trx_id int, retailer_id int, description string, amount decimal(38,18), city_id int, trx_year int, trx_month int, trx_date int\"\n",
    "df_2 = spark.read \\\n",
    "  .format(\"parquet\") \\\n",
    "  .schema(_schema) \\\n",
    "  .load(\"/user/hive/delta-warehouse/sales_partitioned.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073bdcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the data as table\n",
    "_create_sql = \"\"\"\n",
    "CREATE TABLE default.sales_partitioned USING PARQUET LOCATION '/user/hive/delta-warehouse/sales_partitioned.parquet'\n",
    "\"\"\"\n",
    "_refresh_sql = \"\"\"\n",
    "MSCK repair table default.sales_partitioned\n",
    "\"\"\"\n",
    "# Run the SQL commands\n",
    "spark.sql(_create_sql)\n",
    "spark.sql(_refresh_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48ac423",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This command is read from the catalog\n",
    "df_3 = spark.read.table(\"default.sales_partitioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bf965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the degree of parallelism\n",
    "spark.sparkContext.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f678f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable all AQE optimization for benchmarking tests\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.localShuffleReader.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228a7710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the degree of parallelism\n",
    "spark.sparkContext.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b64600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define shuffle partitions which is not Factor of core\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f70f6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not repartitiong with factor\n",
    "spark.read.format(\"parquet\").load(\"dataset/sales.parquet/\").repartition(9).write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1132ac",
   "metadata": {},
   "outputs": [],
   "source": [
    " Repartitiong based on factor of cores\n",
    "spark.read.format(\"parquet\").load(\"dataset/sales.parquet/\").repartition(8).write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5415c1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fix_header(df: DataFrame) -> list:\n",
    "    fixed_col_list: list = []\n",
    "    for col in df.columns:\n",
    "        fixed_col_list.append(f\"`{str(col).strip()}` as {str(col).strip().replace(' ','_').lower()}\")\n",
    "        \n",
    "    return fixed_col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba70357c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in-memory'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate if catalog is Hive\n",
    "spark.conf.get(\"spark.sql.catalogImplementation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a685059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Metastore Without HiveSupport\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"spark-warehouse\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bfc43f",
   "metadata": {},
   "source": [
    "## !pip install sparksql-magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ff34317",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparksql_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9f7d80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b508a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
