{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7ef2a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import boto3\n",
    "import awswrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20584558",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparker = SparkSession.builder.appName(\"stream reading\"). \\\n",
    "                getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e6f34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating sparksession that reads from the stream\n",
    "\n",
    "read_lines = sparker.readStream.format(\"socket\"). \\\n",
    "            option(\"host\",\"localhost\"). \\\n",
    "            option(\"port\",9999). \\\n",
    "            load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1b403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_lines.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a4b3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "counting_sheep = read_lines.select(split(\"value\",' ').alias('words')). \\\n",
    "                    groupBy('words'). \\\n",
    "                    agg(count('words').alias('word_count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0af9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "counting_sheep.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324308b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "writing_count = counting_sheep.writeStream \\\n",
    "            .format('console') \\\n",
    "            .outputMode('update') \\\n",
    "            .start()\n",
    "\n",
    "writing_count.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1097a407",
   "metadata": {},
   "outputs": [],
   "source": [
    "practiceList = [\n",
    "    [1,\"This line is the first of many\"],\n",
    "    [2,\"Machine is talking a lot\"],\n",
    "    [3,\"Making Machines speak has been the passion for many\"]\n",
    "]\n",
    "schema_df = [\"id int\",\"line string\"]\n",
    "practiceDF = sparker.createDataFrame(data=practiceList,schema=schema_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbdef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "practiceDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530dc6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_sample.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd172cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparker.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2db1d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "newSparker = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Streaming Process Files\") \\\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cda9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "newSparker.conf.set(\"spark.sql.streaming.schemaInference\", True)\n",
    "newSparker.conf.set(\"spark.sql.adaptive.enabled\",False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e6d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating sparksession that reads from the stream\n",
    "\n",
    "read_lines = newSparker.readStream.format(\"socket\"). \\\n",
    "            option(\"host\",\"localhost\"). \\\n",
    "            option(\"port\",9999). \\\n",
    "            load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f891874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can count the sheep jumping in the spark stream\n",
    "\n",
    "count_sheep_1 = read_lines.select(explode(split('value',' ')).alias('words')). \\\n",
    "                        groupBy('words'). \\\n",
    "                        agg(count('words').alias('count_words'))\n",
    "count_sheep_1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ddcdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To write the output to file watermarking is required\n",
    "\n",
    "count_sheep_1 = read_lines.select(explode(split('value',' ')).alias('words')). \\\n",
    "                        groupBy('words'). \\\n",
    "                        agg(count('words').alias('count_words'))\n",
    "count_sheep_1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cab958",
   "metadata": {},
   "outputs": [],
   "source": [
    "writeDf = count_sheep_1.writeStream.format(\"memory\"). \\\n",
    "        queryName('stream_table'). \\\n",
    "        outputMode(\"update\"). \\\n",
    "        start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1528dfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next lets get the output from the table\n",
    "\n",
    "newSparker.sql(\"SELECT * FROM stream_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6f0601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1e4d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "newtime = datetime.now().strftime('%H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a7f04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "newtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d94494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see if for each batch execution can be done\n",
    "count_sheep_1 = read_lines.select(explode(split('value',' ')).alias('words')). \\\n",
    "                        groupBy('words'). \\\n",
    "                        agg(count('words').alias('count_words')). \\\n",
    "                        withColumn('time_date', lit(datetime.now().strftime('%H:%M')))\n",
    "count_sheep_1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b643a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "writeDf = count_sheep_1.writeStream.format(\"memory\"). \\\n",
    "        queryName('time_table'). \\\n",
    "        outputMode(\"update\"). \\\n",
    "        start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2908e5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "writeDf = count_sheep_1.writeStream.format(\"memory\"). \\\n",
    "        queryName('wat_mar'). \\\n",
    "        outputMode(\"update\"). \\\n",
    "        start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fd4d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "newSparker.sql(\"SELECT * FROM time_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2127d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5c2f07",
   "metadata": {},
   "source": [
    "The chart updates after the new data is taken from the table and fed into the pandas dataframe. It is not real time streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688af041",
   "metadata": {},
   "outputs": [],
   "source": [
    "chartingData = newSparker.sql(\"SELECT * FROM time_table\").to_pandas_on_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb621091",
   "metadata": {},
   "outputs": [],
   "source": [
    "chartingData = chartingData[chartingData.words != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c6e3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "\n",
    "plt.pyplot.bar(x=chartingData['words'].to_numpy(),\n",
    "               height=chartingData['count_words'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25baed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before trying the file data ingestion, want to check water marking\n",
    "\n",
    "# Lets see if for each batch execution can be done\n",
    "count_sheep_wm = read_lines.select(explode(split('value',' ')).alias('words')). \\\n",
    "                        groupBy('words'). \\\n",
    "                        agg(count('words').alias('count_words')). \\\n",
    "                        withColumn('time_date', lit(datetime.now().strftime('%H:%M'))). \\\n",
    "                        withColumn('time_stamp', lit(datetime.now())). \\\n",
    "                        withWatermark('time_stamp','1 seconds')\n",
    "count_sheep_wm.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01953d51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_sheep_wm.writeStream.format(\"csv\"). \\\n",
    "            outputMode(\"append\"). \\\n",
    "            option(\"checkpointLocation\",\"checkpoint_dir\"). \\\n",
    "            option(\"path\",\"samplespace/\"). \\\n",
    "            start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87302937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreach_batch_function(df, epoch_id):\n",
    "    df.where(\"count_words > 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea3bdd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_sheep_wm.writeStream. \\\n",
    "            foreach(foreach_batch_function). \\\n",
    "            outputMode('')\n",
    "            start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da37bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileData= newSparker.readStream.format('csv'). \\\n",
    "    option(\"cleanSource\",\"archive\"). \\\n",
    "    option(\"sourceArchiveDir\",\"checkpoint_dir\"). \\\n",
    "    option(\"maxFilesPerTrigger\",1). \\\n",
    "    load('samplesource',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301fd266",
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedData = fileData.selectExpr(\"_c0 as date\",\"_c2 as borough\",\n",
    "                                   \"_c3 as zipcode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2293e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedData.writeStream.outputMode(\"append\").format('console') \\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0ad873",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "export PYSPARK_SUBMIT_ARGS=\"--master spark://192.168.2.40:7077\"\n",
    "#export PYSPARK_SUBMIT_ARGS=\"--master local[*] pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd17f586",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "echo $PYSPARK_SUBMIT_ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8629f71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/26 17:02:27 WARN Utils: Your hostname, codeStation resolves to a loopback address: 127.0.1.1; using 192.168.78.83 instead (on interface wlo1)\n",
      "23/01/26 17:02:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/solverbot/.local/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/solverbot/.ivy2/cache\n",
      "The jars for the packages stored in: /home/solverbot/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-04b6dea8-1441-42c8-80c8-2e53db3f5377;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.32 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.3.0/spark-sql-kafka-0-10_2.12-3.3.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0!spark-sql-kafka-0-10_2.12.jar (956ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.3.0/spark-token-provider-kafka-0-10_2.12-3.3.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0!spark-token-provider-kafka-0-10_2.12.jar (440ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.8.1/kafka-clients-2.8.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;2.8.1!kafka-clients.jar (4218ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (483ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (523ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (440ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.2/hadoop-client-runtime-3.3.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.2!hadoop-client-runtime.jar (19792ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (987ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.8.4/snappy-java-1.1.8.4.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.8.4!snappy-java.jar(bundle) (1680ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.32/slf4j-api-1.7.32.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.32!slf4j-api.jar (467ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.2/hadoop-client-api-3.3.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.2!hadoop-client-api.jar (12713ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...\n",
      "\t[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (510ms)\n",
      ":: resolution report :: resolve 32533ms :: artifacts dl 43237ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.32 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   12  |   12  |   0   ||   12  |   12  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-04b6dea8-1441-42c8-80c8-2e53db3f5377\n",
      "\tconfs: [default]\n",
      "\t12 artifacts copied, 0 already retrieved (56631kB/228ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/26 17:03:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Streaming from Kafka\") \\\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0') \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eae7367b",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_df = spark.readStream\\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"local_test\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "303a4afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_df = spark.read\\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"local_test\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9280503b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f93492cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, StructField, StructType, DecimalType\n",
    "\n",
    "json_schema = StructType([StructField('Date', StringType(), True), \\\n",
    "StructField('Open', DecimalType(), True), \\\n",
    "StructField('High', DecimalType(), True), \\\n",
    "StructField('Low', DecimalType(), True), \\\n",
    "StructField('Close', DecimalType(), True), \\\n",
    "StructField('Adj Close', DecimalType(), True), \\\n",
    "StructField('Adj Close', DecimalType(), True)])                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4413e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse value from binay to string\n",
    "json_df = streaming_df.selectExpr(\"cast(value as string) as value\")\n",
    "\n",
    "# Apply Schema to JSON value column and expand the value\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "json_expanded_df = json_df.withColumn(\"value\", from_json(json_df[\"value\"], json_schema)).select(\"value.*\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccf46ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+---+-----+---------+---------+\n",
      "|      Date|Open|High|Low|Close|Adj Close|Adj Close|\n",
      "+----------+----+----+---+-----+---------+---------+\n",
      "|2017-06-21| 156| 156|153|  154|     null|      148|\n",
      "|2015-02-27| 161| 162|160|  162|     null|      141|\n",
      "|2013-08-07| 190| 190|188|  189|     null|      158|\n",
      "|2017-12-21| 153| 153|151|  152|     null|      148|\n",
      "|2014-03-10| 188| 188|186|  186|     null|      158|\n",
      "|2013-05-31| 209| 212|208|  208|     null|      174|\n",
      "|2016-07-14| 159| 161|159|  160|     null|      148|\n",
      "|2015-02-10| 157| 159|155|  159|     null|      138|\n",
      "|2014-09-17| 193| 194|192|  193|     null|      166|\n",
      "|2016-06-16| 150| 151|149|  151|     null|      140|\n",
      "+----------+----+----+---+-----+---------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "json_expanded_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ad160d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df = json_expanded_df.where(\"Open > 150\") \\\n",
    "    .withColumn(\"tradeDate\", to_date(\"Date\", \"yyyy-MM-dd\")) \\\n",
    "    .select(\"Open\",\"High\",\"Low\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a469a476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Open: decimal(10,0) (nullable = true)\n",
      " |-- High: decimal(10,0) (nullable = true)\n",
      " |-- Low: decimal(10,0) (nullable = true)\n",
      "\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "| 156| 156|153|\n",
      "| 161| 162|160|\n",
      "| 190| 190|188|\n",
      "| 153| 153|151|\n",
      "| 188| 188|186|\n",
      "| 209| 212|208|\n",
      "| 159| 161|159|\n",
      "| 157| 159|155|\n",
      "| 193| 194|192|\n",
      "| 156| 156|154|\n",
      "| 161| 161|160|\n",
      "| 198| 198|193|\n",
      "| 178| 178|177|\n",
      "| 177| 178|176|\n",
      "| 191| 194|191|\n",
      "| 180| 181|179|\n",
      "| 183| 185|183|\n",
      "| 175| 178|175|\n",
      "| 155| 155|153|\n",
      "| 194| 195|191|\n",
      "+----+----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check the schema of the agg_df, post a kafka message and change readStream to read \n",
    "filter_df.printSchema()\n",
    "filter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1bf2058d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/26 17:55:36 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "| 156| 156|153|\n",
      "| 161| 162|160|\n",
      "| 190| 190|188|\n",
      "| 153| 153|151|\n",
      "| 188| 188|186|\n",
      "| 209| 212|208|\n",
      "| 159| 161|159|\n",
      "| 157| 159|155|\n",
      "| 193| 194|192|\n",
      "| 156| 156|154|\n",
      "| 161| 161|160|\n",
      "| 198| 198|193|\n",
      "| 178| 178|177|\n",
      "| 177| 178|176|\n",
      "| 191| 194|191|\n",
      "| 180| 181|179|\n",
      "| 183| 185|183|\n",
      "| 175| 178|175|\n",
      "| 155| 155|153|\n",
      "| 194| 195|191|\n",
      "+----+----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 4:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "| 165| 166|164|\n",
      "| 182| 183|181|\n",
      "| 191| 192|191|\n",
      "| 182| 184|182|\n",
      "| 168| 170|168|\n",
      "| 157| 157|154|\n",
      "| 167| 169|167|\n",
      "+----+----+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "| 174| 175|174|\n",
      "| 160| 162|160|\n",
      "| 167| 168|166|\n",
      "+----+----+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "| 153| 153|152|\n",
      "+----+----+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "| 156| 157|153|\n",
      "+----+----+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "| 182| 183|182|\n",
      "+----+----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "| 152| 153|150|\n",
      "+----+----+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "| 154| 154|153|\n",
      "+----+----+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "| 188| 188|186|\n",
      "+----+----+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "+----+----+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 10\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "| 152| 153|152|\n",
      "+----+----+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 11\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "+----+----+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 12\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "+----+----+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 13\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "| 156| 157|155|\n",
      "+----+----+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 14\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "| 156| 156|153|\n",
      "+----+----+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 15\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "+----+----+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 16\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "| 182| 183|182|\n",
      "+----+----+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 17\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "| 174| 175|174|\n",
      "+----+----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 21:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 18\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "| 151| 153|151|\n",
      "+----+----+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 19\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "| 174| 175|174|\n",
      "+----+----+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 20\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "| 172| 175|172|\n",
      "+----+----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 24:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 21\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "| 151| 151|149|\n",
      "+----+----+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 22\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "| 158| 158|155|\n",
      "+----+----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 26:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 23\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "| 151| 153|151|\n",
      "+----+----+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 24\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "| 154| 156|154|\n",
      "+----+----+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 25\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "| 192| 192|190|\n",
      "+----+----+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 26\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "| 158| 161|158|\n",
      "+----+----+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 27\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "+----+----+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 28\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "| 181| 181|179|\n",
      "+----+----+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 29\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "+----+----+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 30\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "| 193| 194|192|\n",
      "+----+----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 34:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 31\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "| 162| 163|160|\n",
      "+----+----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 35:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 32\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "| 161| 162|160|\n",
      "+----+----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 36:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 33\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "| 189| 190|189|\n",
      "+----+----+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 34\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "| 164| 164|163|\n",
      "+----+----+---+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 35\n",
      "-------------------------------------------\n",
      "+----+----+---+\n",
      "|Open|High|Low|\n",
      "+----+----+---+\n",
      "| 190| 190|189|\n",
      "+----+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "writing_df = filter_df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"checkpointLocation\",\"checkpoint_dir\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fff8e73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbf3a63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c00381",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
