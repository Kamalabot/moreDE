{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "568a93a5",
   "metadata": {},
   "source": [
    "**Store Sales Forecasting** an ongoing Kaggle competition that I \n",
    "have decided to use pyspark to load, data model, analyse and then \n",
    "move it into data modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ab19d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2262440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#credentials = \"postgresql://{}:{}@{}:{}/{}\".format(user,passwd,host,port,db)\n",
    "\n",
    "#using psycopg2 to test connection since there are no tables\n",
    "#import psycopg2\n",
    "#try:\n",
    " #   conn = psycopg2.connect(host=host,dbname=db,user=user,password=passwd,port=port)\n",
    "#except Exception as e:\n",
    " #   print(e)\n",
    "    \n",
    "#conn.set_session(autocommit=True)\n",
    "\n",
    "#try:\n",
    " #   cur = conn.cursor()\n",
    "    \n",
    "#except:\n",
    " #   print(e)\n",
    "    \n",
    "#Helper functions to work with the database\n",
    "def schemaGen(dataframe, schemaName):\n",
    "    localSchema = pd.io.sql.get_schema(dataframe,schemaName)\n",
    "    localSchema = localSchema.replace('TEXT','VARCHAR(255)').replace('INTEGER','NUMERIC').replace('\\n','').replace('\"',\"\")\n",
    "    return \"\".join(localSchema)\n",
    "\n",
    "#Using pandas read_sql for getting schema\n",
    "def getSchema(tableName, credentials):\n",
    "    schema = pd.read_sql(\"\"\"SELECT * FROM information_schema.columns where table_name='{}'\"\"\".format(tableName),con=credentials)\n",
    "    return schema\n",
    "\n",
    "#Issue is in using pd.read_sql to write data to the database. so using psycopg2\n",
    "def queryTable(query):\n",
    "    try:\n",
    "        schema = cur.execute(query)\n",
    "        return \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "#This doesn't return anything\n",
    "\n",
    "#Using the pd.read_sql for getting data from db\n",
    "def queryBase(query):\n",
    "    requiredTable = pd.read_sql(query,con=credentials)\n",
    "    return requiredTable\n",
    "\n",
    "#This returns the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5de618",
   "metadata": {},
   "source": [
    "#I am maintaining the above psycopg code, just in case \n",
    "#it is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df1823ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a97651d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SalesFC_Datamodeling_Pyspark.ipynb\n",
      "store-sales-time-series-forecasting.zip\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b476c431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bring in the data\n",
    "\n",
    "import shutil\n",
    "shutil.unpack_archive('store-sales-time-series-forecasting.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c318fdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## That should unpack all the data for our consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c549170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holidays_events.csv\n",
      "oil.csv\n",
      "SalesFC_Datamodeling_Pyspark.ipynb\n",
      "sample_submission.csv\n",
      "store-sales-time-series-forecasting.zip\n",
      "stores.csv\n",
      "test.csv\n",
      "train.csv\n",
      "transactions.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "671d2845",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets assign var names to the source files for easy references\n",
    "\n",
    "holidays = 'holidays_events.csv'\n",
    "oil = 'oil.csv'\n",
    "stores = 'stores.csv'\n",
    "train = 'train.csv'\n",
    "txn = 'transactions.csv'\n",
    "#We wont be needing those for quite some time\n",
    "test = 'test.csv'\n",
    "sample = 'sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b397c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/01 04:35:24 WARN Utils: Your hostname, codeStation resolves to a loopback address: 127.0.1.1; using 172.17.0.1 instead (on interface docker0)\n",
      "23/01/01 04:35:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/01 04:35:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#starting the spark session and getting the database setup.\n",
    "\n",
    "spark = SparkSession.builder.appName('sales_fc').getOrCreate()\n",
    "sparkql= spark.sql\n",
    "sparkreader = spark.read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7dba59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------------------------------------------------------------------------------+\n",
      "|key                    |value                                                                                      |\n",
      "+-----------------------+-------------------------------------------------------------------------------------------+\n",
      "|spark.sql.warehouse.dir|file:/run/media/solverbot/repoA/gitFolders/moreDE/store_sales_data_analysis/spark-warehouse|\n",
      "+-----------------------+-------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkql(\"SET spark.sql.warehouse.dir\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f89be5fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating local database, even though not having hive file system\n",
    "sparkql(\"CREATE DATABASE IF NOT EXISTS sales_forecast\")\n",
    "sparkql(\"USE sales_forecast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f42ca424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Reading in the data\n",
    "holidays_data = sparkreader.csv(holidays,inferSchema=True,header=True)\n",
    "oil_data = sparkreader.csv(oil,inferSchema=True,header=True)\n",
    "stores_data = sparkreader.csv(stores,inferSchema=True,header=True)\n",
    "train_data = sparkreader.csv(train,inferSchema=True,header=True)\n",
    "txn_data = sparkreader.csv(txn,inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641809d3",
   "metadata": {},
   "source": [
    "Anything that is outside the database is data, once it is \n",
    "inside then it is a table. That will keep things separate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa781d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create temp views of the tables first. \n",
    "holidays_data.createOrReplaceTempView(\"holidays_table\")\n",
    "oil_data.createOrReplaceTempView(\"oil_table\")\n",
    "stores_data.createOrReplaceTempView(\"stores_table\")\n",
    "train_data.createOrReplaceTempView(\"train_table\")\n",
    "txn_data.createOrReplaceTempView(\"txn_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdac4b11",
   "metadata": {},
   "source": [
    "The temp tables are dropped like the usual sql tables. sparkql(\"DROP TABLE holidays_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ac81a17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+-----------+\n",
      "|namespace|     tableName|isTemporary|\n",
      "+---------+--------------+-----------+\n",
      "|         |    date_table|       true|\n",
      "|         |full_oil_table|       true|\n",
      "|         |holidays_table|       true|\n",
      "|         |     oil_table|       true|\n",
      "|         |  stores_table|       true|\n",
      "|         |   train_table|       true|\n",
      "|         |     txn_table|       true|\n",
      "+---------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We have the data inside the spark data base to start manipulation\n",
    "#using sql\n",
    "sparkql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f8c167",
   "metadata": {},
   "source": [
    "Remove the *.csv files from the local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "abc79bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "rm -f *.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a92777",
   "metadata": {},
   "source": [
    "### Lets get to know the data... one table at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e10fdd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 12:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+--------+-----------+--------------------+-----------+\n",
      "|               date|   type|  locale|locale_name|         description|transferred|\n",
      "+-------------------+-------+--------+-----------+--------------------+-----------+\n",
      "|2012-03-02 00:00:00|Holiday|   Local|      Manta|  Fundacion de Manta|      false|\n",
      "|2012-04-01 00:00:00|Holiday|Regional|   Cotopaxi|Provincializacion...|      false|\n",
      "|2012-04-12 00:00:00|Holiday|   Local|     Cuenca| Fundacion de Cuenca|      false|\n",
      "|2012-04-14 00:00:00|Holiday|   Local|   Libertad|Cantonizacion de ...|      false|\n",
      "|2012-04-21 00:00:00|Holiday|   Local|   Riobamba|Cantonizacion de ...|      false|\n",
      "+-------------------+-------+--------+-----------+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sparkql(\"select * from holidays_data limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a59c3068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Observe there are multiple categories, type, locale, locale_name\n",
    "holidays_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7af014fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+--------+-----------+\n",
      "|categ_counts|      type|  locale|locale_name|\n",
      "+------------+----------+--------+-----------+\n",
      "|          12|   Holiday|   Local|     Ambato|\n",
      "|           6|   Holiday|   Local|    Cayambe|\n",
      "|           6|   Holiday|Regional|   Cotopaxi|\n",
      "|           6|   Holiday|   Local|     Cuenca|\n",
      "|           1|  Transfer|   Local|     Cuenca|\n",
      "|          40|Additional|National|    Ecuador|\n",
      "|          56|     Event|National|    Ecuador|\n",
      "|           8|  Transfer|National|    Ecuador|\n",
      "|          60|   Holiday|National|    Ecuador|\n",
      "|           5|  Work Day|National|    Ecuador|\n",
      "|           5|    Bridge|National|    Ecuador|\n",
      "|           6|   Holiday|   Local|  El Carmen|\n",
      "|           6|   Holiday|   Local| Esmeraldas|\n",
      "|          12|   Holiday|   Local|   Guaranda|\n",
      "|           5|   Holiday|   Local|  Guayaquil|\n",
      "|           5|Additional|   Local|  Guayaquil|\n",
      "|           1|  Transfer|   Local|  Guayaquil|\n",
      "|           1|  Transfer|   Local|     Ibarra|\n",
      "|           6|   Holiday|   Local|     Ibarra|\n",
      "|           6|   Holiday|Regional|   Imbabura|\n",
      "+------------+----------+--------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkql(\"\"\"select count(*) as categ_counts, hd.type, \\\n",
    "            hd.locale,hd.locale_name \\\n",
    "            from holidays_table hd \\\n",
    "            group by hd.type,hd.locale,hd.locale_name \n",
    "            order by hd.locale_name\n",
    "            \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb5e345e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1218"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oil_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4153ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-18,48.59\n",
      "2017-08-21,47.39\n",
      "2017-08-22,47.65\n",
      "2017-08-23,48.45\n",
      "2017-08-24,47.24\n",
      "2017-08-25,47.65\n",
      "2017-08-28,46.4\n",
      "2017-08-29,46.46\n",
      "2017-08-30,45.96\n",
      "2017-08-31,47.26\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "tail oil.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73c69ca",
   "metadata": {},
   "source": [
    "### The data is available till Aug'17 starting from Jan'13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "105ddd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n",
      "|               date|dcoilwtico|\n",
      "+-------------------+----------+\n",
      "|2013-01-01 00:00:00|      null|\n",
      "|2013-01-02 00:00:00|     93.14|\n",
      "|2013-01-03 00:00:00|     92.97|\n",
      "|2013-01-04 00:00:00|     93.12|\n",
      "|2013-01-07 00:00:00|      93.2|\n",
      "+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkql(\"select * from oil_table limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3657885d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------------------+----+-------+\n",
      "|store_nbr|         city|               state|type|cluster|\n",
      "+---------+-------------+--------------------+----+-------+\n",
      "|        1|        Quito|           Pichincha|   D|     13|\n",
      "|        2|        Quito|           Pichincha|   D|     13|\n",
      "|        3|        Quito|           Pichincha|   D|      8|\n",
      "|        4|        Quito|           Pichincha|   D|      9|\n",
      "|        5|Santo Domingo|Santo Domingo de ...|   D|      4|\n",
      "+---------+-------------+--------------------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkql(\"select * from stores_table limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90ea9b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "261eb0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+---------+----------+-----+-----------+\n",
      "| id|               date|store_nbr|    family|sales|onpromotion|\n",
      "+---+-------------------+---------+----------+-----+-----------+\n",
      "|  0|2013-01-01 00:00:00|        1|AUTOMOTIVE|  0.0|          0|\n",
      "|  1|2013-01-01 00:00:00|        1| BABY CARE|  0.0|          0|\n",
      "|  2|2013-01-01 00:00:00|        1|    BEAUTY|  0.0|          0|\n",
      "|  3|2013-01-01 00:00:00|        1| BEVERAGES|  0.0|          0|\n",
      "|  4|2013-01-01 00:00:00|        1|     BOOKS|  0.0|          0|\n",
      "+---+-------------------+---------+----------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkql(\"select * from train_table limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80c82b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3000888"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1f9819a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+------------+\n",
      "|               date|store_nbr|transactions|\n",
      "+-------------------+---------+------------+\n",
      "|2013-01-01 00:00:00|       25|         770|\n",
      "|2013-01-02 00:00:00|        1|        2111|\n",
      "|2013-01-02 00:00:00|        2|        2358|\n",
      "|2013-01-02 00:00:00|        3|        3487|\n",
      "|2013-01-02 00:00:00|        4|        1922|\n",
      "+-------------------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkql(\"SELECT * FROM txn_table LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d8687cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83488"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txn_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e744fa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(123, 1, \"01/01/2021\",),\n",
    "        (123, 0, \"01/02/2021\",),\n",
    "        (123, 1, \"01/03/2021\",),\n",
    "        (123, 0, \"01/06/2021\",),\n",
    "        (123, 0, \"01/08/2021\",),\n",
    "        (777, 0, \"01/01/2021\",),\n",
    "        (777, 1, \"01/03/2021\",), ]\n",
    "\n",
    "df = spark.createDataFrame(data, (\"ID\", \"FLAG\", \"DATE\",)) \\\n",
    "        .withColumn(\"DATE\", to_date(col(\"DATE\"), \"dd/MM/yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b53a9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+\n",
      "| ID|FLAG|      DATE|\n",
      "+---+----+----------+\n",
      "|123|   1|2021-01-01|\n",
      "|123|   0|2021-02-01|\n",
      "+---+----+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "90c38093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------------------+\n",
      "|           min_date|           max_date|            interval|\n",
      "+-------------------+-------------------+--------------------+\n",
      "|2013-01-01 00:00:00|2017-08-31 00:00:00|INTERVAL '-1703 0...|\n",
      "+-------------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkql(\"\"\"SELECT MIN(date) as min_date,MAX(date) as max_date,\n",
    "                MIN(date) - MAX(date) as interval\n",
    "                from oil_table\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ebb51aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dates_df = df.groupBy(\"id\").agg(\n",
    "    date_trunc(\"mm\", max(to_date(\"date\", \"dd/MM/yyyy\"))).\\\n",
    "            alias(\"max_date\"),\n",
    "    date_trunc(\"mm\", min(to_date(\"date\", \"dd/MM/yyyy\"))). \\\n",
    "            alias(\"min_date\")). \\\n",
    "    select(\"id\",expr(\"sequence(min_date, max_date, interval 1 month)\").alias(\"date_seq\")). \\\n",
    "        withColumn(\"date_new\",explode(\"date_seq\")). \\\n",
    "        withColumn(\"date_form\",date_format(\"date_new\", \"dd/MM/yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7248c4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|           max_date|           min_date|\n",
      "+-------------------+-------------------+\n",
      "|2017-08-01 00:00:00|2013-01-01 00:00:00|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oil_data.select(date_trunc(\"mm\", max(to_date(\"date\", \"dd/MM/yyyy\"))).\\\n",
    "            alias(\"max_date\"),\n",
    "            date_trunc(\"mm\", min(to_date(\"date\", \"dd/MM/yyyy\"))). \\\n",
    "            alias(\"min_date\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b154ac1e",
   "metadata": {},
   "source": [
    "### Creating the date sequence that we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ec5cec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_date_series = oil_data.select(date_trunc(\"mm\", max(to_date(\"date\", \"dd/MM/yyyy\"))).\\\n",
    "            alias(\"max_date\"),\n",
    "            date_trunc(\"mm\", min(to_date(\"date\", \"dd/MM/yyyy\"))). \\\n",
    "            alias(\"min_date\")). \\\n",
    "    select(expr(\"sequence(min_date, max_date, interval 1 day)\").alias(\"date_seq\")). \\\n",
    "        withColumn(\"date_new\",explode(\"date_seq\")). \\\n",
    "        withColumn(\"date_form\",date_format(\"date_new\", \"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7dc50800",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_series=data_date_series.drop(\"date_seq\",\"date_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "020a44b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1674"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_series.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61b75b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_series.co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3194e24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_series.createOrReplaceTempView('date_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "595a213c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| date_form|\n",
      "+----------+\n",
      "|2013-01-01|\n",
      "|2013-01-02|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkql(\"\"\"SELECT date_form \n",
    "            from date_table\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "469ead27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n",
      "|               date|dcoilwtico|\n",
      "+-------------------+----------+\n",
      "|2013-01-01 00:00:00|      null|\n",
      "|2013-01-02 00:00:00|     93.14|\n",
      "+-------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkql(\"\"\"SELECT date, dcoilwtico\n",
    "            from oil_table\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1636e588",
   "metadata": {},
   "source": [
    "### Build the tables SQL style:Not so fast. \n",
    "\n",
    "In Spark SQL implementation the constraints like\n",
    "Primary, Secondary is not established. The PR has been already raised in tho ASF though.\n",
    "\n",
    "sparkql(\"\"\" CREATE TABLE full_oil_table AS\n",
    "        \n",
    "        SELECT date_form, COALESCE(dcoilwtico,0) as dcoilwtico\n",
    "        \n",
    "        FROM date_table dt LEFT JOIN oil_table ot\n",
    "        \n",
    "        ON dt.date_form = ot.date\"\"\")\n",
    "        \n",
    "        \n",
    "The above command requires hive support, and errors out. We cannot create fully constrained tables in spark context. We have to do it in RDBMS environment if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d768de30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resorting to the Temp view creation route instead\n",
    "sparkql(\"\"\" SELECT date_form, COALESCE(dcoilwtico,0) as dcoilwtico\n",
    "        FROM date_table dt LEFT JOIN oil_table ot\n",
    "        ON dt.date_form = ot.date\"\"\"). \\\n",
    "    createOrReplaceTempView('full_oil_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cd43a987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "| date_form|dcoilwtico|\n",
      "+----------+----------+\n",
      "|2013-01-01|       0.0|\n",
      "|2013-01-02|     93.14|\n",
      "+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating table the sql style\n",
    "sparkql(\"\"\"SELECT * \n",
    "            FROM full_oil_table\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e733d5d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948cd246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dab1468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628e9a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcc636d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
