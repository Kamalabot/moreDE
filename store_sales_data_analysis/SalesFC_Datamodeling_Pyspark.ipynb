{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "568a93a5",
   "metadata": {},
   "source": [
    "**Store Sales Forecasting** an ongoing Kaggle competition that I \n",
    "have decided to use pyspark to load, data model, analyse and then \n",
    "move it into data modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ab19d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2262440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#credentials = \"postgresql://{}:{}@{}:{}/{}\".format(user,passwd,host,port,db)\n",
    "\n",
    "#using psycopg2 to test connection since there are no tables\n",
    "#import psycopg2\n",
    "#try:\n",
    " #   conn = psycopg2.connect(host=host,dbname=db,user=user,password=passwd,port=port)\n",
    "#except Exception as e:\n",
    " #   print(e)\n",
    "    \n",
    "#conn.set_session(autocommit=True)\n",
    "\n",
    "#try:\n",
    " #   cur = conn.cursor()\n",
    "    \n",
    "#except:\n",
    " #   print(e)\n",
    "    \n",
    "#Helper functions to work with the database\n",
    "def schemaGen(dataframe, schemaName):\n",
    "    localSchema = pd.io.sql.get_schema(dataframe,schemaName)\n",
    "    localSchema = localSchema.replace('TEXT','VARCHAR(255)').replace('INTEGER','NUMERIC').replace('\\n','').replace('\"',\"\")\n",
    "    return \"\".join(localSchema)\n",
    "\n",
    "#Using pandas read_sql for getting schema\n",
    "def getSchema(tableName, credentials):\n",
    "    schema = pd.read_sql(\"\"\"SELECT * FROM information_schema.columns where table_name='{}'\"\"\".format(tableName),con=credentials)\n",
    "    return schema\n",
    "\n",
    "#Issue is in using pd.read_sql to write data to the database. so using psycopg2\n",
    "def queryTable(query):\n",
    "    try:\n",
    "        schema = cur.execute(query)\n",
    "        return \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "#This doesn't return anything\n",
    "\n",
    "#Using the pd.read_sql for getting data from db\n",
    "def queryBase(query):\n",
    "    requiredTable = pd.read_sql(query,con=credentials)\n",
    "    return requiredTable\n",
    "\n",
    "#This returns the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5de618",
   "metadata": {},
   "source": [
    "#I am maintaining the above psycopg code, just in case \n",
    "#it is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df1823ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a97651d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_interpolation_pyspark.ipynb\n",
      "SalesFC_Datamodeling_Pyspark.ipynb\n",
      "spark-warehouse\n",
      "store-sales-time-series-forecasting.zip\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b476c431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bring in the data\n",
    "\n",
    "import shutil\n",
    "shutil.unpack_archive('store-sales-time-series-forecasting.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c318fdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## That should unpack all the data for our consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c549170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_interpolation_pyspark.ipynb\n",
      "holidays_events.csv\n",
      "oil.csv\n",
      "SalesFC_Datamodeling_Pyspark.ipynb\n",
      "sample_submission.csv\n",
      "spark-warehouse\n",
      "store-sales-time-series-forecasting.zip\n",
      "stores.csv\n",
      "test.csv\n",
      "train.csv\n",
      "transactions.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "671d2845",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets assign var names to the source files for easy references\n",
    "\n",
    "holidays = 'holidays_events.csv'\n",
    "oil = 'oil.csv'\n",
    "stores = 'stores.csv'\n",
    "train = 'train.csv'\n",
    "txn = 'transactions.csv'\n",
    "#We wont be needing those for quite some time\n",
    "test = 'test.csv'\n",
    "sample = 'sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b397c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/01 10:25:11 WARN Utils: Your hostname, codeStation resolves to a loopback address: 127.0.1.1; using 192.168.192.83 instead (on interface wlo1)\n",
      "23/01/01 10:25:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/01 10:25:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#starting the spark session and getting the database setup.\n",
    "\n",
    "spark = SparkSession.builder.appName('sales_fc').getOrCreate()\n",
    "sparkql= spark.sql\n",
    "sparkreader = spark.read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7dba59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------------------------------------------------------------------------------+\n",
      "|key                    |value                                                                                      |\n",
      "+-----------------------+-------------------------------------------------------------------------------------------+\n",
      "|spark.sql.warehouse.dir|file:/run/media/solverbot/repoA/gitFolders/moreDE/store_sales_data_analysis/spark-warehouse|\n",
      "+-----------------------+-------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkql(\"SET spark.sql.warehouse.dir\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f89be5fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating local database, even though not having hive file system\n",
    "sparkql(\"CREATE DATABASE IF NOT EXISTS sales_forecast\")\n",
    "sparkql(\"USE sales_forecast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f42ca424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Reading in the data\n",
    "holidays_data = sparkreader.csv(holidays,inferSchema=True,header=True)\n",
    "oil_data = sparkreader.csv(oil,inferSchema=True,header=True)\n",
    "stores_data = sparkreader.csv(stores,inferSchema=True,header=True)\n",
    "train_data = sparkreader.csv(train,inferSchema=True,header=True)\n",
    "txn_data = sparkreader.csv(txn,inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641809d3",
   "metadata": {},
   "source": [
    "Anything that is outside the database is data, once it is \n",
    "inside then it is a table. That will keep things separate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa781d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create temp views of the tables first. \n",
    "holidays_data.createOrReplaceTempView(\"holidays_table\")\n",
    "oil_data.createOrReplaceTempView(\"oil_table\")\n",
    "stores_data.createOrReplaceTempView(\"stores_table\")\n",
    "train_data.createOrReplaceTempView(\"train_table\")\n",
    "txn_data.createOrReplaceTempView(\"txn_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdac4b11",
   "metadata": {},
   "source": [
    "The temp tables are dropped like the usual sql tables. sparkql(\"DROP TABLE holidays_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac81a17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+-----------+\n",
      "|namespace|     tableName|isTemporary|\n",
      "+---------+--------------+-----------+\n",
      "|         |holidays_table|       true|\n",
      "|         |     oil_table|       true|\n",
      "|         |  stores_table|       true|\n",
      "|         |   train_table|       true|\n",
      "|         |     txn_table|       true|\n",
      "+---------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We have the data inside the spark data base to start manipulation\n",
    "#using sql\n",
    "sparkql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39570334",
   "metadata": {},
   "source": [
    "### Execute this code only when comitting the code to repo\n",
    "\n",
    "Remove the *.csv files from the local directory. \n",
    "This saves the space, and bandwidth when the code along with the data is committed to the repository.The extraction code is implemented using the shutil above. The readers can always use that inflate the zip file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e3f0e287",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "rm -f *.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a92777",
   "metadata": {},
   "source": [
    "### Lets get to know the data... one table at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1e10fdd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+--------+-----------+--------------------+-----------+\n",
      "|               date|   type|  locale|locale_name|         description|transferred|\n",
      "+-------------------+-------+--------+-----------+--------------------+-----------+\n",
      "|2012-03-02 00:00:00|Holiday|   Local|      Manta|  Fundacion de Manta|      false|\n",
      "|2012-04-01 00:00:00|Holiday|Regional|   Cotopaxi|Provincializacion...|      false|\n",
      "|2012-04-12 00:00:00|Holiday|   Local|     Cuenca| Fundacion de Cuenca|      false|\n",
      "|2012-04-14 00:00:00|Holiday|   Local|   Libertad|Cantonizacion de ...|      false|\n",
      "|2012-04-21 00:00:00|Holiday|   Local|   Riobamba|Cantonizacion de ...|      false|\n",
      "+-------------------+-------+--------+-----------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkql(\"select * from holidays_table limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a59c3068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Observe there are multiple categories, type, locale, locale_name\n",
    "holidays_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7af014fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+--------+-----------+\n",
      "|categ_counts|      type|  locale|locale_name|\n",
      "+------------+----------+--------+-----------+\n",
      "|          12|   Holiday|   Local|     Ambato|\n",
      "|           6|   Holiday|   Local|    Cayambe|\n",
      "|           6|   Holiday|Regional|   Cotopaxi|\n",
      "|           6|   Holiday|   Local|     Cuenca|\n",
      "|           1|  Transfer|   Local|     Cuenca|\n",
      "|          40|Additional|National|    Ecuador|\n",
      "|          56|     Event|National|    Ecuador|\n",
      "|           8|  Transfer|National|    Ecuador|\n",
      "|          60|   Holiday|National|    Ecuador|\n",
      "|           5|  Work Day|National|    Ecuador|\n",
      "|           5|    Bridge|National|    Ecuador|\n",
      "|           6|   Holiday|   Local|  El Carmen|\n",
      "|           6|   Holiday|   Local| Esmeraldas|\n",
      "|          12|   Holiday|   Local|   Guaranda|\n",
      "|           5|   Holiday|   Local|  Guayaquil|\n",
      "|           5|Additional|   Local|  Guayaquil|\n",
      "|           1|  Transfer|   Local|  Guayaquil|\n",
      "|           1|  Transfer|   Local|     Ibarra|\n",
      "|           6|   Holiday|   Local|     Ibarra|\n",
      "|           6|   Holiday|Regional|   Imbabura|\n",
      "+------------+----------+--------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkql(\"\"\"select count(*) as categ_counts, hd.type, \\\n",
    "            hd.locale,hd.locale_name \\\n",
    "            from holidays_table hd \\\n",
    "            group by hd.type,hd.locale,hd.locale_name \n",
    "            order by hd.locale_name\n",
    "            \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb5e345e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1218"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oil_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4153ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-18,48.59\n",
      "2017-08-21,47.39\n",
      "2017-08-22,47.65\n",
      "2017-08-23,48.45\n",
      "2017-08-24,47.24\n",
      "2017-08-25,47.65\n",
      "2017-08-28,46.4\n",
      "2017-08-29,46.46\n",
      "2017-08-30,45.96\n",
      "2017-08-31,47.26\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "tail oil.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73c69ca",
   "metadata": {},
   "source": [
    "### The data is available till Aug'17 starting from Jan'13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "105ddd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n",
      "|               date|dcoilwtico|\n",
      "+-------------------+----------+\n",
      "|2013-01-01 00:00:00|      null|\n",
      "|2013-01-02 00:00:00|     93.14|\n",
      "|2013-01-03 00:00:00|     92.97|\n",
      "|2013-01-04 00:00:00|     93.12|\n",
      "|2013-01-07 00:00:00|      93.2|\n",
      "+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkql(\"select * from oil_table limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3657885d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------------------+----+-------+\n",
      "|store_nbr|         city|               state|type|cluster|\n",
      "+---------+-------------+--------------------+----+-------+\n",
      "|        1|        Quito|           Pichincha|   D|     13|\n",
      "|        2|        Quito|           Pichincha|   D|     13|\n",
      "|        3|        Quito|           Pichincha|   D|      8|\n",
      "|        4|        Quito|           Pichincha|   D|      9|\n",
      "|        5|Santo Domingo|Santo Domingo de ...|   D|      4|\n",
      "+---------+-------------+--------------------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkql(\"select * from stores_table limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90ea9b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "261eb0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+---------+----------+-----+-----------+\n",
      "| id|               date|store_nbr|    family|sales|onpromotion|\n",
      "+---+-------------------+---------+----------+-----+-----------+\n",
      "|  0|2013-01-01 00:00:00|        1|AUTOMOTIVE|  0.0|          0|\n",
      "|  1|2013-01-01 00:00:00|        1| BABY CARE|  0.0|          0|\n",
      "|  2|2013-01-01 00:00:00|        1|    BEAUTY|  0.0|          0|\n",
      "|  3|2013-01-01 00:00:00|        1| BEVERAGES|  0.0|          0|\n",
      "|  4|2013-01-01 00:00:00|        1|     BOOKS|  0.0|          0|\n",
      "+---+-------------------+---------+----------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkql(\"select * from train_table limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80c82b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3000888"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1f9819a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+------------+\n",
      "|               date|store_nbr|transactions|\n",
      "+-------------------+---------+------------+\n",
      "|2013-01-01 00:00:00|       25|         770|\n",
      "|2013-01-02 00:00:00|        1|        2111|\n",
      "|2013-01-02 00:00:00|        2|        2358|\n",
      "|2013-01-02 00:00:00|        3|        3487|\n",
      "|2013-01-02 00:00:00|        4|        1922|\n",
      "+-------------------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkql(\"SELECT * FROM txn_table LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d8687cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83488"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txn_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73732434",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(123, 1, \"01/01/2021\",),\n",
    "        (123, 0, \"01/02/2021\",),\n",
    "        (123, 1, \"01/03/2021\",),\n",
    "        (123, 0, \"01/06/2021\",),\n",
    "        (123, 0, \"01/08/2021\",),\n",
    "        (777, 0, \"01/01/2021\",),\n",
    "        (777, 1, \"01/03/2021\",), ]\n",
    "\n",
    "df = spark.createDataFrame(data, (\"ID\", \"FLAG\", \"DATE\",)) \\\n",
    "        .withColumn(\"DATE\", to_date(col(\"DATE\"), \"dd/MM/yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7878ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+\n",
      "| ID|FLAG|      DATE|\n",
      "+---+----+----------+\n",
      "|123|   1|2021-01-01|\n",
      "|123|   0|2021-02-01|\n",
      "+---+----+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "974b18e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------------------+\n",
      "|           min_date|           max_date|            interval|\n",
      "+-------------------+-------------------+--------------------+\n",
      "|2013-01-01 00:00:00|2017-08-31 00:00:00|INTERVAL '-1703 0...|\n",
      "+-------------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkql(\"\"\"SELECT MIN(date) as min_date,MAX(date) as max_date,\n",
    "                MIN(date) - MAX(date) as interval\n",
    "                from oil_table\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ebb51aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dates_df = df.groupBy(\"id\").agg(\n",
    "    date_trunc(\"mm\", max(to_date(\"date\", \"dd/MM/yyyy\"))).\\\n",
    "            alias(\"max_date\"),\n",
    "    date_trunc(\"mm\", min(to_date(\"date\", \"dd/MM/yyyy\"))). \\\n",
    "            alias(\"min_date\")). \\\n",
    "    select(\"id\",expr(\"sequence(min_date, max_date, interval 1 month)\").alias(\"date_seq\")). \\\n",
    "        withColumn(\"date_new\",explode(\"date_seq\")). \\\n",
    "        withColumn(\"date_form\",date_format(\"date_new\", \"dd/MM/yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "96fc97d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------------------+\n",
      "|           max_date|           min_date|           diff_date|\n",
      "+-------------------+-------------------+--------------------+\n",
      "|2017-08-01 00:00:00|2013-01-01 00:00:00|INTERVAL '1673 00...|\n",
      "+-------------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oil_data.select(date_trunc(\"mm\", max(to_date(\"date\", \"dd/MM/yyyy\"))).\\\n",
    "            alias(\"max_date\"),\n",
    "            date_trunc(\"mm\", min(to_date(\"date\", \"dd/MM/yyyy\"))). \\\n",
    "            alias(\"min_date\"),\n",
    "            (date_trunc(\"mm\", max(to_date(\"date\", \"dd/MM/yyyy\"))) - \\\n",
    "            date_trunc(\"mm\", min(to_date(\"date\", \"dd/MM/yyyy\")))).alias('diff_date')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c3ad99",
   "metadata": {},
   "source": [
    "### Creating the date sequence that we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec5cec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_date_series = oil_data.select(date_trunc(\"mm\", max(to_date(\"date\", \"dd/MM/yyyy\"))).\\\n",
    "            alias(\"max_date\"),\n",
    "            date_trunc(\"mm\", min(to_date(\"date\", \"dd/MM/yyyy\"))). \\\n",
    "            alias(\"min_date\")). \\\n",
    "    select(expr(\"sequence(min_date, max_date, interval 1 day)\").alias(\"date_seq\")). \\\n",
    "        withColumn(\"date_new\",explode(\"date_seq\")). \\\n",
    "        withColumn(\"date_form\",date_format(\"date_new\", \"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dc50800",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_series=data_date_series.drop(\"date_seq\",\"date_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad754e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1674"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_series.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d472d7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_series.createOrReplaceTempView('date_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aed6718c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| date_form|\n",
      "+----------+\n",
      "|2013-01-01|\n",
      "|2013-01-02|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkql(\"\"\"SELECT date_form \n",
    "            from date_table\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fd7129",
   "metadata": {},
   "source": [
    "### Build the tables SQL style:Not so fast. \n",
    "\n",
    "In Spark SQL implementation the constraints like\n",
    "Primary, Secondary is not established. The PR has been already raised in tho ASF though.\n",
    "\n",
    "sparkql(\"\"\" CREATE TABLE full_oil_table AS\n",
    "        \n",
    "        SELECT date_form, COALESCE(dcoilwtico,0) as dcoilwtico\n",
    "        \n",
    "        FROM date_table dt LEFT JOIN oil_table ot\n",
    "        \n",
    "        ON dt.date_form = ot.date\"\"\")\n",
    "        \n",
    "        \n",
    "The above command requires hive support, and errors out. We cannot create fully constrained tables in spark context. We have to do it in RDBMS environment if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4b74e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resorting to the Temp view creation route instead\n",
    "sparkql(\"\"\" SELECT date_form, COALESCE(dcoilwtico,0) as dcoilwtico\n",
    "        FROM date_table dt LEFT JOIN oil_table ot\n",
    "        ON dt.date_form = ot.date\"\"\"). \\\n",
    "    createOrReplaceTempView('full_oil_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbde1e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "| date_form|dcoilwtico|\n",
      "+----------+----------+\n",
      "|2013-01-01|       0.0|\n",
      "|2013-01-02|     93.14|\n",
      "+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating table the sql style\n",
    "sparkql(\"\"\"SELECT * \n",
    "            FROM full_oil_table\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965ea595",
   "metadata": {},
   "source": [
    "#### Attempting to join the oil_data with holiday_table \n",
    "\n",
    "-- Doing some recon on the table columns, the ranges and data types\n",
    "\n",
    "-- Thinking of the format to be used for columns used for joining "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90c8439d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------------------+\n",
      "|           max_date|           min_date|           avbl_span|\n",
      "+-------------------+-------------------+--------------------+\n",
      "|2017-12-26 00:00:00|2012-03-02 00:00:00|INTERVAL '2125 00...|\n",
      "+-------------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkql(\"\"\"SELECT MAX(date) as max_date,\n",
    "            MIN(date) as min_date,\n",
    "            MAX(date) - MIN(date) as avbl_span\n",
    "            FROM holidays_table\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9ea7acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+--------+-----------+--------------------+-----------+\n",
      "|               date|   type|  locale|locale_name|         description|transferred|\n",
      "+-------------------+-------+--------+-----------+--------------------+-----------+\n",
      "|2012-03-02 00:00:00|Holiday|   Local|      Manta|  Fundacion de Manta|      false|\n",
      "|2012-04-01 00:00:00|Holiday|Regional|   Cotopaxi|Provincializacion...|      false|\n",
      "+-------------------+-------+--------+-----------+--------------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkql(\"\"\"SELECT *\n",
    "            FROM holidays_table\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5c773b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|date_format(date, yyyy-MM-dd)|\n",
      "+-----------------------------+\n",
      "|                   2012-03-02|\n",
      "|                   2012-04-01|\n",
      "+-----------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Date_trunc is avble but not useful\n",
    "# To_char is not avble\n",
    "# Extract is avble, did not try\n",
    "# date_format, found today only, my new_year gift ;)\n",
    "\n",
    "sparkql(\"\"\"SELECT date_format(date, 'yyyy-MM-dd') FROM holidays_table\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "87e87be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+--------+--------+-----------+----------+\n",
      "|date_form |date               |type    |locale  |locale_name|dcoilwtico|\n",
      "+----------+-------------------+--------+--------+-----------+----------+\n",
      "|2013-01-01|2013-01-01 00:00:00|Holiday |National|Ecuador    |0.0       |\n",
      "|2013-01-05|2013-01-05 00:00:00|Work Day|National|Ecuador    |0.0       |\n",
      "+----------+-------------------+--------+--------+-----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkql(\"\"\" SELECT ot.date_form, ht.date, ht.type, ht.locale,\n",
    "        ht.locale_name,ot.dcoilwtico\n",
    "        FROM holidays_table ht JOIN full_oil_table ot\n",
    "        ON date_format(ht.date,'yyyy-MM-dd') = ot.date_form\n",
    "\"\"\").show(2, truncate=False)\n",
    "## The tables are joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c5b7203",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkql(\"\"\" SELECT ot.date_form, COALESCE(ht.type,'Working') as type, \n",
    "        COALESCE(ht.locale,'National') as locale,\n",
    "        COALESCE(ht.locale_name,'National') as locale_name,\n",
    "        ot.dcoilwtico\n",
    "        FROM holidays_table ht RIGHT JOIN full_oil_table ot\n",
    "        ON date_format(ht.date,'yyyy-MM-dd') = ot.date_form\n",
    "\"\"\").createOrReplaceTempView('full_oil_with_holidays')\n",
    "## The tables are joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c443484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------+-----------+----------+\n",
      "| date_form|   type|  locale|locale_name|dcoilwtico|\n",
      "+----------+-------+--------+-----------+----------+\n",
      "|2013-01-01|Holiday|National|    Ecuador|       0.0|\n",
      "|2013-01-02|Working|National|   National|     93.14|\n",
      "+----------+-------+--------+-----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkql(\"\"\"SELECT * \n",
    "            FROM full_oil_with_holidays\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ff93e2",
   "metadata": {},
   "source": [
    "Validating the table join\n",
    "\n",
    "- Check if there is extra rows\n",
    "\n",
    "- Find the extra rows \n",
    "\n",
    "- Ensure there is no duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c3c56f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1704"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkql(\"\"\" SELECT *\n",
    "        FROM full_oil_with_holidays\n",
    "\"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "2da158d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1674"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkql(\"\"\" SELECT distinct date_form\n",
    "        FROM full_oil_with_holidays\n",
    "\"\"\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4517ed84",
   "metadata": {},
   "source": [
    "We can observe the date has been duplicated. The reason must be linked with the locales and types. Running a group by with those \n",
    "columns must assure there is no data duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "11ab0319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+\n",
      "|typ_counts| date_form|   type|\n",
      "+----------+----------+-------+\n",
      "|         3|2013-06-25|Holiday|\n",
      "|         2|2013-07-03|Holiday|\n",
      "|         3|2014-06-25|Holiday|\n",
      "|         2|2014-07-03|Holiday|\n",
      "|         3|2015-06-25|Holiday|\n",
      "|         2|2015-07-03|Holiday|\n",
      "|         2|2016-05-08|  Event|\n",
      "|         3|2016-06-25|Holiday|\n",
      "|         2|2016-07-03|Holiday|\n",
      "|         2|2017-04-14|Holiday|\n",
      "|         3|2017-06-25|Holiday|\n",
      "|         2|2017-07-03|Holiday|\n",
      "+----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkql(\"\"\" SELECT COUNT(1) as typ_counts, date_form, type\n",
    "        FROM full_oil_with_holidays\n",
    "        GROUP BY date_form, type\n",
    "        HAVING COUNT(1) > 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ac8b308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|date_form|dcoilwtico|\n",
      "+---------+----------+\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkql(\"\"\" SELECT ft.date_form, ft.dcoilwtico\n",
    "        FROM full_oil_with_holidays ft\n",
    "        EXCEPT\n",
    "        SELECT ot.date_form, ot.dcoilwtico \n",
    "        FROM full_oil_table ot\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13d0246",
   "metadata": {},
   "source": [
    "Based on above checks the table join and new view creation is successful. Proceeding to the next join\n",
    "\n",
    "Stores table shown below looks like a dimension table. The store_nbr can be the unique id. Lets check that.\n",
    "\n",
    "The store-nbr is arbitrary, to identify a particular store. There are multiple store in same city, state, type and cluster. It is a valid joiner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c11071fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------------------+----+-------+\n",
      "|store_nbr|         city|               state|type|cluster|\n",
      "+---------+-------------+--------------------+----+-------+\n",
      "|        1|        Quito|           Pichincha|   D|     13|\n",
      "|        2|        Quito|           Pichincha|   D|     13|\n",
      "|        3|        Quito|           Pichincha|   D|      8|\n",
      "|        4|        Quito|           Pichincha|   D|      9|\n",
      "|        5|Santo Domingo|Santo Domingo de ...|   D|      4|\n",
      "+---------+-------------+--------------------+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkql(\"\"\" SELECT st.*\n",
    "        FROM stores_table st\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6862e2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkql(\"\"\" SELECT tt.id, date_format(tt.date,'yyyy-MM-dd') as date,\n",
    "            tt.store_nbr, tt.family, \n",
    "            tt.sales, tt.onpromotion\n",
    "        FROM train_table tt\"\"\").createOrReplaceTempView('full_train_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "efb97dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 53:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|day_data|      date|\n",
      "+--------+----------+\n",
      "|    1782|2013-01-01|\n",
      "|    1782|2013-01-02|\n",
      "+--------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sparkql(\"\"\"select COUNT(1) as day_data,tt.date\n",
    "            FROM full_train_table tt\n",
    "            GROUP BY tt.date\n",
    "            ORDER BY tt.date\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "687b6dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 247:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------------------+\n",
      "|           max_date|           min_date|           avbl_span|\n",
      "+-------------------+-------------------+--------------------+\n",
      "|2017-08-15 00:00:00|2013-01-01 00:00:00|INTERVAL '1687 00...|\n",
      "+-------------------+-------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sparkql(\"\"\"SELECT MAX(date) as max_date,\n",
    "            MIN(date) as min_date,\n",
    "            MAX(date) - MIN(date) as avbl_span\n",
    "            FROM train_table\"\"\").show(2)\n",
    "\n",
    "#Number of days is 1687 which is 14 days more than data\n",
    "# available in oil_data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392ba90d",
   "metadata": {},
   "source": [
    "Remember the SQL follows the \n",
    "                        \n",
    "                        From\n",
    "                                Join\n",
    "                                \n",
    "                             Where\n",
    "                             \n",
    "                         Groupby\n",
    "                         \n",
    "                                 Select\n",
    "                                 \n",
    "                                        Order by for execution.\n",
    "                                        \n",
    "\n",
    "Based on above execution, order by can see variables present in Select. But G / W cannot see them\n",
    "\n",
    "Lets get joining the train table with the stores and full_oil_table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e869560f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+----------+-----+-----------+---------+-----+---------+----+-------+----------+----------+\n",
      "| id|      date|store_nbr|    family|sales|onpromotion|store_nbr| city|    state|type|cluster| date_form|dcoilwtico|\n",
      "+---+----------+---------+----------+-----+-----------+---------+-----+---------+----+-------+----------+----------+\n",
      "|  0|2013-01-01|        1|AUTOMOTIVE|  0.0|          0|        1|Quito|Pichincha|   D|     13|2013-01-01|       0.0|\n",
      "|  1|2013-01-01|        1| BABY CARE|  0.0|          0|        1|Quito|Pichincha|   D|     13|2013-01-01|       0.0|\n",
      "+---+----------+---------+----------+-----+-----------+---------+-----+---------+----+-------+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkql(\"\"\" SELECT ftt.*, st.*,fot.*\n",
    "        FROM full_train_table ftt JOIN stores_table st\n",
    "        on ftt.store_nbr = st.store_nbr\n",
    "        join full_oil_table fot\n",
    "        on fot.date_form = ftt.date\n",
    "\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "47ab3d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 103:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+----------+-----+-----------+---------+-----+---------+----+-------+----------+----------+\n",
      "|  id|      date|store_nbr|    family|sales|onpromotion|store_nbr| city|    state|type|cluster| date_form|dcoilwtico|\n",
      "+----+----------+---------+----------+-----+-----------+---------+-----+---------+----+-------+----------+----------+\n",
      "|3564|2013-01-03|        1|AUTOMOTIVE|  3.0|          0|        1|Quito|Pichincha|   D|     13|2013-01-03|     92.97|\n",
      "|3565|2013-01-03|        1| BABY CARE|  0.0|          0|        1|Quito|Pichincha|   D|     13|2013-01-03|     92.97|\n",
      "+----+----------+---------+----------+-----+-----------+---------+-----+---------+----+-------+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sparkql(\"\"\" SELECT ftt.*, st.*,fot.*\n",
    "        FROM full_train_table ftt JOIN stores_table st\n",
    "        on ftt.store_nbr = st.store_nbr\n",
    "        RIGHT JOIN full_oil_table fot\n",
    "        on fot.date_form = ftt.date\n",
    "\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab35fe59",
   "metadata": {},
   "source": [
    "Lets try validating the join by the usual process of checking the data\n",
    "\n",
    "-- Row Counts of store Number of individual tables and final \n",
    "joined tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a0e6ac0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3000888"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkql(\"\"\"SELECT * FROM train_table\"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9afc0b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2975940"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkql(\"\"\" SELECT ftt.*, st.*,fot.*\n",
    "        FROM full_train_table ftt JOIN stores_table st\n",
    "        on ftt.store_nbr = st.store_nbr\n",
    "        join full_oil_table fot\n",
    "        on fot.date_form = ftt.date\n",
    "\"\"\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b2c771",
   "metadata": {},
   "source": [
    "Hmm the rows has been lost... I guess some of the full oil table \n",
    "has lesser date rows... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ca2174b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1684"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkql(\"\"\"select COUNT(1) as day_data,tt.date\n",
    "            FROM full_train_table tt\n",
    "            GROUP BY tt.date\n",
    "            ORDER BY tt.date\"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c8162453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1674"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkql(\"\"\"select COUNT(1) as day_data,tt.date_form\n",
    "            FROM full_oil_table tt\n",
    "            GROUP BY tt.date_form\n",
    "            ORDER BY tt.date_form\"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b746803a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2983068"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# That provides part of the answer.\n",
    "3000888 - 1782 * 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c492137a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets check the store numbers. That tallys up with the \n",
    "# 54 store numbers\n",
    "sparkql(\"\"\"select COUNT(1) as day_data,tt.store_nbr\n",
    "            FROM full_train_table tt\n",
    "            GROUP BY tt.store_nbr\n",
    "            ORDER BY tt.store_nbr\"\"\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f18111c",
   "metadata": {},
   "source": [
    "So there we found the culprit. We had to do left outer join.\n",
    "There might be days which is present in train_table and not \n",
    "in oil_data. We need to work on that next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "608eca0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3000888"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkql(\"\"\" SELECT ftt.*, st.*,fot.*\n",
    "        FROM full_train_table ftt JOIN stores_table st\n",
    "        on ftt.store_nbr = st.store_nbr\n",
    "        LEFT JOIN full_oil_table fot\n",
    "        on fot.date_form = ftt.date\"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e37bc52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkql(\"\"\" SELECT ftt.id, ftt.date,ftt.store_nbr,ftt.family,\n",
    "            ftt.sales, ftt.onpromotion, st.city, st.state,st.type,\n",
    "            st.cluster,fot.dcoilwtico\n",
    "        FROM full_train_table ftt JOIN stores_table st\n",
    "        on ftt.store_nbr = st.store_nbr\n",
    "        LEFT JOIN full_oil_table fot\n",
    "        on fot.date_form = ftt.date\"\"\"). \\\n",
    "        createOrReplaceTempView(\"train_store_oil_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "82da7c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 330:===================>                                     (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+----------+-----+-----------+-----+---------+----+-------+----------+\n",
      "|     id|      date|store_nbr|    family|sales|onpromotion| city|    state|type|cluster|dcoilwtico|\n",
      "+-------+----------+---------+----------+-----+-----------+-----+---------+----+-------+----------+\n",
      "|2975940|2017-08-02|        1|AUTOMOTIVE|  4.0|          0|Quito|Pichincha|   D|     13|      null|\n",
      "|2975941|2017-08-02|        1| BABY CARE|  0.0|          0|Quito|Pichincha|   D|     13|      null|\n",
      "+-------+----------+---------+----------+-----+-----------+-----+---------+----+-------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 330:======================================>                  (2 + 1) / 3]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sparkql(\"\"\"SELECT tsot.*\n",
    "            FROM train_store_oil_table tsot\n",
    "            WHERE tsot.date = '2017-08-02'\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7f0d5ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|      date|total_txn|\n",
      "+----------+---------+\n",
      "|2013-01-01|      770|\n",
      "|2013-01-02|    93215|\n",
      "|2013-01-03|    78504|\n",
      "|2013-01-04|    78494|\n",
      "|2013-01-05|    93573|\n",
      "+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkql(\"\"\"SELECT date_format(txt.date,'yyyy-MM-dd') as date,\n",
    "                sum(txt.transactions) as total_txn\n",
    "                FROM txn_table txt\n",
    "            GROUP BY date_format(txt.date,'yyyy-MM-dd')\n",
    "            ORDER BY date\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "135602fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------------------+\n",
      "|           max_date|           min_date|           avbl_span|\n",
      "+-------------------+-------------------+--------------------+\n",
      "|2017-08-15 00:00:00|2013-01-01 00:00:00|INTERVAL '1687 00...|\n",
      "+-------------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkql(\"\"\"SELECT MAX(date) as max_date,\n",
    "            MIN(date) as min_date,\n",
    "            MAX(date) - MIN(date) as avbl_span\n",
    "            FROM txn_table\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0c74c201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(date='2017-08-11', total_txn=89551),\n",
       " Row(date='2017-08-12', total_txn=89927),\n",
       " Row(date='2017-08-13', total_txn=85993),\n",
       " Row(date='2017-08-14', total_txn=85448),\n",
       " Row(date='2017-08-15', total_txn=86561)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkql(\"\"\"SELECT date_format(txt.date,'yyyy-MM-dd') as date,\n",
    "                sum(txt.transactions) as total_txn\n",
    "                FROM txn_table txt\n",
    "            GROUP BY date_format(txt.date,'yyyy-MM-dd')\n",
    "            ORDER BY date\"\"\").tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599811d4",
   "metadata": {},
   "source": [
    "There are missing txn data in the middle of the span. Which the above way of checking will not show. Lets proceed with the joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f2d0fa95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+----------+-----+-----------+-----+---------+----+-------+----------+----------+------------+---------+\n",
      "| id|      date|store_nbr|    family|sales|onpromotion| city|    state|type|cluster|dcoilwtico|  txn_date|transactions|store_nbr|\n",
      "+---+----------+---------+----------+-----+-----------+-----+---------+----+-------+----------+----------+------------+---------+\n",
      "|  0|2013-01-01|        1|AUTOMOTIVE|  0.0|          0|Quito|Pichincha|   D|     13|       0.0|2013-01-01|         770|       25|\n",
      "|  1|2013-01-01|        1| BABY CARE|  0.0|          0|Quito|Pichincha|   D|     13|       0.0|2013-01-01|         770|       25|\n",
      "+---+----------+---------+----------+-----+-----------+-----+---------+----+-------+----------+----------+------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkql(\"\"\"SELECT tsot.*, date_format(txt.date,'yyyy-MM-dd') as txn_date,\n",
    "            txt.transactions, txt.store_nbr\n",
    "            FROM train_store_oil_table tsot LEFT JOIN txn_table txt\n",
    "            on tsot.date = date_format(txt.date,'yyyy-MM-dd')\n",
    "        \"\"\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a9f9247e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3000888"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkql(\"\"\"SELECT tsot.*, date_format(txt.date,'yyyy-MM-dd') as txn_date,\n",
    "            txt.transactions, txt.store_nbr\n",
    "            FROM train_store_oil_table tsot LEFT JOIN txn_table txt\n",
    "            on tsot.date = date_format(txt.date,'yyyy-MM-dd') and\n",
    "                tsot.store_nbr = txt.store_nbr\n",
    "        \"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "899744a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83488"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkql(\"\"\"SELECT date_format(txt.date,'yyyy-MM-dd') as txn_date,\n",
    "            txt.transactions, txt.store_nbr\n",
    "            FROM txn_table txt\"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "43dc7303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250538137344"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "83488 * 3000888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2f6439df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkql(\"\"\"SELECT tsot.*, \n",
    "            COALESCE(DATE_FORMAT(txt.date,'yyyy-MM-dd'),tsot.date) as txn_date,\n",
    "            COALESCE(txt.transactions,0) as store_txns, \n",
    "            COALESCE(txt.store_nbr, tsot.store_nbr) as store_nbr\n",
    "            FROM train_store_oil_table tsot LEFT JOIN txn_table txt\n",
    "            on tsot.date = date_format(txt.date,'yyyy-MM-dd')\n",
    "            and tsot.store_nbr = txt.store_nbr\n",
    "        \"\"\").createOrReplaceTempView(\"all_data_joined_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "dc423d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+-------------------+---------+-----------+-------+-----------+----+-------+----------+----------+----------+---------+\n",
      "| id|      date|store_nbr|             family|    sales|onpromotion|   city|      state|type|cluster|dcoilwtico|  txn_date|store_txns|store_nbr|\n",
      "+---+----------+---------+-------------------+---------+-----------+-------+-----------+----+-------+----------+----------+----------+---------+\n",
      "|561|2013-01-01|       25|         AUTOMOTIVE|      0.0|          0|Salinas|Santa Elena|   D|      1|       0.0|2013-01-01|       770|       25|\n",
      "|562|2013-01-01|       25|          BABY CARE|      0.0|          0|Salinas|Santa Elena|   D|      1|       0.0|2013-01-01|       770|       25|\n",
      "|563|2013-01-01|       25|             BEAUTY|      2.0|          0|Salinas|Santa Elena|   D|      1|       0.0|2013-01-01|       770|       25|\n",
      "|564|2013-01-01|       25|          BEVERAGES|    810.0|          0|Salinas|Santa Elena|   D|      1|       0.0|2013-01-01|       770|       25|\n",
      "|565|2013-01-01|       25|              BOOKS|      0.0|          0|Salinas|Santa Elena|   D|      1|       0.0|2013-01-01|       770|       25|\n",
      "|566|2013-01-01|       25|       BREAD/BAKERY|  180.589|          0|Salinas|Santa Elena|   D|      1|       0.0|2013-01-01|       770|       25|\n",
      "|567|2013-01-01|       25|        CELEBRATION|      0.0|          0|Salinas|Santa Elena|   D|      1|       0.0|2013-01-01|       770|       25|\n",
      "|568|2013-01-01|       25|           CLEANING|    186.0|          0|Salinas|Santa Elena|   D|      1|       0.0|2013-01-01|       770|       25|\n",
      "|569|2013-01-01|       25|              DAIRY|    143.0|          0|Salinas|Santa Elena|   D|      1|       0.0|2013-01-01|       770|       25|\n",
      "|570|2013-01-01|       25|               DELI|    71.09|          0|Salinas|Santa Elena|   D|      1|       0.0|2013-01-01|       770|       25|\n",
      "|571|2013-01-01|       25|               EGGS|     46.0|          0|Salinas|Santa Elena|   D|      1|       0.0|2013-01-01|       770|       25|\n",
      "|572|2013-01-01|       25|       FROZEN FOODS|29.654999|          0|Salinas|Santa Elena|   D|      1|       0.0|2013-01-01|       770|       25|\n",
      "|573|2013-01-01|       25|          GROCERY I|    700.0|          0|Salinas|Santa Elena|   D|      1|       0.0|2013-01-01|       770|       25|\n",
      "|574|2013-01-01|       25|         GROCERY II|     15.0|          0|Salinas|Santa Elena|   D|      1|       0.0|2013-01-01|       770|       25|\n",
      "|575|2013-01-01|       25|           HARDWARE|      0.0|          0|Salinas|Santa Elena|   D|      1|       0.0|2013-01-01|       770|       25|\n",
      "|576|2013-01-01|       25| HOME AND KITCHEN I|      0.0|          0|Salinas|Santa Elena|   D|      1|       0.0|2013-01-01|       770|       25|\n",
      "|577|2013-01-01|       25|HOME AND KITCHEN II|      0.0|          0|Salinas|Santa Elena|   D|      1|       0.0|2013-01-01|       770|       25|\n",
      "|578|2013-01-01|       25|    HOME APPLIANCES|      0.0|          0|Salinas|Santa Elena|   D|      1|       0.0|2013-01-01|       770|       25|\n",
      "|579|2013-01-01|       25|          HOME CARE|      0.0|          0|Salinas|Santa Elena|   D|      1|       0.0|2013-01-01|       770|       25|\n",
      "|580|2013-01-01|       25|         LADIESWEAR|      0.0|          0|Salinas|Santa Elena|   D|      1|       0.0|2013-01-01|       770|       25|\n",
      "+---+----------+---------+-------------------+---------+-----------+-------+-----------+----+-------+----------+----------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkql(\"\"\"SELECT * FROM all_data_joined_data adj\n",
    "            WHERE adj.date = '2013-01-01'\n",
    "            and adj.store_txns != 0\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
